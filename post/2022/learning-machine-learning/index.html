<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><meta name=author content="Anıl Zenginoğlu"><meta name=description content="This is a short tutorial on machine learning. We implement a single-layer neural network to recognize images of handwritten digits from the MNIST dataset using only NumPy as a simple starting point for further experimentation. A [Colab](https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing) notebook is available."><link rel=alternate hreflang=en-us href=https://anilzen.github.io/post/2022/learning-machine-learning/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.1f9c62afd7c290abaaba48f4374e349d.css><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=google-site-verification content="CVDtdnmvDztdvP8XA_Fh0StEBOfqy3DM5L6GyojfVgk"><script async src="https://www.googletagmanager.com/gtag/js?id=G-1Q5LP68RP1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-1Q5LP68RP1",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu6b147281c14d31ddea9d4b1ec87dcb1f_1135_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu6b147281c14d31ddea9d4b1ec87dcb1f_1135_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://anilzen.github.io/post/2022/learning-machine-learning/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@AnilZenginoglu"><meta property="twitter:creator" content="@AnilZenginoglu"><meta property="og:site_name" content="Anıl Zenginoğlu"><meta property="og:url" content="https://anilzen.github.io/post/2022/learning-machine-learning/"><meta property="og:title" content="Learning Machine Learning | Anıl Zenginoğlu"><meta property="og:description" content="This is a short tutorial on machine learning. We implement a single-layer neural network to recognize images of handwritten digits from the MNIST dataset using only NumPy as a simple starting point for further experimentation. A [Colab](https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing) notebook is available."><meta property="og:image" content="https://anilzen.github.io/post/2022/learning-machine-learning/featured.png"><meta property="twitter:image" content="https://anilzen.github.io/post/2022/learning-machine-learning/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-12-08T00:01:00+00:00"><meta property="article:modified_time" content="2022-12-08T00:01:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://anilzen.github.io/post/2022/learning-machine-learning/"},"headline":"Learning Machine Learning","image":["https://anilzen.github.io/post/2022/learning-machine-learning/featured.png"],"datePublished":"2022-12-08T00:01:00Z","dateModified":"2022-12-08T00:01:00Z","author":{"@type":"Person","name":"Anıl Zenginoğlu"},"publisher":{"@type":"Organization","name":"Anıl Zenginoğlu","logo":{"@type":"ImageObject","url":"https://anilzen.github.io/media/icon_hu6b147281c14d31ddea9d4b1ec87dcb1f_1135_192x192_fill_lanczos_center_3.png"}},"description":"This is a short tutorial on machine learning. We implement a single-layer neural network to recognize images of handwritten digits from the MNIST dataset using only NumPy as a simple starting point for further experimentation. A [Colab](https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing) notebook is available."}</script><title>Learning Machine Learning | Anıl Zenginoğlu</title>
<link rel=me href=https://mathstodon.xyz/@anilzen></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=71f4ae963ccb66abcec46c8ab8481080><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Anıl Zenginoğlu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Anıl Zenginoğlu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/nsf-award-2309084><span>NSF Award</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Learning Machine Learning</h1><p class=page-subtitle>A simple starting point for learning machine learning: we implement a single-layer neural network to recognize images of handwritten digits from the MNIST dataset using only NumPy.</p><div class=article-metadata><div><span class=author-highlighted>Anıl Zenginoğlu</span></div><span class=article-date>Dec 8, 2022
</span><span class=middot-divider></span>
<span class=article-reading-time>12 min read
</span><span class=middot-divider></span>
<span class=article-categories><i class="fas fa-folder mr-1"></i><a href=/category/tutorial/>Tutorial</a></span></div></div><div class=article-container><div class=article-style><a target=_blank href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a><h2 id=neural-network-what-is-machine-learning>Neural Network: What is machine learning?</h2><p>The core of a machine learning algorithm is the neural network that maps inputs to desired outputs. When you read &ldquo;neural network,&rdquo; you might think of the human brain with layers of interconnected neurons working together to solve problems. But in machine learning, we are simply talking about a function with parameters. Lots and lots of parameters. &ldquo;Learning&rdquo; is adjusting these parameters until the difference between the desired output and the actual output of the function is sufficiently small. That&rsquo;s it. In a way, machine learning is the rediscovery of the old adage that you can fit any <a href=https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant target=_blank rel=noopener>elephant</a> with sufficient parameters.</p><p>This was a very short summary of what people mean by machine learning. You can imagine that the function, the parameters, and the adjustment process are all rather sophisticated and can get very complicated. I&rsquo;ll expand on this basic idea below with an example of a machine-learning algorithm.</p><p>First, some terminology. Think of each input dimension as a neuron in a neural network. The parameters of the neural network, or the function, are called weights and biases. Weights represent the strength of the connection between neurons; biases shift the activation threshold of a neuron. By adjusting the weights and biases based on the output, the network learns the patterns in the data and makes predictions on new data.</p><p>Let&rsquo;s write this down. I mentioned that the neural network is just a function with parameters. Its output is usually a probability, so we&rsquo;ll call it $p$. The network should look something like $p=f(x; W,b)$, where $x$ is the input array, $p$ is the network&rsquo;s output array, $W$ are the weights, and $b$ are the biases. A simple neural network could then be written like this
$$ p = W \cdot x + b. $$
But wait, you say; this is just a linear transformation! Layering linear transformations on top of each other can only create a linear network. You can&rsquo;t learn complex patterns and make accurate predictions with just linear transformations. To introduce nonlinearity into the model, we throw this into a nonlinear activation function $\sigma$, so the output looks like
$$ p = \sigma(W \cdot x + b). \label{1} \tag{1} $$
There are a few commonly used activation functions that one frequently encounters: the sigmoid function, the hyperbolic tangent (tanh) function, the rectified linear unit (ReLU) function, and so on. We&rsquo;ll use a generalization of the sigmoid (or logistic) function for our experiments.
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
This function maps a real-valued input to a value between 0 and 1, so the output can be interpreted as a probability, making it directly useful in classification problems.</p><p>To get into more detail, we need to understand and prepare the input. I use the MNIST dataset for the demonstration below. We will avoid the powerful machine learning packages and only use NumPy<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, so all operations can be considered elemental. You can follow along on <a href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing" target=_blank rel=noopener>Colab</a>.
<a target=_blank href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing"><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></p><h2 id=mnist-dataset-prepare-the-input>MNIST Dataset: Prepare the input</h2><p>The MNIST dataset is a large database of handwritten digits consisting of 60,000 training images and 10,000 test images. Each image is a 28x28 grayscale image labeled with the correct digit, from 0 to 9. It&rsquo;s commonly used for training and testing various image processing and machine learning algorithms. The digits in grayscale look like this<figure id=figure-handwritten-digits-0-and-1-from-the-mnist-dataset><div class="d-flex justify-content-center"><div class=w-100><img alt="Handwritten digits from the MNIST dataset" srcset="/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp 400w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_3fc866dcf8d288cdcdc65060faa475c2.webp 760w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp width=600 height=400 loading=lazy data-zoomable></div></div><figcaption>Handwritten digits 0 and 1 from the MNIST dataset.</figcaption></figure></p><p>Our goal is to teach the single-layer network (\ref{1}) to recognize these handwritten digits. The network learns from the training dataset by adjusting its weights to minimize the difference between the desired output and the actual output, that is, the loss. This process is repeated until the loss is sufficiently small, implying that the network has learned the dataset. So we need to define the loss, calculate how it depends on layer parameters, and find a way to minimize it iteratively.</p><h2 id=layer-and-loss-build-the-model>Layer and Loss: Build the model</h2><p>Our model architecture consists of just the one layer in (\ref{1}). So this is an example of shallow learning. But even with shallow networks, it can get confusing with the number of samples, inputs, and outputs. To recap, we have $M=60,000$ samples in the training set; each sample has $N=28\times28=784$ dimensions (one for each pixel in a flattened 1D-array); the output has $K=$10 dimensions (one for each digit). Accounting is worse when you have hidden layers in between. They all live in different spaces, so it makes sense to introduce different types of letters into the tensor notation for each type of space. To clarify each space, I like to define indices with their own ranges<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>:
$$ \begin{align}
a,b&=1,2,\dots,M=60,000. \\
\alpha, \beta&=1,2,\dots,N=784. \\
i,j&=1,2,\dots,K=10.
\end{align} $$
We can then write the output of our AI algorithm as
$$ p_{ai} = \sigma(z_{ai}) = \sigma\left( \sum_{\alpha=1}^{N} x_{a\alpha} W_{\alpha i} + b_i \right). \tag{2} \label{2} $$
Here, $\sigma$ is a generalization of the sigmoid function, called the softmax function. In code, we write</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>p_ai</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>W</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>softmax</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span></code></pre></div><p>This step is sometimes called the forward pass.</p><p>We do not use the sigmoid function because we have to ensure that the output probabilities sum to one. The softmax function is defined as
$$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}. \tag{3} \label{3} $$
It maps a vector of arbitrary real values, $z_i$, to a vector of values between 0 and 1. The sum of all outputs is 1, so each output can be interpreted as a probability. This makes it a useful activation function for multiclass classification tasks, where the predicted probabilities must sum to 1.</p><p>The softmax function is numerically unstable if implemented naively, so we rewrite it such that the maximum value of the input array is 0.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>z</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>z</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>axis</span> <span class=o>=</span> <span class=mi>1</span><span class=p>)[:,</span><span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>z</span><span class=p>)</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>z</span><span class=p>),</span> <span class=n>axis</span> <span class=o>=</span> <span class=mi>1</span><span class=p>)[:,</span><span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span>
</span></span></code></pre></div><hr><p>We now need to devise a way to tell our network when its outputs, $p_{ai}$, are losers. This is done by defining a loss function that measures the difference between the desired output, $y$, and the actual output, $p$. There are many possible choices. For example, when predicting a continuous variable, one typically uses a regression loss function such as mean squared error or mean absolute error. We have a multiclass classification problem (one class for each digit), so we&rsquo;ll use the cross-entropy loss<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> defined as
$$ L = - \frac{1}{M} \sum_{a=1}^M \sum_{i=1}^N y_{ai} \log p_{ai}, \tag{4} \label{4} $$
where $p$ is the predicted probability, $y$ is the actual probability, $M$ is the number of samples, and $N$ is the number of classes. In NumPy</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>cross_entropy</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>y</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>p</span><span class=p>))</span>
</span></span></code></pre></div><p>The cross-entropy loss is often used with a softmax activation function in the output layer of a classification model. One of the main reasons is the beautiful simplification of its derivative with respect to the input. But I&rsquo;m getting ahead of myself here.</p><p>How do we minimize the loss? We can evaluate the function at different locations in the parameter space to search for the minimum, but that&rsquo;s not an efficient approach, especially in high dimensions. In our simple toy example, the parameter space of weights and biases has $784\times 10+10=7,850$ dimensions.</p><p>A better approach is gradient descent. We start at some random point which will invariably have a large loss. It&rsquo;s like starting on a big hill. To go to the bottom, you take small steps downhill, that is, you descend along the negative gradient, until you can&rsquo;t go reasonably further. We need to compute the gradient of the loss function with respect to the weights and biases to determine the downhill direction. Using the chain rule, we obtain the following formula for the derivative of the loss function with respect to bias
$$ \frac{\partial{L}}{\partial b_{j}} = - \frac{1}{M}\sum_{a=1}^M \sum_{i=1}^N \sum_{k=1}^N y_{ai}\frac{\partial \log p_{ai}}{\partial z_{ak}} \frac{\partial z_{ak}}{\partial b_j} . $$
This calculation is where the simplification comes in when you combine the cross-entropy loss with softmax activation. To demonstrate, write the total loss as the mean of the losses of all samples, $L=\tfrac{1}{M}\sum_{a=1}^M \ell_a$. Let&rsquo;s compute the derivative for a single sample, suppressing its index
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = - \sum_{i=1}^N \sum_{k=1}^N y_{i}\frac{\partial \log p_{i}}{\partial z_{k}} \frac{\partial z_{k}}{\partial b_j} . $$
The log-term<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> with the definition of softmax (\ref{3}) reads
$$ \log p_i = \log (\sigma(z_i)) = z_i - \log\left(\sum_{j=1}^K e^{z_j}\right).$$
We get for the $z$-derivative
$$ \frac{\partial \log p_i}{\partial z_k} = \delta_{ik} - \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} = \delta_{ik} - p_k.$$
Combining with the summation over $y$, we obtain this very simple formula
$$ \sum_{i=1}^{N} y_{i} (\delta_{ik} - p_k) = y_k - p_k \sum_{i=1}^{N} y_{i} = y_k - p_k. $$
For the last step, remember that $y_i$ are probabilities that sum up to 1. We then have
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = \sum_{k=1}^N (p_k - y_k)\frac{\partial z_{k}}{\partial b_j} $$
Now we can insert the dependence of $z$ on $b$ and bring back the summation over the samples with index $a$ to get
$$ \frac{\partial{L}}{\partial b_{j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}). $$
Similarly, for the weights
$$ \frac{\partial{L}}{\partial W_{\beta j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}) x_{a\beta}. $$
In NumPy, the gradient of the loss is then calculated by<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>gradient</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>dL</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=o>/</span><span class=nb>len</span><span class=p>(</span><span class=n>x</span><span class=p>))</span><span class=o>*</span><span class=p>(</span><span class=n>p</span><span class=o>-</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>db</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dL</span><span class=p>,</span><span class=n>axis</span> <span class=o>=</span> <span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dW</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>dL</span><span class=o>.</span><span class=n>T</span><span class=p>,</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>dW</span><span class=p>,</span> <span class=n>db</span>
</span></span></code></pre></div><p>We then update the weights and biases based on these gradients with some step size $\lambda$. What mathematicians call step size is called learning rate in machine learning, even though it&rsquo;s not quite a rate. Anyway, we update the weights and biases iteratively as follows
$$ \begin{align}
W^{n+1} &= W^n - \lambda \frac{\partial{L}}{\partial W}, \\
b^{n+1} &= b^n - \lambda \frac{\partial{L}}{\partial b}.
\end{align} $$</p><h2 id=train-and-evaluate>Train and Evaluate</h2><p>We now have everything in place to train the network using the training set. To recap, we initialize the weights and biases randomly and then run multiple epochs<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, during which we get the network output (forward pass), compute the gradient of the associated loss, and update the weights and biases in the direction of the negative gradient with a constant learning rate (backpropagation). Below is all of this in code.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span> <span class=o>=</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>W</span><span class=p>,</span> <span class=n>b</span> <span class=o>=</span> <span class=n>init</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>output_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>p_ai</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dW</span><span class=p>,</span> <span class=n>db</span> <span class=o>=</span> <span class=n>gradient</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>,</span> <span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>W</span><span class=p>,</span> <span class=n>b</span> <span class=o>=</span> <span class=n>update</span><span class=p>(</span><span class=n>W</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>dW</span><span class=p>,</span> <span class=n>db</span><span class=p>,</span> <span class=n>lr</span><span class=p>)</span>
</span></span></code></pre></div><p>To evaluate the accuracy of the network on the training set, we compare the network&rsquo;s prediction with the labels</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>accuracy</span><span class=o>=</span><span class=p>[];</span> <span class=n>entropy_loss</span><span class=o>=</span><span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl>    <span class=n>accuracy</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=mi>100</span><span class=o>*</span><span class=n>np</span><span class=o>.</span><span class=n>count_nonzero</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>p</span><span class=p>,</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>y_train</span><span class=p>,</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>X_train</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>entropy_loss</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>y_train</span><span class=p>,</span><span class=n>p</span><span class=p>))</span>
</span></span></code></pre></div><p>We get about %90 accuracy with this simple method on both the test and the training sets!</p><p><figure id=figure-final-accuracy-and-loss-after-training-for-200-steps><div class="d-flex justify-content-center"><div class=w-100><img alt="Final result of training" srcset="/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp 400w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_2dc1e69ab398c40bfcd5a110fc5ac088.webp 760w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp width=760 height=380 loading=lazy data-zoomable></div></div><figcaption>Final accuracy and loss after training for 200 steps.</figcaption></figure></p><h2 id=wrapping-up>Wrapping up</h2><p>The building block of a neural network is the single layer (\ref{1}). I hope you obtained an understanding of how a neural network layer is trained and what people mean when they say the machine has learned something. My goal was to provide a basic understanding of this procedure. This can get very complicated. Large operational machine learning models, such as <a href=https://en.wikipedia.org/wiki/GPT-3 target=_blank rel=noopener>GPT-3</a>, <a href=https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval target=_blank rel=noopener>Gopher</a>, or <a href=https://developer.nvidia.com/megatron-turing-natural-language-generation target=_blank rel=noopener>Megatron-Turing NLG</a> use many network layers with hundreds of billions of parameters, but the procedure is similar.</p><p>You now have a few simple tools as a starting point for further inquiry. Here are a couple of directions to go from here:</p><ul><li>Use a more complex neural network with at least one hidden layer. The &ldquo;deep&rdquo; in deep learning comes from hidden layers. Each layer dramatically increases the parameter space&rsquo;s dimension, which also increases computation time. But the gain in accuracy may be worth it. You can easily get about %97 accuracy with just one additional layer for the MNIST dataset.</li><li>Batch-process your samples. Batch processing was not necessary for his example, but with higher dimensions and larger training sets, it becomes necessary. There are also indications that batch-processed (stochastic) gradient descent generalizes better.</li><li>Use a better optimization procedure. The step size, $\lambda$, or <code>lr</code>, is the most important parameter in gradient descent. I chose the rather large value of <code>lr=6</code>. You can see in the plots that the loss doesn&rsquo;t decrease monotonously. A simple fix for this is to multiply the learning rate with a scalar slightly less than 1, so it gets smaller at every epoch. But more importantly, you should use a better optimizer such as Adams. In fact, my interest in optimization is the reason I wrote this post. With Jingcheng Lu and Eitan Tadmor from the University of Maryland, we constructed a <a href=/publication/2022-swarm-based-gradient-descent/>swarm-based gradient descent</a> that avoids getting trapped at local minima. The method beats existing optimizers in certain types of non-convex optimization problems. I&rsquo;m experimenting to see whether it&rsquo;s useful in machine learning applications.</li></ul><script src=https://giscus.app/client.js data-repo=anilzen/anilzen.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk=" data-category=General data-category-id=DIC_kwDOFh8YOc4CTAsV data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=dark data-lang=en data-loading=lazy crossorigin=anonymous async></script><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>NumPy is a free, open-source Python library for scientific computing and data analysis. It has a lot of functionality, but the main reason for using NumPy here is its speed. It uses vectorization instead of looping through individual elements to perform calculations on an array, allowing faster calculations and more efficient use of memory. NumPy operations are implemented in C using highly optimized libraries that take advantage of modern processor architectures, so we are not slowed down by Python&rsquo;s excruciatingly inefficient loops.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>This is common practice in theoretical physics, in particular, in general relativity, where we have maps between and projections onto various different spaces and subspaces. The tensor notation using different types of letters helps keep track of the spaces in the computations. I was tempted to introduce the Einstein summation convention here as well, but it doesn&rsquo;t quite work.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>You might recognize this expression of entropy. It appears commonly as $-\sum p_i \log p_i$ in different entropy formulas such as Gibbs entropy, Shannon entropy, and von Neumann entropy. In our case, it&rsquo;s a way of measuring the difference between two probability distributions. We compare the true distribution of the labeled data, $y$, and the predicted probability distribution from the model, $p$. When the same probability distribution $p$ is used on either side of the log function, for example, in Shannon entropy, it measures the amount of uncertainty or randomness in that given probability distribution, which is useful for encoding information or measuring the level of disorder in a system. The purpose and interpretation are different in those cases, but the underlying similarity is the quantification of disorder, which justifies using the term entropy.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>It seems that in machine learning, NumPy, and many other mathematical software programs, $\log$ refers to the natural logarithm with base $e$, which I adopted for this post. It is confusing because, in information theory, $\log$ commonly refers to base 2. But of course <a href=https://en.wikipedia.org/wiki/Logarithm#Particular_bases target=_blank rel=noopener>everybody knows that</a> $\log$ is base 10, and for the other stuff, you either write $\ln$ for base $e$ or $\log_2$ for base 2. In short, use your $\log$ with caution.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>You might think that the double transpose in <code>np.dot(dL.T,x).T</code> is unnecessary. Why not write it as <code>np.dot(x.T, dL)</code>? It turns out that the former dot product is faster because of the way the data is stored in memory.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>These are actually iterations. The term epochs alludes to a bright future where you might want to process your training data in batches. Batch processing is among the many directions you might want to expand the code.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=article-tags><a class="badge badge-light" href=/tag/machine-learning/>machine learning</a>
<a class="badge badge-light" href=/tag/optimization/>optimization</a>
<a class="badge badge-light" href=/tag/gradient-descent/>gradient descent</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://anilzen.github.io/post/2022/learning-machine-learning/&amp;text=Learning%20Machine%20Learning" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://anilzen.github.io/post/2022/learning-machine-learning/&amp;t=Learning%20Machine%20Learning" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Learning%20Machine%20Learning&amp;body=https://anilzen.github.io/post/2022/learning-machine-learning/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://anilzen.github.io/post/2022/learning-machine-learning/&amp;title=Learning%20Machine%20Learning" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Learning%20Machine%20Learning%20https://anilzen.github.io/post/2022/learning-machine-learning/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://anilzen.github.io/post/2022/learning-machine-learning/&amp;title=Learning%20Machine%20Learning" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://anilzen.github.io><img class="avatar mr-3 avatar-circle" src=/authors/anil/avatar_hubdbe8f760004904350d82355aac46df7_167987_270x270_fill_q75_lanczos_center.jpg alt="Anıl Zenginoğlu"></a><div class=media-body><h5 class=card-title><a href=https://anilzen.github.io>Anıl Zenginoğlu</a></h5><h6 class=card-subtitle>Assistant Research Scientist</h6><p class=card-text>Research scientist and administrator exploring spacetime infinity.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:anil@umd.edu><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/anilzen/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://scholar.google.com/citations?user=M8NnUIQAAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/anilzen target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://orcid.org/0000-0001-7896-6268 target=_blank rel=noopener><i class="fab fa-orcid"></i></a></li><li><a href=https://twitter.com/AnilZenginoglu target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://mathstodon.xyz/@anilzen target=_blank rel=noopener><i class="fab fa-mastodon"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Anıl Zenginoğlu. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script><script src=/en/js/wowchemy.min.42010733157c11a71adebfe9bae43355.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.af9327db0521d4a01354bfc8b77a4324.js type=module></script></body></html>