<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Anıl Zenginoğlu</title><link>https://anilzen.github.io/post/</link><atom:link href="https://anilzen.github.io/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Anıl Zenginoğlu</copyright><lastBuildDate>Thu, 23 Nov 2023 00:01:00 +0000</lastBuildDate><image><url>https://anilzen.github.io/media/sharing.jpg</url><title>Posts</title><link>https://anilzen.github.io/post/</link></image><item><title>Excising the universe over the rainbow</title><link>https://anilzen.github.io/post/2023/misner-hyperboloidal/</link><pubDate>Thu, 23 Nov 2023 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2023/misner-hyperboloidal/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Charles_W._Misner" target="_blank" rel="noopener">Charles W. Misner&lt;/a> passed on July 24, 2023, and there was a lovely &lt;a href="https://umdphysics.umd.edu/events/misnersymposium.html" target="_blank" rel="noopener">memorial symposium&lt;/a> at the University of Maryland on November 11, 2023, to celebrate his life. Misner was a prolific scientist who significantly influenced the early developments of general relativity during its first &lt;a href="https://en.wikipedia.org/wiki/History_of_general_relativity#Golden_age" target="_blank" rel="noopener">golden age&lt;/a>. His last three submissions on the arXiv are on hyperboloidal compactification, a rather niche topic in relativity that happens to be my research area, so I wrote this post about it.&lt;/p>
&lt;p>The symposium gave an excellent overview of Misner&amp;rsquo;s work and provided some insight into his character. Misner is most widely known as the &amp;ldquo;M&amp;rdquo; in &lt;a href="https://en.wikipedia.org/wiki/Gravitation_%28book%29" target="_blank" rel="noopener">MTW&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/ADM_formalism" target="_blank" rel="noopener">ADM&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. One cannot possibly discuss every topic that Misner influenced in a single day, so, inevitably, some of his research areas were left out; among them was his work on hyperboloidal methods. Below are Misner&amp;rsquo;s last three submissions on the arXiv.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://arxiv.org/abs/gr-qc/0409073" target="_blank" rel="noopener">Hyperboloidal Slices and Artificial Cosmology for Numerical Relativity&lt;/a> (2004) published as part of &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/5688" target="_blank" rel="noopener">Deserfest&lt;/a>,&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/gr-qc/0512167" target="_blank" rel="noopener">Over the Rainbow: Numerical Relativity beyond Scri+&lt;/a> (2005),&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/gr-qc/0603034" target="_blank" rel="noopener">Excising das All: Evolving Maxwell waves beyond scri&lt;/a> with James R. van Meter and David R. Fiske (2006) published in &lt;a href="https://doi.org/10.1103/PhysRevD.74.064003" target="_blank" rel="noopener">PRD&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Misner&amp;rsquo;s main idea is to add an artificial cosmological constant to the Einstein equations to help with hyperboloidal compactification. Below, I will discuss his idea in two sections.&lt;/p>
&lt;p>First, the &lt;a href="#the-coordinates">coordinates&lt;/a>. Misner proposes a hyperboloidal compactification in the toy model of de Sitter spacetime to demonstrate his idea. The coordinates he proposes are, in my opinion, not particularly useful. We now have a good understanding of hyperboloidal compactification that wasn&amp;rsquo;t available then. I will discuss his coordinates based on our current understanding.&lt;/p>
&lt;p>Second, the &lt;a href="#artificial-cosmological-constant">artificial cosmological constant&lt;/a>. Misner proposes to modify the Einstein equations with a small cosmological constant to stabilize hyperboloidal evolution. This is a very interesting and promising idea that, to my knowledge, has never been tried before.&lt;/p>
&lt;h2 id="the-coordinates">The coordinates&lt;/h2>
&lt;p>The main construction is clearly laid out in Misner&amp;rsquo;s &lt;a href="https://arxiv.org/abs/gr-qc/0409073" target="_blank" rel="noopener">first paper&lt;/a> prepared for the celebration of his long-time collaborator &lt;a href="https://en.wikipedia.org/wiki/Stanley_Deser" target="_blank" rel="noopener">Stanley Deser&lt;/a>. The other papers are commentary, demonstration, and further development.&lt;/p>
&lt;p>Before discussing the coordinates, we&amp;rsquo;ll review our current understanding of hyperboloidal regularization of coordinate singularities at null horizons to put Misner&amp;rsquo;s construction in context.&lt;/p>
&lt;h3 id="hyperboloidal-regularization">Hyperboloidal regularization&lt;/h3>
&lt;p>Take a spherically symmetric, stationary spacetime. For brevity, we&amp;rsquo;ll ignore the spherical part of the spacetime and discuss only the $\{t,r\}$ plane. We write the metric as
$$ ds^2 = - f(r) dt^2 + \frac{1}{f(r)} dr^2. $$
We get Minkowski with $f=1$, Schwarzschild with $f=1-\frac{2M}{r}$, or de Sitter with $ f = 1-\frac{r^2}{L^2}$. There&amp;rsquo;s a (coordinate) singularity where $f$ vanishes: the event horizon of Schwarzschild at $r=2M$ or the cosmological horizon of de Sitter at $r=L$. From the conformal point of view, spatial infinity is also a coordinate singularity, but it&amp;rsquo;s not visible on the metric without compactification.&lt;/p>
&lt;p>After decades of confusion following Schwarzschild&amp;rsquo;s first publication of the Schwarzschild metric, we now know that a suitable re-slicing of spacetime (a different definition of time) resolves such coordinate singularities. There are many different ways to do this, but, especially for numerics, we&amp;rsquo;d like to maintain the time symmetry of the background when introducing a new time function. Otherwise, the coefficients of the transformed metric depend on time. The time symmetry is respected by transformations of the form
$$ \tau = t + h(r).$$
The function $h(r)$ is called the height function because it describes the height of the new coordinate $\tau$ with respect to $t$ when plotted on the $\{t,r\}$ plane. This transformation leaves the time derivatives invariant as intended: $\partial_t = \partial_\tau$. The metric becomes
$$ ds^2 = - f d\tau^2 + 2 f H \ d\tau dr + \frac{1-f^2 H^2}{f} dr^2, $$
where we have defined $H:=dh/dr$. Hyperboloidal regularization refers to choosing a suitable $H$ such that the $rr$-component of the above metric is regular at the zeros of $f$. This requirement implies $fH \sim \pm 1$ at those zeros. To ensure that the new coordinate $\tau$ is a time coordinate with spacelike level sets, we require $|fH|&amp;lt;1$ away from the zeros of $f$. The sign of $H$ determines the orientation with respect to the null cone: whether we resolve the white hole or the black hole, past null infinity or future null infinity. If you compare the time transformation above with the null coordinates, $u=t-r$ and $v=t+r$, you can see that the slicing resembles outgoing null rays, $u$, for $H&amp;lt;0$ and ingoing null rays, $v$, for $H&amp;gt;0$.&lt;/p>
&lt;p>This, in a nutshell, is our modern understanding of hyperboloidal regularization. The name &amp;ldquo;hyperboloidal&amp;rdquo; comes from the asymptotic similarity of this construction to the behavior of the standard hyperboloid in suitable coordinates:
$$\tau = t - \sqrt{1+r^2}. \tag{1} \label{1} $$
To see that this is a hyperboloid, think of it as $(\tau-t)^2 - r^2 = 1$. For large $r$, the hyperboloidal coordinate $\tau$ approaches the null coordinate $u$ but has spacelike level sets everywhere. The defining property of hyperboloidal coordinates is the spacelike regularization of coordinate singularities at null horizons. I wrote more about this in the context of &lt;a href="https://anilzen.github.io/post/hyperbolic-relativity/">hyperbolic geometry for special relativity&lt;/a> or &lt;a href="https://anilzen.github.io/post/2023/splitting-spacetime/">splitting spacetime into space and time&lt;/a>.&lt;/p>
&lt;p>In asymptotically flat spacetimes, hyperboloidal regularization allows us to fix future null infinity at a constant spatial coordinate (&lt;a href="../../../publication/zenginoglu-2008-hyperboloidal/">scri-fixing&lt;/a>). In black hole spacetimes, it leads to horizon-penetrating coordinates crucial for the excision of black hole interiors. In cosmological spacetimes, it allows us to perform numerical simulations, including the cosmological horizon. The cosmological case is what Misner discusses in his papers.&lt;/p>
&lt;h3 id="de-sitter-spacetime">de Sitter spacetime&lt;/h3>
&lt;p>We get de Sitter spacetime with&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>
$$ f = 1 - \frac{r^2}{L^2}. $$
As mentioned above, there is a coordinate singularity at the cosmological horizon, $r=L$, where $f$ vanishes. The hallmark of the coordinate singularity is intersecting time slices in the conformal diagram&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>.
&lt;figure id="figure-conformal-diagram-of-de-sitter-spacetime-with-t-slices-depicted-on-the-domain-of-influence-the-slices-intersect-at-a-single-point-on-the-cosmological-horizon">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Standard de Sitter" srcset="
/post/2023/misner-hyperboloidal/standard_hu06d267e0cb33895da239e2c52bc66331_90656_6dae36c8dbb990f1d3af45c6aa4617d9.webp 400w,
/post/2023/misner-hyperboloidal/standard_hu06d267e0cb33895da239e2c52bc66331_90656_30f7e5a25899d46e272b57b48e8765a4.webp 760w,
/post/2023/misner-hyperboloidal/standard_hu06d267e0cb33895da239e2c52bc66331_90656_1200x1200_fit_q75_h2_lanczos_2.webp 1200w"
src="https://anilzen.github.io/post/2023/misner-hyperboloidal/standard_hu06d267e0cb33895da239e2c52bc66331_90656_6dae36c8dbb990f1d3af45c6aa4617d9.webp"
width="733"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Conformal diagram of de Sitter spacetime with $t$-slices depicted on the domain of influence. The slices intersect at a single point on the cosmological horizon.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>We want to construct a hyperboloidal regularization of the future cosmological horizon. The null cone structure is given by $u=t-r_\ast$ and $v=t+r_\ast$ with $r_\ast = \int \frac{dr}{f} = L \ \mathrm{archtanh }\ \frac{r}{L}$. We are primarily interested in the domain up to the cosmological horizon, $r\leq L$. Here&amp;rsquo;s a suitable choice that satisfies our two requirements ($fH \to - 1$ as $r\to L$ and $|fH|&amp;lt;1$ for $r&amp;lt;L$):
$$ f H = - \frac{r^2}{L^2}.$$
By integration, we get the new time coordinate
$$\tau = t + r - L \ \mathrm{archtanh }\ \frac{r}{L}.$$
The metric becomes
$$ ds^2 = - f d\tau^2 - \frac{2 r^2}{L^2} d\tau dr + \left(1+\frac{r^2}{L^2}\right) dr^2. $$
This metric is particularly nice because it can be written in Kerr-Schild form&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> as the sum of the Minkowski metric and a scaled null field
$$ ds^2 = - d\tau^2 + dr^2 + \frac{r^2}{L^2} \left(d\tau - dr\right)^2. $$
The singularity at the cosmological horizon $r=L$ is resolved. This construction is hyperboloidal and allows us to perform numerical calculations including the cosmological horizon. The time slices resolve the coordinate singularity in the previous conformal diagram. Instead, we obtain a smooth foliation of the cosmological horizon, as seen below. The diagram also demonstrates that Kerr-Schild coordinates beyond the horizon resemble trumpet slices in black hole spacetimes.&lt;/p>
&lt;p>
&lt;figure id="figure-kerr-schild-slicing-provides-a-hyperboloidal-foliation-of-the-cosmological-horizon-the-slices-approach-timelike-infinity-beyond-the-horizon-resembling-trumpet-slices-in-black-hole-spacetimes">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Kerr-Schild de Sitter" srcset="
/post/2023/misner-hyperboloidal/featured_hu61f272e7fd7b50e750389b4fdc66f51f_102036_4144df01cc22e81d197a4dd6cdaa6943.webp 400w,
/post/2023/misner-hyperboloidal/featured_hu61f272e7fd7b50e750389b4fdc66f51f_102036_478889429f38d755583f15423b06120b.webp 760w,
/post/2023/misner-hyperboloidal/featured_hu61f272e7fd7b50e750389b4fdc66f51f_102036_1200x1200_fit_q75_h2_lanczos_2.webp 1200w"
src="https://anilzen.github.io/post/2023/misner-hyperboloidal/featured_hu61f272e7fd7b50e750389b4fdc66f51f_102036_4144df01cc22e81d197a4dd6cdaa6943.webp"
width="733"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Kerr-Schild slicing provides a hyperboloidal foliation of the cosmological horizon. The slices approach timelike infinity beyond the horizon resembling trumpet slices in black-hole spacetimes.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Misner takes the Kerr-Schild form of the de Sitter metric as a starting point and adds another time transformation
$$ T = \tau - \sqrt{1+r^2}. $$
This is what he calls hyperboloidal. The transformation is formally that of the standard hyperboloid as in Eq. \eqref{1}, but applying this in the geometry of de Sitter spacetime is awkward. By our discussion above, this transformation is not hyperboloidal. The point of the standard hyperboloid is that asymptotically in Minkowski spacetime (for large $r$), the transformed time coordinate approaches the outgoing null cone: $T\sim t - r$. However, the outgoing null cone in de Sitter spacetime does not have this form; therefore, the transformation is not hyperboloidal with respect to the actual background. As the time transformation doesn&amp;rsquo;t respect the underlying causal structure of de Sitter spacetime, it does nothing to regularize an already regular coordinate system. Nevertheless, let&amp;rsquo;s write down the metric
$$ ds^2 = -f d\tau^2 - 2 \left( \frac{r}{L^2} + \frac{f}{\sqrt{1+r^2}} \right) d\tau dr $$
$$ + \frac{L^2 + r^2 + 2 r^4 - 2r^3\sqrt{1+r^2}}{L^2 (1+r^2)} dr^2. $$&lt;/p>
&lt;p>The only impact of this transformation on the asymptotic behavior of the time slices is when $L=1$ or when the coefficient of $r$ in the hyperboloid matches the cosmological radius. In this case, the time slices end at future spacelike infinity, $\mathscr I^+$, instead of future timelike infinity, $i^+$.
&lt;figure id="figure-misner-slicing-also-provides-a-hyperboloidal-foliation-of-the-cosmological-horizon-the-main-difference-to-the-kerr-schild-slicing-is-that-beyond-the-horizon-the-slices-hit-mathscr-i-when-l1">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Misner de Sitter" srcset="
/post/2023/misner-hyperboloidal/misner_hue932acea94ee83a0328abf4843b27fa3_80876_cd49ed438ef414a6bf4e02c99f73f896.webp 400w,
/post/2023/misner-hyperboloidal/misner_hue932acea94ee83a0328abf4843b27fa3_80876_1f21af67d63242fe6ea9826f43192960.webp 760w,
/post/2023/misner-hyperboloidal/misner_hue932acea94ee83a0328abf4843b27fa3_80876_1200x1200_fit_q75_h2_lanczos_2.webp 1200w"
src="https://anilzen.github.io/post/2023/misner-hyperboloidal/misner_hue932acea94ee83a0328abf4843b27fa3_80876_cd49ed438ef414a6bf4e02c99f73f896.webp"
width="733"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Misner slicing also provides a hyperboloidal foliation of the cosmological horizon. The main difference to the Kerr-Schild slicing is that, beyond the horizon, the slices hit $\mathscr I^+$ when $L=1$.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Next, Misner introduces the spatial compactification
$$ r = \frac{\rho}{1-\left(\frac{\rho}{2}\right)^2}. $$
Remember that future infinity, $\mathscr I^+$, is spacelike in cosmological spacetimes. The compactification maps $\mathscr I^+$ to $\rho=2$. But no compactification is needed if we&amp;rsquo;re interested in the cosmological horizon, which is at a finite distance. One can consider $r$ already as a compactification of the tortoise coordinate $r_*$. In the case of de Sitter, the cosmological horizon at $r=L$ acts as the ingoing null surface representing a far-away observer. We don&amp;rsquo;t need to access spacelike infinity, so the compactification seems unnecessary. Compactification might be useful for very large $L$, but even then, what we need is not necessarily a compactification but a mapping of the cosmological horizon to a coordinate sphere whose location is independent of the cosmological constant.&lt;/p>
&lt;p>That&amp;rsquo;s all I have to say about the coordinates. There are better choices if you want to evolve fields accurately in asymptotically de Sitter spacetimes. But Misner&amp;rsquo;s goal is not to evolve fields on the background of asymptotically de Sitter spacetimes. He aims to evolve the Einstein equations to construct astrophysically relevant asymptotically flat spacetimes using an &lt;em>artificial&lt;/em> cosmological constant. And that&amp;rsquo;s a very promising idea.&lt;/p>
&lt;h2 id="artificial-cosmological-constant">Artificial cosmological constant&lt;/h2>
&lt;p>Let&amp;rsquo;s say we are interested in solving the Einstein equations all the way up to and including future null infinity. To include infinity, we solve the equations for a conformally rescaled metric $g= \Omega^2 \tilde{g}$. The conformal Einstein equations read
$$ R_{\mu\nu} - \frac{1}{2} g_{\mu\nu} R = T_{\mu\nu}[\Omega], $$
with conformal source terms
$$ T[\Omega] = -\frac{2}{ {\color{darkred}\Omega}} \left(\nabla_\mu \nabla_\nu \Omega - \Box \Omega g_{\mu\nu}\right) - \frac{3}{ {\color{darkred}\Omega^2} } \nabla_\lambda \Omega \nabla^\lambda \Omega g_{\mu\nu}. $$
The conformal factor vanishes at infinity: $\Omega=0$. If we want to include the conformal boundary in our numerical computations, we must somehow deal with the formal singularity of these terms at $\Omega=0$. I write &amp;ldquo;formal singularity&amp;rdquo; because one can show that the terms are regular under reasonable assumptions. However, it&amp;rsquo;s hard to impose that regularity on hyperboloidal surfaces while keeping the evolution numerically stable.&lt;/p>
&lt;p>Misner&amp;rsquo;s idea is to add a small cosmological term to the equations
$$ R_{\mu\nu} - \frac{1}{2} g_{\mu\nu} R {\color{darkgreen} - \frac{3}{L^2 \Omega^2} g_{\mu\nu}} = T_{\mu\nu}[\Omega]. $$
For large $L$, this term can be as small as we want as long as $\Omega&amp;gt;0$. However, the global causal structure of spacetime is dramatically different at $\Omega=0$. Infinity is not a null surface anymore; it&amp;rsquo;s a spacelike surface! Even the definition of gravitational radiation becomes difficult and is an &lt;a href="https://arxiv.org/abs/1409.3816" target="_blank" rel="noopener">ongoing research topic&lt;/a>.&lt;/p>
&lt;p>Misner&amp;rsquo;s insight is that we don&amp;rsquo;t have to evolve the fields until the conformal boundary. It&amp;rsquo;s sufficient to go slightly beyond the cosmological horizon and stop at a spacelike surface. In Misner&amp;rsquo;s approach, &lt;strong>the conformal factor does not vanish anywhere on the grid&lt;/strong>! This method mimics the excision method inside a black-hole (apparent) horizon. For large $L$, the resulting radiation field should match the radiation field in the asymptotically flat case. We consider the cosmological constant an artificial term, a numerical trick, allowing us to apply excision beyond the cosmological horizon. This is the core idea of Misner&amp;rsquo;s approach: excise the unobservable universe beyond the cosmological horizon.&lt;/p>
&lt;h3 id="potential-advantages">Potential advantages&lt;/h3>
&lt;p>Theoretically, this approach provides all the benefits of hyperboloidal compactification: improved efficiency, no artificial boundary conditions, and no radiation extrapolation problem. In addition, one can expect improved stability due to the artificial cosmological constant. There are some caveats that I&amp;rsquo;ll mention; I think they&amp;rsquo;re sufficiently mild for practical numerical work.&lt;/p>
&lt;h4 id="efficiency">Efficiency&lt;/h4>
&lt;p>In the asymptotically flat scenario, the numerical efficiency comes from the time transformation that stretches the spatial wavelength of outgoing waves. This wave-stretching, or redshift, is then combined with compactification to resolve oscillatory solutions on infinite domains.&lt;/p>
&lt;p>In the cosmological scenario, the waves are also stretched, but the interpretation of wave-stretching is now due to the expansion of space and is called cosmological redshift. The &lt;strong>caveat&lt;/strong> is that the cosmological redshift is a physical effect. The cosmological constant changes the waveform physically. I expect that the impact of a sufficiently small cosmological constant will be negligible on the waveform but significant on numerical efficiency.&lt;/p>
&lt;h4 id="no-boundary-conditions">No boundary conditions&lt;/h4>
&lt;p>In the asymptotically flat scenario, no boundary conditions are needed because one places the outer boundary at future null infinity, which is an ingoing null surface. No signals can propagate faster than the speed of light, so no characteristics can enter the numerical domain.&lt;/p>
&lt;p>In the cosmological scenario, the outer boundary is spacelike. This is even better than an ingoing null boundary. A spacelike boundary will pull away any perturbations and further stabilize the code. This setup emulates excision at the black hole horizon. Consider the conformal diagrams above and think of spacelike infinity as the spacelike black-hole singularity and the cosmological horizon as the event horizon.&lt;/p>
&lt;h4 id="no-radiation-extrapolation">No radiation extrapolation&lt;/h4>
&lt;p>In the asymptotically flat scenario, gravitational waves are defined unambiguously at future null infinity. With the numerical outer boundary placed at future null infinity, one can directly read off the asymptotic fields and perform the relatively simple transformations to extract the gravitational radiation without any error-prone extrapolation procedures.&lt;/p>
&lt;p>In the cosmological scenario, one would read off the radiation at the cosmological horizon. For large $L$, this radiation should match the radiation at future null infinity. Again, no extrapolation would be needed because the cosmological horizon is part of the simulation domain. The &lt;strong>caveat&lt;/strong> is that there is no unambiguous (coordinate-independent) definition of gravitational radiation at the cosmological horizon. In fact, the horizon itself depends on the coordinate system, even in pure de Sitter spacetime. We&amp;rsquo;ll need to understand this problem better once we have stable simulations. Note that the cosmological constant in our Universe is non-vanishing, so this problem is physically interesting.&lt;/p>
&lt;h4 id="improved-stability">Improved stability&lt;/h4>
&lt;p>It is well-known that de Sitter spacetimes are &amp;ldquo;more stable&amp;rdquo; than asymptotically flat spacetimes. Friedrich demonstrated the stability of asymptotically de Sitter spacetimes in 1986, years before the stability of Minkowski spacetimes was proven. We also know from &lt;a href="https://arxiv.org/abs/1104.3702" target="_blank" rel="noopener">Bizoń and Rostworowski&lt;/a> that AdS with no-flux boundary conditions is globally unstable. The flat case is, in some imprecise sense, at the edge of stability with respect to the cosmological constant. A positive constant moves us towards better stability.&lt;/p>
&lt;h2 id="beyond-the-horizon-there-be-monsters">Beyond the horizon there be monsters&lt;/h2>
&lt;p>Hyperboloidal evolution of astrophysically relevant phenomena (binary black holes, neutron stars) is an open problem in numerical relativity. Today, the effort is driven mostly by David Hilditch and Alex Vañó-Viñuales. Since I gave up on this hard problem in 2013, this is the most exciting idea that I haven&amp;rsquo;t tried yet. Exciting because I can&amp;rsquo;t think of a reason why this shouldn&amp;rsquo;t work. It&amp;rsquo;s certainly worth trying out for a few months.&lt;/p>
&lt;p>It would be remarkable if Misner&amp;rsquo;s suggestion would lead to a resolution of this important open problem. Can&amp;rsquo;t wait to see the outcome.&lt;/p>
&lt;!-- The biggest frustration with this problem for me is that it's a hard binary: it works or it doesn't. And when it doesn't work, you can't do anything incremental to improve it. This idea is different. We know that evolution with a cosmological constant will work. Once we have a stable code, we can improve it bit by bit using smaller cosmological constants, better coordinates, improved extraction schemes, etc. -->
&lt;!-- One of the most influential events on my career was the funeral of [Jürgen Ehlers](https://en.wikipedia.org/wiki/J%C3%BCrgen_Ehlers), another giant of general relativity who was the founding director of the Max-Planck Institute for Gravitation where I did my Ph.D. thesis. -->
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>MTW is named after Misner, Thorne, and Wheeler, who are the authors of the book &lt;a href="https://en.wikipedia.org/wiki/Gravitation_%28book%29" target="_blank" rel="noopener">Gravitation&lt;/a>. ADM stands for Arnowitt, Deser, and Misner. We learned in the memorial symposium that Deser and Misner were both invited by Niels Bohr to the institute known as Niels Bohr Institute today. They worked in a school building on an island near Copenhagen during the summer of their visit, kneeling on the low blackboard designed for school children to make their calculations.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The coordinates we use here cover only part of the de Sitter spacetime called the static patch. There are many other coordinate systems that don&amp;rsquo;t exhibit a singularity at the cosmological horizon. For example, &lt;a href="https://en.wikipedia.org/wiki/De_Sitter_space#Closed_slicing" target="_blank" rel="noopener">global coordinates&lt;/a> are regular everywhere and can be used to construct conformal diagrams. However, the static patch is helpful for numerical calculations because the time symmetry is explicit in the coordinates.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>If you&amp;rsquo;re interested in how to draw such conformal diagrams, see my &lt;a href="https://anilzen.github.io/post/2022/drawing-penrose-diagrams">blog post&lt;/a>.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>The Kerr-Schild form of a metric is $g_{\mu\nu} = \eta_{\mu\nu} + F\ k_\mu k_\nu$, where $\eta_{\mu\nu}$ is the Minkowski metric and $k_\mu$ is a null vector.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Splitting Spacetime</title><link>https://anilzen.github.io/post/2023/splitting-spacetime/</link><pubDate>Wed, 20 Sep 2023 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2023/splitting-spacetime/</guid><description>&lt;h2 id="predicting-the-future">Predicting the future&lt;/h2>
&lt;p>Contrary to common belief, physicists are in the business of making predictions more than fortune tellers or astrologers. The hows and whys, the fundamentals and principles, &amp;ldquo;whether or not the world has three dimensions,&amp;rdquo; all boil down to one question: Can you predict the future?&lt;/p>
&lt;p>Predicting the future for a physicist is, of course, a much more accurate undertaking than for fortune tellers, astrologers, or &lt;a href="https://www.yahoo.com/news/finance/news/wall-street-economists-consistently-wrong-171740454.html" target="_blank" rel="noopener">economists&lt;/a>. We want to know what happens to well-defined variables describing the mathematical idealization of a natural system given a snapshot in time&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We call this process &amp;ldquo;solving an initial value problem.&amp;rdquo; We solve an equation of motion, typically a partial differential equation (PDE), with prescribed initial data. The PDE is the mathematical idealization of our model of the physical system under consideration. That&amp;rsquo;s where all the whys and hows go. The PDE describes how the chosen variables of the system vary in relation to each other. The initial data is the snapshot of these variables at a given time. The solution to this initial value problem is our prediction of the future.&lt;/p>
&lt;p>The language of a snapshot in time distinguishes time and space. But didn&amp;rsquo;t special relativity teach us that time by itself and space by itself are mere shadows? Well, it did, and it didn&amp;rsquo;t. Special relativity taught us that the notion of time and simultaneity is observer-dependent. But time is still a special dimension in a relativistic spacetime, distinct from its spatial friends. You can already see at the level of the metric signature, $(-+++)$, that time is special.&lt;/p>
&lt;p>To formulate an initial value problem, we need to split spacetime into space and time. In relativity, there are three approaches to splitting spacetime to formulate an initial value problem: Cauchy, characteristic, and hyperboloidal. Let&amp;rsquo;s take the flat, 1+1 dimensional spacetime to demonstrate these approaches. In standard coordinates $(t,r)$, the flat metric reads
$$ ds^2 = -dt^2 + dr^2 \tag{1} \label{1} $$
The most intuitive approach is to take the snapshot at $t=0$. The corresponding surface&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> is called a &lt;a href="https://en.wikipedia.org/wiki/Cauchy_surface" target="_blank" rel="noopener">Cauchy surface&lt;/a> and has specific desirable properties that generalize to more interesting cases. The associated &lt;a href="https://ems.press/books/esi/66" target="_blank" rel="noopener">Cauchy problem&lt;/a> plays a central role in mathematical and numerical relativity.&lt;/p>
&lt;p>We also have null surfaces in relativity, named as such because they provide the &lt;a href="https://en.wikipedia.org/wiki/Kernel_%28linear_algebra%29" target="_blank" rel="noopener">nullspace&lt;/a> of the metric \eqref{1} through $t\pm r$. This null cone is fundamental in many aspects. First, it determines the &lt;a href="https://plato.stanford.edu/entries/spacetime-singularities/lightcone.html" target="_blank" rel="noopener">causal structure of spacetime&lt;/a>. Second, it describes the propagation of disturbances. In particular, massless fields propagate along the null cone. Even before the advent of relativity, it was known that disturbances to solutions of wave equations propagate along these special surfaces called &lt;a href="https://en.wikipedia.org/wiki/Method_of_characteristics" target="_blank" rel="noopener">characteristics&lt;/a>. One can provide data on such characteristic surfaces and solve a &lt;a href="https://link.springer.com/article/10.12942/lrr-2012-2" target="_blank" rel="noopener">characteristic evolution problem&lt;/a>. This approach is particularly valuable in the modern era of gravitational-wave astronomy because gravitational waves are only defined at infinity along outgoing null directions.&lt;/p>
&lt;h2 id="hyperboloidal-surfaces">Hyperboloidal surfaces&lt;/h2>
&lt;p>The hyperboloidal approach is the odd one. You take a spacetime hyperboloid, say
$t^2 - r^2 = 1,$
and give initial data on the surface defined by
$t(r) = \sqrt{1+r^2}.$
This construction seems weird and arbitrary when you first encounter it. Why would you do something like this? How is this surface a &lt;em>snapshot&lt;/em>? Why not pick another transformation?&lt;/p>
&lt;p>Many relativists feel that hyperboloidal surfaces are somewhat unnatural, lacking the simplicity and elegance of the Cauchy or characteristic approaches. Below, I demonstrate how hyperboloidal surfaces extend the mathematical notion of a sphere to relativity and satisfy a physical principle. The concepts that we&amp;rsquo;ll encounter will lead us to speculations in the quantum realm.&lt;/p>
&lt;h3 id="mathematical-notion">Mathematical notion&lt;/h3>
&lt;p>&lt;em>Hyperboloids as analogs of spheres in spacetime.&lt;/em>&lt;/p>
&lt;p>
&lt;figure id="figure-a-spacetime-hyperboloid-times-up">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./hyperboloid.png" alt="Hyperboloid" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
A spacetime hyperboloid. Time&amp;rsquo;s up.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>A circle is a set of points equidistant from a given point. This definition is among the purest in mathematics&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. To derive the equation for a circle, consider the Euclidean metric in two dimensions
$$ ds^2 = dx^2 + dy^2. $$
The set of points $\{x,y\}$ at distance $R$ from the origin satisfies
$$ x^2 + y^2 = R^2. $$
The circle (or sphere in three dimensions) is the simplest nontrivial example of a constant curvature surface, with its curvature being the reciprocal of its radius, $K=1/R$. Now, let&amp;rsquo;s consider the corresponding object in relativity.&lt;/p>
&lt;p>&lt;strong>What is the set of events equidistant from a given event?&lt;/strong>&lt;/p>
&lt;p>In the relativistic setting, spacetime distance is proper time. Using the metric \eqref{1}, the set of events $\{t, r\}$ at proper time $T$ from the origin satisfies
$$ - t^2 + r^2 = \pm T^2. $$
This equation describes a spacetime hyperbola. The sign in the equation determines whether the hyperbola is spacelike or timelike&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. We&amp;rsquo;ll pick the negative sign for the spacelike hyperbola because we aim to construct a snapshot. I mentioned before that time is special, so let&amp;rsquo;s solve the above equation for time to obtain the future sheet of the spacelike hyperbola
$$ t(r) = \sqrt{T^2+r^2}.$$
This spacelike surface consists of points at proper time $T$ from the origin. Like the circle above, this surface has constant curvature, $K=1/T.$ The higher dimensional version of a hyperbola is called the hyperboloid.&lt;/p>
&lt;p>We see that hyperboloids are the relativistic analogs of spheres. Hyperboloids are as natural in Lorentzian manifolds as spheres are in Riemannian manifolds.&lt;/p>
&lt;h3 id="physical-principle">Physical principle&lt;/h3>
&lt;p>&lt;em>Hyperboloids as analogs of soap bubbles in spacetime.&lt;/em>&lt;/p>
&lt;p>
&lt;figure id="figure-a-soap-bubble-cluster-by-kym-coxhttpswwwkymcoxcomwork-statement-exhibitions">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./featured.jpg" alt="Soap bubble cluster" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
A soap bubble cluster. By &lt;a href="https://www.kymcox.com/work-statement-exhibitions" target="_blank" rel="noopener">Kym Cox&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The circle has another attractive property: among all figures with a given area, the circle has the smallest perimeter. We&amp;rsquo;ve known this for over 2,000 years&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>. The corresponding statement for the sphere was only proven in the 19th century by &lt;a href="https://en.wikipedia.org/wiki/Hermann_Schwarz" target="_blank" rel="noopener">Schwarz&lt;/a>. Such optimization problems appear in many areas. One that is familiar to most people from early childhood is the soap bubble.&lt;/p>
&lt;p>In mathematical terms, soap bubbles are minimal surfaces enclosing a fixed volume of air. The variational problem has the following action
$$ S = A - \lambda \cdot V. \tag{2} \label{2} $$
Minimize the area $A$ with the constraint that the volume $V$ is fixed. The mathematics of soap bubbles includes fascinating open problems with many &lt;a href="https://www.quantamagazine.org/monumental-math-proof-solves-triple-bubble-problem-and-more-20221006/" target="_blank" rel="noopener">recent developments&lt;/a>. Frank Morgan, who proved the &lt;a href="https://en.wikipedia.org/wiki/Double_bubble_theorem" target="_blank" rel="noopener">double bubble theorem&lt;/a>, has a wonderful lecture on the topic.&lt;/p>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CbpfpxOdUzU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;br>
Here&amp;rsquo;s a particularly relevant quote from the lecture&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;If you want to understand the universe, start out by understanding the soap bubble.&amp;rdquo;&lt;/p>
&lt;p>Frank Morgan&lt;/p>
&lt;/blockquote>
&lt;p>Let&amp;rsquo;s try to understand the universe by understanding the soap bubble. We must first answer the following question as we did with the circle:&lt;/p>
&lt;p>&lt;strong>What is the relativistic analog of a soap bubble?&lt;/strong>&lt;/p>
&lt;p>Formally, we can use the action for the soap bubble \eqref{2} and go one dimension higher.
$$ S = V - \lambda \cdot W. \tag{3}\label{3} $$
Now, the task is to &lt;em>maximize the volume&lt;/em> $V$ with the constraint that the &lt;em>spacetime volume&lt;/em> $W$ is fixed. In contrast to the soap bubble problem, we&amp;rsquo;re not looking for a minimal hypersurface in the relativistic setup but a maximal one. The minimal hypersurface would be null with vanishing volume.&lt;/p>
&lt;p>Let&amp;rsquo;s see the solution of this problem in a simple case. We compute the relativistic volume by integrating the square root of the determinant of the metric&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>. Denoting the determinant of the induced metric on the hypersurface by $h$ and the determinant of the spacetime metric by $g$, we have
$$ S = \int \sqrt{h}\ d^3x - \lambda \int \sqrt{-g}\ d^4x. $$
The extremization of this action leads to the requirement of constant mean curvature: $K = \lambda$, where $K$ is the trace of the extrinsic curvature&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>. We&amp;rsquo;ll do this for the 1+1 Minkowski metric to find an explicit expression for the maximizer. We&amp;rsquo;re looking for an embedding, $t=t(r)$, that maximizes the action for the relativistic bubble. The induced metric is
$$ ds^2 = -dt^2 + dr^2 = \left(-t&amp;rsquo;^2 + 1\right) dr^2, $$
where $t&amp;rsquo;\equiv dt(r)/dr$. The volume from the origin to some radius $R$ is
$$ V = \int_0^R \sqrt{h} \ dr = \int_0^R \sqrt{1-t&amp;rsquo;^2}\ dr. $$
To calculate the spacetime volume, we need a reference surface. It turns out that it doesn&amp;rsquo;t matter which surface, so let&amp;rsquo;s just take the surface $t=0$. The spacetime volume is given by
$$ W = \int_0^R \int_0^{t(r)} \sqrt{-g} \ dt dr = \int_0^R t(r) dr. $$
Bringing these expressions together in the action \eqref{3}, we can read off the Lagrangian density
$$ \mathscr{L} = \sqrt{1-t&amp;rsquo;^2} - \lambda t. $$
The Euler-Lagrange equation for varying $t(r)$ with respect to the parameter $r$ reads
$$ \frac{d}{dr} \frac{\partial\mathscr{L}}{\partial t&amp;rsquo;} = \frac{\partial \mathscr{L}}{\partial t} \ \iff &lt;br>
\frac{d}{dr} \frac{t&amp;rsquo;}{\sqrt{1-t&amp;rsquo;^2}} = \lambda. $$
We integrate by $r$ and solve for $t&amp;rsquo;$ with the boundary condition $t&amp;rsquo;(0)=0$
$$ t&amp;rsquo;(r) = \pm \frac{\lambda r}{\sqrt{1+ \lambda^2 r^2}}. $$
By choosing the positive sign and integrating again, we get the equation for the spacetime hyperboloid
$$ t(r) = \sqrt{\frac{1}{\lambda^2} + r^2}. $$
We recognize the mean extrinsic curvature as $K=\lambda$ from our previous discussion on the relativistic analog of a circle. We now see that the hyperboloid is the relativistic analog of a soap bubble.&lt;/p>
&lt;p>You may wonder what determines the value of the constant mean curvature. It&amp;rsquo;s the spacetime volume that we prescribe in the constraint. The larger the spacetime volume to be enclosed, the smaller the constant mean curvature. You can see this behavior in the Penrose diagram below. As I mentioned above, the reference surface doesn&amp;rsquo;t matter for the qualitative discussion. You can pick $t=0$ or $u=t-r=0$. The spacetime volume enclosed by the hyperboloid with $K=0.5$ is larger than the spacetime volume enclosed by the one with $K=2$ in both cases. The difference is shaded yellow.&lt;/p>
&lt;p>
&lt;figure id="figure-smaller-curvature-k-corresponds-to-larger-spacetime-volume-irrespective-of-the-reference-surface">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./tikz_figs/K_volume_t.webp" alt="Smaller curvature $K$ corresponds to larger spacetime volume irrespective of the reference surface." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Smaller curvature $K$ corresponds to larger spacetime volume irrespective of the reference surface.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>This behavior is actually very intuitive, as it&amp;rsquo;s the same for a soap bubble. The larger the volume of air that the bubble needs to enclose, the larger the bubble, and therefore the smaller the curvature of the bubble.&lt;/p>
&lt;p>In summary, there&amp;rsquo;s a clear analogy between soap bubbles and hyperboloids. Soap bubbles are minimal surfaces enclosing a fixed volume of air. Hyperboloids are maximal surfaces enclosing a fixed volume of spacetime.&lt;/p>
&lt;h2 id="quantum-foam">Quantum foam&lt;/h2>
&lt;p>What would the extremization of the action \eqref{3} mean physically? Does space arise from the extremization of spacetime bubbles?&lt;/p>
&lt;p>It has been long predicted that spacetime shouldn&amp;rsquo;t be smooth at the Planck scale due to quantum fluctuations. It should be &amp;ldquo;foamy,&amp;rdquo; or &amp;ldquo;bubbly,&amp;rdquo; or &amp;ldquo;grainy,&amp;rdquo; or whatever you want to call it. Wheeler, the naming authority of theoretical physics&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup>, called this the &lt;a href="https://en.wikipedia.org/wiki/Quantum_foam" target="_blank" rel="noopener">quantum foam&lt;/a>. A foam is defined as a dispersion of bubbles, and we have just seen that the relativistic generalization of a bubble is a hyperboloid.&lt;/p>
&lt;p>We arrived at the realm of speculation. We don&amp;rsquo;t know how to test the quantum foam idea experimentally. But we can imagine that quantum foam arises from quantum fluctuations of spacetime bubbles. The hypersurfaces that enclose those bubbles are hyperboloids. As usual, this mental image gives rise to further questions. What is the mechanism for the formation of such hyperboloids? How are they related to quantum fields?&lt;/p>
&lt;p>One idea is that the hypersurfaces arise through entanglement. I&amp;rsquo;ve written about &lt;a href="../../hyperboloidal-holography/">hyperboloidal holography&lt;/a> before, where I argued that holographic entanglement entropy naturally extends to flat spacetime if we use hyperboloids as time slices. The variational principle suggests that these surfaces could also be useful in the context of &lt;a href="https://www.quantamagazine.org/in-new-paradox-black-holes-appear-to-evade-heat-death-20230606/" target="_blank" rel="noopener">quantum complexity&lt;/a>.&lt;/p>
&lt;p>These speculations open more questions than answers. Nevertheless, we now have a better sense that soap bubbles and their clusters may indeed provide a key to understanding spacetime.&lt;/p>
&lt;hr>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./foam.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>To be fair, not all physical theories focus on snapshots in time. There are theories formulated in terms of statistical ensembles, boundary value problems, or, as discussed in this post, variational principles. But there is a sense in which all these also arise from initial value problems under certain conditions.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>I&amp;rsquo;ll sometimes refer to lines as (hyper)surfaces and circles as spheres. I&amp;rsquo;m demonstrating the mathematics in the simplest case with one spatial dimension, but the concepts generalize trivially to three-dimensional space.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>A fascinating &lt;a href="https://www.newyorker.com/magazine/2022/05/16/the-mysterious-disappearance-of-a-revolutionary-mathematician" target="_blank" rel="noopener">story on Grothendieck&amp;rsquo;s life&lt;/a> mentions his first encounter with this definition as a twelve year old boy at an internment camp in France.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>Timelike hyperbolas describe observers with uniform acceleration, called &lt;a href="https://en.wikipedia.org/wiki/Rindler_coordinates" target="_blank" rel="noopener">Rindler observers&lt;/a>, and play an important role in quantum field theory in curved spacetimes, among others.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>This statement in a slightly different form was first asserted by &lt;a href="https://en.wikipedia.org/wiki/Zenodorus_%28mathematician%29" target="_blank" rel="noopener">Zenodorus&lt;/a>, who stated that a circle has a greater area than any polygon with the same perimeter. You can find a lovely account of the evolution of the isoperimetric problem &lt;a href="https://maa.org/sites/default/files/pdf/upload_library/22/Ford/blasjo526.pdf" target="_blank" rel="noopener">here&lt;/a>.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>To understand how we compute volumes in relativity, read a bit about &lt;a href="https://en.wikipedia.org/wiki/Volume_form" target="_blank" rel="noopener">volume forms&lt;/a>.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>I&amp;rsquo;m not going through the general calculation here to avoid introducing lots of definitions, but the calculation itself is very short and can be found, for example, in the paper by &lt;a href="https://pubs.aip.org/aip/jmp/article-abstract/21/12/2789/444574/K-surfaces-in-the-Schwarzschild-space-time-and-the" target="_blank" rel="noopener">Brill, Cavallo, and Isenberg&lt;/a> from 1980.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/John_Archibald_Wheeler" target="_blank" rel="noopener">John Wheeler&lt;/a> was a physicist who coined many terms that are now part of the standard vocabulary of theoretical physics. The most famous one is &amp;ldquo;black hole.&amp;rdquo; Other expressions initiated by Wheeler include wormhole, quantum foam, geon, it from bit, neutron moderator, and participatory anthropic principle.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Cross-entropy</title><link>https://anilzen.github.io/post/2023/cross-entropy/</link><pubDate>Sun, 02 Apr 2023 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2023/cross-entropy/</guid><description>&lt;p>We&amp;rsquo;re all gonna die.&lt;/p>
&lt;p>Blame the Second Law of Thermodynamics. Entropy increases, we get older, and we die.
&lt;figure id="figure-mark-thompson-new-yorker-magazine">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./entropy_new_yorker_cartoon.jpg" alt="I blame entropy - New Yorker Cartoon" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Mark Thompson, New Yorker Magazine
&lt;/figcaption>&lt;/figure>
This perception of entropy, prevalent in popular culture, presents it as the driving force for decay and disorder. But this cannot be the whole story. The incredible abundance of life on our planet would not have been possible without the self-organization of complex systems. The story of &lt;a href="https://en.wikipedia.org/wiki/Entropy_and_life" target="_blank" rel="noopener">entropy and life&lt;/a> is more complicated than the boy in the above cartoon implies. To understand these larger questions about Life, the Universe, and Everything, we need to first clarify what entropy is.&lt;/p>
&lt;p>This post is about a variant of entropy&amp;mdash;called cross-entropy&amp;mdash;that I wrote about in a post on &lt;a href="../../2022/learning-machine-learning/">machine learning&lt;/a>. There, I presented cross-entropy as a measure of &lt;a href="http://localhost:1313/post/2022/learning-machine-learning/#fn:3" target="_blank" rel="noopener">the difference between two probability distributions&lt;/a>. Most explanations of the concept, including its &lt;a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">Wikipedia entry&lt;/a>, mainly focus on its relevance in information theory, not physics.&lt;/p>
&lt;p>I learned in a &lt;a href="https://www.youtube.com/watch?v=x9COqqqsFtc" target="_blank" rel="noopener">talk&lt;/a> by &lt;a href="https://www.preposterousuniverse.com/" target="_blank" rel="noopener">Sean Carroll&lt;/a> during the &lt;a href="https://qtd-hub.umd.edu/event/symposium-2023/" target="_blank" rel="noopener">Maryland Quantum-Thermodynamics Symposium&lt;/a> that cross-entropy plays a central role in the informational reformulation of the Second Law. This way of thinking about entropy and the Second Law builds a fascinating bridge between machine learning and physics. Before we cross that bridge, let&amp;rsquo;s talk about plain old entropy.&lt;/p>
&lt;h2 id="entropy-without-a-cross">Entropy Without a Cross&lt;/h2>
&lt;p>Entropy is one of the most important concepts in physics. It&amp;rsquo;s the main character of the Second Law of Thermodynamics, which states that the entropy of an isolated system increases over time.&lt;/p>
&lt;p>Despite its importance, entropy is not as widely known or used as energy. Whether you&amp;rsquo;re trying to count your calories, arguing about the geopolitics of natural gas, or worrying about climate change, energy seems to be the main character. But it doesn&amp;rsquo;t quite make sense. We know that energy is conserved; all we do is transform energy from one form to another. Yet we sense that something is irreversibly lost when we &amp;ldquo;spend energy.&amp;rdquo; What exactly are we spending when we burn food or natural gas? Check out the next paragraph. The answer will &lt;a href="#surpriiise">surprise&lt;/a> you!&lt;/p>
&lt;p>Well, it&amp;rsquo;s entropy. And its story starts with heat.&lt;/p>
&lt;h3 id="the-birth-of-heat">The Birth of Heat&lt;/h3>
&lt;p>Thermodynamics is the study of heat, energy, and work. It was born in the 19th century during the Industrial Revolution from the desire to understand how to efficiently convert heat energy into mechanical work.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot" target="_blank" rel="noopener">Sadi Carnot&lt;/a> showed that the efficiency of a heat engine depends only on the temperature difference between the hot and cold reservoirs and not on the specific working substance or the details of the engine design.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> While this observation had huge practical implications, his main contribution for our purposes is the distinction between reversible and irreversible processes, which led to the notion of entropy.&lt;/p>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Entropy#Etymology" target="_blank" rel="noopener">term entropy&lt;/a> was coined by the German physicist &lt;a href="https://en.wikipedia.org/wiki/Rudolf_Clausius" target="_blank" rel="noopener">Rudolf Clausius&lt;/a> in 1865 as a counterpart to the term energy. The 19th-century German intellectuals were enamored with &lt;a href="https://en.wikipedia.org/wiki/Hellenism_%28neoclassicism%29" target="_blank" rel="noopener">neoclassical hellenism&lt;/a>, which resulted in lots of Greek words in scientific literature. &lt;em>Entropia&lt;/em> means &amp;ldquo;transformation to&amp;rdquo; in Greek. So the German word &amp;ldquo;Entropie&amp;rdquo; is the germanized Greek translation of the German word &amp;ldquo;Verwandlungsinhalt,&amp;rdquo; which Clausius used to describe the transformational content of energy.&lt;/p>
&lt;p>When you burn natural gas to generate heat, you spend the transformational content of the natural gas. The heat that results in this process cannot be transformed back; entropy increases. Clausius formulated the observation that heat flows naturally from a hot body to a cooler one through the inequality
$$ dS ≥ \frac{\delta Q}{T}. $$
Clausius used $S$ for entropy in honor of Sadi Carnot, so $dS$ denotes a small change in entropy, $\delta Q$ is the heat the system absorbs from its surroundings, and $T$ is the temperature at which the heat is absorbed. In an adiabatic process without heat exchange, we have $\delta Q=0$, and entropy can never decrease in accordance with the Second Law.&lt;/p>
&lt;p>Entropy encapsulates the irreversible processes that we typically associate with energy usage. Concepts like &lt;a href="https://en.wikipedia.org/wiki/Energy_crisis" target="_blank" rel="noopener">energy crisis&lt;/a> actually refer to entropy crisis: we need a continuous supply of low entropy to keep the world running.&lt;/p>
&lt;p>Clausius&amp;rsquo; inequality doesn&amp;rsquo;t give an origin story or an explanation for entropy. For that, we need statistical physics.&lt;/p>
&lt;h3 id="atoms">Atoms&lt;/h3>
&lt;p>The famous equality that describes entropy is engraved in Boltzmann&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#/media/File:Boltzmann_equation.JPG" target="_blank" rel="noopener">tombstone&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>
$$ S = k \log W. \tag{1} \label{1} $$
In this expression, $k$ is the Boltzmann constant, $W$ is the number of microstates corresponding to a particular macrostate of the system. To understand what $W$ represents, think of a system composed of many parts, say, tiny atoms. Our description of the system uses a few variables, such as heat and pressure. This macroscopic description is clearly underdetermined: there are gazillions of atomic configurations that result in a given value for heat and pressure. A macrostate is a collection of $W$ individual microstates that are macroscopically indistinguishable. It&amp;rsquo;s the number of equivalent ways the subsystems (atoms) can be arranged without changing the macroscopic state.&lt;/p>
&lt;p>The logarithm in the formula arises from known observations about entropy and simple combinatorics. Consider two systems. It was known that their total entropy is the &lt;em>sum&lt;/em> of their entropies, $S=S_1+S_2$, but the total number of microstates for the full system is the &lt;em>product&lt;/em> of its parts, $W=W_1*W_2$. The only function that converts a product into a sum is the $\log$ which tells us that $S\sim \log W$. The Boltzmann constant in \eqref{1} makes the units work.&lt;/p>
&lt;p>The Second Law is then a probabilistic statement: among different macrostates, the system evolves towards a more probable configuration, one with a larger number of microstates. In this picture, we don&amp;rsquo;t expect entropy to &lt;em>always&lt;/em> increase. It just happens to be more probable. You will run into fluctuations where entropy goes down if you wait enough.&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>
A rather outrageous extrapolation of this idea is the &lt;a href="https://en.wikipedia.org/wiki/Boltzmann_brain" target="_blank" rel="noopener">Boltzmann brain&lt;/a>: a self-aware brain that spontaneously appears in a universe through random fluctuations rather than through biological evolution.&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/p>
&lt;p>
&lt;figure id="figure-boltzmann-brainhttpsenwikipediaorgwikiboltzmann_brain-generated-with-midjourneyhttpswwwmidjourneycom">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./featured.webp" alt="Boltzmann Brain generated with Midjourney" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://en.wikipedia.org/wiki/Boltzmann_brain" target="_blank" rel="noopener">Boltzmann brain&lt;/a> generated with &lt;a href="https://www.midjourney.com" target="_blank" rel="noopener">Midjourney&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="surpriiise">Surpriiise!&lt;/h3>
&lt;p>With the rise of calculators, computers, and communication devices in the 20th century, information started to play a fundamental role in our description of physical phenomena.&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>
&lt;a href="https://en.wikipedia.org/wiki/Claude_Shannon" target="_blank" rel="noopener">Shannon&amp;rsquo;s&lt;/a> reformulation of entropy in &lt;a href="https://ieeexplore.ieee.org/abstract/document/6773024" target="_blank" rel="noopener">The Mathematical Theory of Communication&lt;/a> relates information to surprise.&lt;/p>
&lt;p>What is surprise? To be surprised, you must have a prior expectation, some sense that things happen in a certain way. The more you expect something, the less surprised you are to see it, and vice versa. Therefore, surprise $s$ should be a decreasing function of probability $p\in [0,1]$. Specifically, we&amp;rsquo;re looking for an expression $s(p)$ that satisfies the following reasonable conditions:&lt;/p>
&lt;ul>
&lt;li>If you&amp;rsquo;re absolutely certain of $x$, then $p(x)=1$ and you&amp;rsquo;re not surprised: $s(1)=0$.&lt;/li>
&lt;li>If you&amp;rsquo;re absolutely certain that $x$ can never happen, then $p(x)=0$ and its occurence surprises you infinitely: $s(0) \to \infty$.&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/li>
&lt;li>Surprise should be additive: the total surprise for multiple events should be the addition of the surprise associated with each event. For two events $x_1$ and $x_2$, the combined probability is $p=p_1 * p_2$ and the total surprise should be $s(p) = s(p_1*p_2) = s(p_1) + s(p_2)$&lt;/li>
&lt;/ul>
&lt;p>These conditions are satisfied by a formula that depends logarithmically on the inverse of $p$:
$$ s(p) = \log \frac{1}{p} = - \log p. $$&lt;/p>
&lt;p>Entropy is then the probability-weighted sum of surprise. In other words, entropy is expected surprise:&lt;/p>
&lt;p>$$ S = \sum p_i\ s(p_i) = - \sum p_i \log p_i. \label{2} \tag{2}$$&lt;/p>
&lt;p>Boltzmann used a similar formula &lt;a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#Generalization" target="_blank" rel="noopener">already in 1866&lt;/a>, yet the expression is named after Gibbs and Shannon. It reduces to Boltzmann&amp;rsquo;s first formula \eqref{1} when the probabilities of all microstates are equal, which can then be used to derive Clausius&amp;rsquo; inequality.&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;p>An increase in entropy means that the expected surprise increases. This might sound a bit counterintuitive. We learned that entropy is a measure of disorder. How are disorder and surprise related?&lt;/p>
&lt;p>It may be simpler to understand that patterns reduce &lt;em>total&lt;/em> expected surprise. Let&amp;rsquo;s say every time I order a taxi, I get a yellow cab. Over time, the total expected surprise about the color of the taxi cab will be low even though I might get a blue cab once in a blue moon. If, however, the color of the taxi cab is different every single time, those little surprises add up and maximize the total expected surprise. Disorder increases total expected surprise over a collection of events. It&amp;rsquo;s highest when the events are random.&lt;/p>
&lt;h2 id="the-cross-of-entropy">The Cross of Entropy&lt;/h2>
&lt;p>Boltzmann&amp;rsquo;s entropy \eqref{1} generalizes to Gibbs-Shannon entropy \eqref{2}, allowing different probabilities for the microstates. The next generalization&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> includes a distinction between the expected and observed probabilities and is commonly used to define loss functions in supervised machine learning.&lt;/p>
&lt;h3 id="learning-to-expect-the-unexpected">Learning to Expect the Unexpected&lt;/h3>
&lt;p>In Gibbs-Shannon entropy \eqref{2}, the weights of the sum, $p_i$, are from the same probability distribution that quantifies surprise, $s(p_i)$. But those two distributions are not necessarily the same. Our surprise arises from our assumed expectations, let&amp;rsquo;s call it $q_i$, which may need to be corrected or updated. A good example is climate change, when 100-year storms start happening every decade. The probability distribution for heavy storms has shifted, so we need to adjust our expectations.&lt;/p>
&lt;p>We may formally use the true distribution, $p_i$, for the weights, which are unknown a priori and must be learned from observations. The cross-entropy, $H$, accounts for the difference between true and assumed expectations.
$$ H(p,q) = - \sum p_i \log q_i. $$
The cross-entropy $H$ is the true expected value of our assumed surprise. In other words, it&amp;rsquo;s the expected value, with respect to the true distribution $p$, of our surprise, with respect to the assumed distribution $q$. It measures how likely we are to be surprised (and therefore learn something) if we were told the actual probability distribution of the system.&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> It obtains a minimum when the two distributions are equal.&lt;/p>
&lt;p>This property is why it&amp;rsquo;s so useful in machine learning where cross-entropy is used to construct the &lt;a href="../../2022/learning-machine-learning/#layer-and-loss-build-the-model">loss function&lt;/a> in multiclass classification tasks. The true labels of the training samples serve as the true distribution; the output labels of the neural network serve as the assumed distribution. The cross-entropy loss function is iteratively reduced by numerical optimization. Eventually, the true distribution of the labels matches the predicted distribution from the neural network sufficiently well. At that point, we say the machine learned the training set.&lt;/p>
&lt;h3 id="the-second-coming-of-the-second-law">The Second Coming of the Second Law&lt;/h3>
&lt;p>The Second Law of Thermodynamics has been reformulated using cross-entropy by Bartolotta, Carroll, Leichenauer, and Pollack to &amp;ldquo;incorporate the effects of a measurement of a system at some point in its evolution.&amp;rdquo; &lt;a href="https://arxiv.org/abs/1508.02421" target="_blank" rel="noopener">The Bayesian Second Law of Thermodynamics&lt;/a> uses an information-theoretic approach. Sean Carroll has a great &lt;a href="https://www.preposterousuniverse.com/blog/2015/08/11/the-bayesian-second-law-of-thermodynamics/" target="_blank" rel="noopener">blog post&lt;/a> about this paper; you should read it. Here&amp;rsquo;s a short description in our context.&lt;/p>
&lt;p>According to the Bayesian Second Law, the cross-entropy of the updated (&amp;ldquo;true&amp;rdquo;) distribution with respect to the original (&amp;ldquo;assumed&amp;rdquo;) distribution, plus the generalized heat flow, is larger when evaluated at the end of the experiment than at the beginning. For zero heat transfer, the expected amount of information an observer would learn by being told the true microstate of the system is larger at the final time than at the initial one. Therefore, cross-entropy can change over time according to how well our initial assumptions about a system match its true underlying distribution and how much new information we gain through measurements and updates to our assumptions.&lt;/p>
&lt;p>This updated Second Law describes the increase in cross-entropy as
$$ \Delta H(p, q) + \langle Q \rangle \geq 0, $$
where $\langle Q \rangle$ is the expectation value of a generalized heat flow out of the system, similar to the term $\delta Q$ in Clausius&amp;rsquo; inequality (with a different sign).&lt;/p>
&lt;p>When the assumed distribution differs significantly from the correct distribution during time evolution, it can lead to information loss and, therefore, a large increase in cross-entropy. Cross-entropy increases with time even with zero heat transfer. In this interpretation, what happens during optimization in a machine learning model (decreasing cross-entropy) is the opposite of what happens in stochastic evolution (increasing cross-entropy): The act of learning is a revolt against disorder and decay!&lt;/p>
&lt;h2 id="the-death-of-heat">The Death of Heat&lt;/h2>
&lt;p>At the beginning of the post, I mentioned that the relationship between life and entropy is complicated. When it comes to the Universe, however, things are much simpler. The Universe is evolving towards &lt;a href="https://en.wikipedia.org/wiki/Heat_death_of_the_universe" target="_blank" rel="noopener">heat death&lt;/a>.&lt;/p>
&lt;p>As the Universe continues to expand and matter becomes more dispersed, it will become increasingly difficult for matter to interact with other matter, and energy will become more evenly distributed. Eventually, all stars will have exhausted their fuel, and the Universe will be a cold, dark, lifeless place where nothing happens.&lt;/p>
&lt;p>One of my favorite science-fiction short stories is Asimov&amp;rsquo;s &lt;a href="http://users.ece.cmu.edu/~gamvrosi/thelastq.html" target="_blank" rel="noopener">The Last Question&lt;/a>. It&amp;rsquo;s a story about the heat death of the Universe with the perfect punch line. The story begins with two technicians attending to a giant, self-adjusting, and self-correcting computer, called Multivac that found a way to fulfill the Earth&amp;rsquo;s energy needs by drawing energy from the Sun. The technicians argue that the Sun and all the stars in the Universe will eventually run out. They ask Multivac whether entropy can be reversed, to which Multivac replies, &amp;ldquo;INSUFFICIENT DATA FOR MEANINGFUL ANSWER.&amp;rdquo; The story follows the history of humanity across many eons, through interstellar travel and immortality. The last question remains and is asked repeatedly.&lt;/p>
&lt;p>I won&amp;rsquo;t give away the punchline but it does fit into our observation that &lt;em>learning acts against entropy&lt;/em>. I posed last the question to ChatGPT, our version of the Multivac. Maybe somewhere among the weights and biases in the billions of its connections, ChatGPT is still thinking about it.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./multivac.png" alt="Asking Multivac" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h4 id="footnotes">Footnotes&lt;/h4>
&lt;!-- As techniques from machine learning are applied to solve problems in physics, and vice versa, the connection between the two areas is likely to become even stronger in the future.
Cross-entropy builds a connection between machine learning and fundamental physics through information theory. There are While not ground-breaking, the information-theoretic reformulation of the Second Law may become preferable once it's more widely known. -->
&lt;!-- When you think of a thing, you have what physicists refer to as an isolated system in your mind. In that sense, we are not things. We constantly breath, drink, eat. We are absolutely and critically dependent on a nurturing Universe that keeps us alive.
It is true that everything eventually dies because of entropy, but this is a statement about the universe. It doesn't quite explain why you and me have to die. And why in 100 years. Entropy doesn't prohibit us from living for another 5 billion years. -->&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Unfortunately, Carnot died from cholera at a relatively young age of 36. His book, &lt;a href="https://en.wikipedia.org/wiki/Reflections_on_the_Motive_Power_of_Fire" target="_blank" rel="noopener">Reflections on the Motive Power of Fire&lt;/a>, self-published in 1824, was largely ignored by the scientific community at the time.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Unfortunately, Boltzmann committed suicide while on a beach vacation with his wife and daughter near Trieste, shortly before the experimental verification of his ideas.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>In small systems with a few parts, such fluctuations happen frequently. Their study is a relatively new topic of research that falls under &lt;a href="https://en.wikipedia.org/wiki/Stochastic_thermodynamics" target="_blank" rel="noopener">stochastic thermodynamics&lt;/a>. One of the main results in that area is the &lt;a href="https://en.wikipedia.org/wiki/Jarzynski_equality" target="_blank" rel="noopener">Jarzynski equality&lt;/a> that relates the free energy difference between two equilibrium states to the average work performed on the system during a non-equilibrium process. As the system size increases, however, it becomes increasingly unlikely that such fluctuations reduce entropy and we recover classical thermodynamics.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>It took me about 5 minutes to generate, modify, and upscale this image using &lt;a href="https://www.midjourney.com/" target="_blank" rel="noopener">Midjourney&lt;/a>. An actual Boltzmann brain would presumably take much longer to form but some people argue that it&amp;rsquo;s more likely than the formation of our entire Universe. Personally, I don&amp;rsquo;t like talking about likelihood in the context of the entire Universe. I rather think &lt;a href="https://en.wikipedia.org/wiki/Tractatus_Logico-Philosophicus" target="_blank" rel="noopener">darüber muss man schweigen&lt;/a>.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>As an example on how fundamental information became in physics, consider that one of the most influential physicists of the 20th century, &lt;a href="https://en.wikipedia.org/wiki/John_Archibald_Wheeler" target="_blank" rel="noopener">John Wheeler&lt;/a>, divided his physics career into &lt;a href="https://plus.maths.org/content/it-bit" target="_blank" rel="noopener">three phases&lt;/a>: &amp;ldquo;Everything is Particles,&amp;rdquo; &amp;ldquo;Everything is Fields,&amp;rdquo; and &amp;ldquo;Everything is Information.&amp;rdquo; These stages may sum up the development of physics in the last four centuries. As we are now fully in the informational stage, it will be fascinating to see how machine learning will impact fundamental developments in physics, not only as a tool, but as a conceptual framework for our quest to understand Nature.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>We could consider setting a maximum here. We now know that, indeed, there is a maximum amount of entropy for a given volume of space. This &lt;a href="https://en.wikipedia.org/wiki/Bekenstein_bound" target="_blank" rel="noopener">upper bound&lt;/a> for entropy is named after John Wheeler&amp;rsquo;s student &lt;a href="https://en.wikipedia.org/wiki/Jacob_Bekenstein" target="_blank" rel="noopener">Jacob Bekenstein&lt;/a> and has to do with black holes. But let&amp;rsquo;s leave the quantization of gravity to a later time.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>There are some subtleties here related to the dimensions and underlying probability distributions. The equivalence of the various formulations of entropy must be demonstrated using certain assumptions. If you notice such subtleties, you probably didn&amp;rsquo;t need to read this post, but I hope you enjoyed it.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>There are other generalizations, such as &lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy" target="_blank" rel="noopener">Rényi entropy&lt;/a>, that are interesting but today&amp;rsquo;s focus is on cross-entropy.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>This interpretation is better understood with the &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">Kullback–Leibler divergence&lt;/a> defined by
$$ D(p||q) = \sum p_i \log \frac{p_i}{q_i} = H(p,q) - S(p). $$
This expression vanishes when $p=q$ in accordance with the interpretation that there is nothing left to learn when the true distribution equals our assumed expectation.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Learning Machine Learning</title><link>https://anilzen.github.io/post/2022/learning-machine-learning/</link><pubDate>Thu, 08 Dec 2022 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/learning-machine-learning/</guid><description>&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>
&lt;h2 id="neural-network-what-is-machine-learning">Neural Network: What is machine learning?&lt;/h2>
&lt;p>The core of a machine learning algorithm is the neural network that maps inputs to desired outputs. When you read &amp;ldquo;neural network,&amp;rdquo; you might think of the human brain with layers of interconnected neurons working together to solve problems. But in machine learning, we are simply talking about a function with parameters. Lots and lots of parameters. &amp;ldquo;Learning&amp;rdquo; is adjusting these parameters until the difference between the desired output and the actual output of the function is sufficiently small. That&amp;rsquo;s it. In a way, machine learning is the rediscovery of the old adage that you can fit any &lt;a href="https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant" target="_blank" rel="noopener">elephant&lt;/a> with sufficient parameters.&lt;/p>
&lt;p>This was a very short summary of what people mean by machine learning. You can imagine that the function, the parameters, and the adjustment process are all rather sophisticated and can get very complicated. I&amp;rsquo;ll expand on this basic idea below with an example of a machine-learning algorithm.&lt;/p>
&lt;p>First, some terminology. Think of each input dimension as a neuron in a neural network. The parameters of the neural network, or the function, are called weights and biases. Weights represent the strength of the connection between neurons; biases shift the activation threshold of a neuron. By adjusting the weights and biases based on the output, the network learns the patterns in the data and makes predictions on new data.&lt;/p>
&lt;p>Let&amp;rsquo;s write this down. I mentioned that the neural network is just a function with parameters. Its output is usually a probability, so we&amp;rsquo;ll call it $p$. The network should look something like $p=f(x; W,b)$, where $x$ is the input array, $p$ is the network&amp;rsquo;s output array, $W$ are the weights, and $b$ are the biases. A simple neural network could then be written like this
$$ p = W \cdot x + b. $$
But wait, you say; this is just a linear transformation! Layering linear transformations on top of each other can only create a linear network. You can&amp;rsquo;t learn complex patterns and make accurate predictions with just linear transformations. To introduce nonlinearity into the model, we throw this into a nonlinear activation function $\sigma$, so the output looks like
$$ p = \sigma(W \cdot x + b). \label{1} \tag{1} $$
There are a few commonly used activation functions that one frequently encounters: the sigmoid function, the hyperbolic tangent (tanh) function, the rectified linear unit (ReLU) function, and so on. We&amp;rsquo;ll use a generalization of the sigmoid (or logistic) function for our experiments.
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
This function maps a real-valued input to a value between 0 and 1, so the output can be interpreted as a probability, making it directly useful in classification problems.&lt;/p>
&lt;p>To get into more detail, we need to understand and prepare the input. I use the MNIST dataset for the demonstration below. We will avoid the powerful machine learning packages and only use NumPy&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, so all operations can be considered elemental. You can follow along on &lt;a href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing" target="_blank" rel="noopener">Colab&lt;/a>.
&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>&lt;/p>
&lt;h2 id="mnist-dataset-prepare-the-input">MNIST Dataset: Prepare the input&lt;/h2>
&lt;p>The MNIST dataset is a large database of handwritten digits consisting of 60,000 training images and 10,000 test images. Each image is a 28x28 grayscale image labeled with the correct digit, from 0 to 9. It&amp;rsquo;s commonly used for training and testing various image processing and machine learning algorithms. The digits in grayscale look like this
&lt;figure id="figure-handwritten-digits-0-and-1-from-the-mnist-dataset">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Handwritten digits from the MNIST dataset" srcset="
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp 400w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_3fc866dcf8d288cdcdc65060faa475c2.webp 760w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp"
width="600"
height="400"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Handwritten digits 0 and 1 from the MNIST dataset.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Our goal is to teach the single-layer network (\ref{1}) to recognize these handwritten digits. The network learns from the training dataset by adjusting its weights to minimize the difference between the desired output and the actual output, that is, the loss. This process is repeated until the loss is sufficiently small, implying that the network has learned the dataset. So we need to define the loss, calculate how it depends on layer parameters, and find a way to minimize it iteratively.&lt;/p>
&lt;h2 id="layer-and-loss-build-the-model">Layer and Loss: Build the model&lt;/h2>
&lt;p>Our model architecture consists of just the one layer in (\ref{1}). So this is an example of shallow learning. But even with shallow networks, it can get confusing with the number of samples, inputs, and outputs. To recap, we have $M=60,000$ samples in the training set; each sample has $N=28\times28=784$ dimensions (one for each pixel in a flattened 1D-array); the output has $K=$10 dimensions (one for each digit). Accounting is worse when you have hidden layers in between. They all live in different spaces, so it makes sense to introduce different types of letters into the tensor notation for each type of space. To clarify each space, I like to define indices with their own ranges&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:
$$ \begin{align}
a,b&amp;amp;=1,2,\dots,M=60,000. \\
\alpha, \beta&amp;amp;=1,2,\dots,N=784. \\
i,j&amp;amp;=1,2,\dots,K=10.
\end{align} $$
We can then write the output of our AI algorithm as
$$ p_{ai} = \sigma(z_{ai}) = \sigma\left( \sum_{\alpha=1}^{N} x_{a\alpha} W_{\alpha i} + b_i \right). \tag{2} \label{2} $$
Here, $\sigma$ is a generalization of the sigmoid function, called the softmax function. In code, we write&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This step is sometimes called the forward pass.&lt;/p>
&lt;p>We do not use the sigmoid function because we have to ensure that the output probabilities sum to one. The softmax function is defined as
$$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}. \tag{3} \label{3} $$
It maps a vector of arbitrary real values, $z_i$, to a vector of values between 0 and 1. The sum of all outputs is 1, so each output can be interpreted as a probability. This makes it a useful activation function for multiclass classification tasks, where the predicted probabilities must sum to 1.&lt;/p>
&lt;p>The softmax function is numerically unstable if implemented naively, so we rewrite it such that the maximum value of the input array is 0.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr />
&lt;p>We now need to devise a way to tell our network when its outputs, $p_{ai}$, are losers. This is done by defining a loss function that measures the difference between the desired output, $y$, and the actual output, $p$. There are many possible choices. For example, when predicting a continuous variable, one typically uses a regression loss function such as mean squared error or mean absolute error. We have a multiclass classification problem (one class for each digit), so we&amp;rsquo;ll use the cross-entropy loss&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> defined as
$$ L = - \frac{1}{M} \sum_{a=1}^M \sum_{i=1}^N y_{ai} \log p_{ai}, \tag{4} \label{4} $$
where $p$ is the predicted probability, $y$ is the actual probability, $M$ is the number of samples, and $N$ is the number of classes. In NumPy&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The cross-entropy loss is often used with a softmax activation function in the output layer of a classification model. One of the main reasons is the beautiful simplification of its derivative with respect to the input. But I&amp;rsquo;m getting ahead of myself here.&lt;/p>
&lt;p>How do we minimize the loss? We can evaluate the function at different locations in the parameter space to search for the minimum, but that&amp;rsquo;s not an efficient approach, especially in high dimensions. In our simple toy example, the parameter space of weights and biases has $784\times 10+10=7,850$ dimensions.&lt;/p>
&lt;p>A better approach is gradient descent. We start at some random point which will invariably have a large loss. It&amp;rsquo;s like starting on a big hill. To go to the bottom, you take small steps downhill, that is, you descend along the negative gradient, until you can&amp;rsquo;t go reasonably further. We need to compute the gradient of the loss function with respect to the weights and biases to determine the downhill direction. Using the chain rule, we obtain the following formula for the derivative of the loss function with respect to bias
$$ \frac{\partial{L}}{\partial b_{j}} = - \frac{1}{M}\sum_{a=1}^M \sum_{i=1}^N \sum_{k=1}^N y_{ai}\frac{\partial \log p_{ai}}{\partial z_{ak}} \frac{\partial z_{ak}}{\partial b_j} . $$
This calculation is where the simplification comes in when you combine the cross-entropy loss with softmax activation. To demonstrate, write the total loss as the mean of the losses of all samples, $L=\tfrac{1}{M}\sum_{a=1}^M \ell_a$. Let&amp;rsquo;s compute the derivative for a single sample, suppressing its index
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = - \sum_{i=1}^N \sum_{k=1}^N y_{i}\frac{\partial \log p_{i}}{\partial z_{k}} \frac{\partial z_{k}}{\partial b_j} . $$
The log-term&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> with the definition of softmax (\ref{3}) reads
$$ \log p_i = \log (\sigma(z_i)) = z_i - \log\left(\sum_{j=1}^K e^{z_j}\right).$$
We get for the $z$-derivative
$$ \frac{\partial \log p_i}{\partial z_k} = \delta_{ik} - \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} = \delta_{ik} - p_k.$$
Combining with the summation over $y$, we obtain this very simple formula
$$ \sum_{i=1}^{N} y_{i} (\delta_{ik} - p_k) = y_k - p_k \sum_{i=1}^{N} y_{i} = y_k - p_k. $$
For the last step, remember that $y_i$ are probabilities that sum up to 1. We then have
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = \sum_{k=1}^N (p_k - y_k)\frac{\partial z_{k}}{\partial b_j} $$
Now we can insert the dependence of $z$ on $b$ and bring back the summation over the samples with index $a$ to get
$$ \frac{\partial{L}}{\partial b_{j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}). $$
Similarly, for the weights
$$ \frac{\partial{L}}{\partial W_{\beta j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}) x_{a\beta}. $$
In NumPy, the gradient of the loss is then calculated by&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We then update the weights and biases based on these gradients with some step size $\lambda$. What mathematicians call step size is called learning rate in machine learning, even though it&amp;rsquo;s not quite a rate. Anyway, we update the weights and biases iteratively as follows
$$ \begin{align}
W^{n+1} &amp;amp;= W^n - \lambda \frac{\partial{L}}{\partial W}, \\
b^{n+1} &amp;amp;= b^n - \lambda \frac{\partial{L}}{\partial b}.
\end{align} $$&lt;/p>
&lt;h2 id="train-and-evaluate">Train and Evaluate&lt;/h2>
&lt;p>We now have everything in place to train the network using the training set. To recap, we initialize the weights and biases randomly and then run multiple epochs&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, during which we get the network output (forward pass), compute the gradient of the associated loss, and update the weights and biases in the direction of the negative gradient with a constant learning rate (backpropagation). Below is all of this in code.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">200&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To evaluate the accuracy of the network on the training set, we compare the network&amp;rsquo;s prediction with the labels&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">accuracy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[];&lt;/span> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">accuracy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count_nonzero&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We get about %90 accuracy with this simple method on both the test and the training sets!&lt;/p>
&lt;p>
&lt;figure id="figure-final-accuracy-and-loss-after-training-for-200-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final result of training" srcset="
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp 400w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_2dc1e69ab398c40bfcd5a110fc5ac088.webp 760w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp"
width="760"
height="380"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final accuracy and loss after training for 200 steps.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>The building block of a neural network is the single layer (\ref{1}). I hope you obtained an understanding of how a neural network layer is trained and what people mean when they say the machine has learned something. My goal was to provide a basic understanding of this procedure. This can get very complicated. Large operational machine learning models, such as &lt;a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noopener">GPT-3&lt;/a>, &lt;a href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval" target="_blank" rel="noopener">Gopher&lt;/a>, or &lt;a href="https://developer.nvidia.com/megatron-turing-natural-language-generation" target="_blank" rel="noopener">Megatron-Turing NLG&lt;/a> use many network layers with hundreds of billions of parameters, but the procedure is similar.&lt;/p>
&lt;p>You now have a few simple tools as a starting point for further inquiry. Here are a couple of directions to go from here:&lt;/p>
&lt;ul>
&lt;li>Use a more complex neural network with at least one hidden layer. The &amp;ldquo;deep&amp;rdquo; in deep learning comes from hidden layers. Each layer dramatically increases the parameter space&amp;rsquo;s dimension, which also increases computation time. But the gain in accuracy may be worth it. You can easily get about %97 accuracy with just one additional layer for the MNIST dataset.&lt;/li>
&lt;li>Batch-process your samples. Batch processing was not necessary for his example, but with higher dimensions and larger training sets, it becomes necessary. There are also indications that batch-processed (stochastic) gradient descent generalizes better.&lt;/li>
&lt;li>Use a better optimization procedure. The step size, $\lambda$, or &lt;code>lr&lt;/code>, is the most important parameter in gradient descent. I chose the rather large value of &lt;code>lr=6&lt;/code>. You can see in the plots that the loss doesn&amp;rsquo;t decrease monotonously. A simple fix for this is to multiply the learning rate with a scalar slightly less than 1, so it gets smaller at every epoch. But more importantly, you should use a better optimizer such as Adams. In fact, my interest in optimization is the reason I wrote this post. With Jingcheng Lu and Eitan Tadmor from the University of Maryland, we constructed a &lt;a href="https://anilzen.github.io/publication/2022-swarm-based-gradient-descent/">swarm-based gradient descent&lt;/a> that avoids getting trapped at local minima. The method beats existing optimizers in certain types of non-convex optimization problems. I&amp;rsquo;m experimenting to see whether it&amp;rsquo;s useful in machine learning applications.&lt;/li>
&lt;/ul>
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>NumPy is a free, open-source Python library for scientific computing and data analysis. It has a lot of functionality, but the main reason for using NumPy here is its speed. It uses vectorization instead of looping through individual elements to perform calculations on an array, allowing faster calculations and more efficient use of memory. NumPy operations are implemented in C using highly optimized libraries that take advantage of modern processor architectures, so we are not slowed down by Python&amp;rsquo;s excruciatingly inefficient loops.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This is common practice in theoretical physics, in particular, in general relativity, where we have maps between and projections onto various different spaces and subspaces. The tensor notation using different types of letters helps keep track of the spaces in the computations. I was tempted to introduce the Einstein summation convention here as well, but it doesn&amp;rsquo;t quite work.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>You might recognize this expression of entropy. It appears commonly as $-\sum p_i \log p_i$ in different entropy formulas such as Gibbs entropy, Shannon entropy, and von Neumann entropy. In our case, it&amp;rsquo;s a way of measuring the difference between two probability distributions. We compare the true distribution of the labeled data, $y$, and the predicted probability distribution from the model, $p$. When the same probability distribution $p$ is used on either side of the log function, for example, in Shannon entropy, it measures the amount of uncertainty or randomness in that given probability distribution, which is useful for encoding information or measuring the level of disorder in a system. The purpose and interpretation are different in those cases, but the underlying similarity is the quantification of disorder, which justifies using the term entropy.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>It seems that in machine learning, NumPy, and many other mathematical software programs, $\log$ refers to the natural logarithm with base $e$, which I adopted for this post. It is confusing because, in information theory, $\log$ commonly refers to base 2. But of course &lt;a href="https://en.wikipedia.org/wiki/Logarithm#Particular_bases" target="_blank" rel="noopener">everybody knows that&lt;/a> $\log$ is base 10, and for the other stuff, you either write $\ln$ for base $e$ or $\log_2$ for base 2. In short, use your $\log$ with caution.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>You might think that the double transpose in &lt;code>np.dot(dL.T,x).T&lt;/code> is unnecessary. Why not write it as &lt;code>np.dot(x.T, dL)&lt;/code>? It turns out that the former dot product is faster because of the way the data is stored in memory.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>These are actually iterations. The term epochs alludes to a bright future where you might want to process your training data in batches. Batch processing is among the many directions you might want to expand the code.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How to draw Penrose diagrams</title><link>https://anilzen.github.io/post/2022/drawing-penrose-diagrams/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/drawing-penrose-diagrams/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Penrose_diagram" target="_blank" rel="noopener">Penrose diagram&lt;/a> is a valuable tool in relativity to illustrate the global causal structure of spacetimes&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Often, qualitative diagrams are sufficient if you&amp;rsquo;re mainly interested in the essential causal relationships. But sometimes, for numerical work or for your own understanding, you need a quantitatively correct diagram.&lt;/p>
&lt;p>Below, we draw Penrose diagrams using free and open-source software. We&amp;rsquo;ll draw the diagrams using the LaTeX package &lt;a href="https://en.wikipedia.org/wiki/PGF/TikZ" target="_blank" rel="noopener">TikZ&lt;/a> (not a drawing program but a program that draws&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>). We&amp;rsquo;ll use Python to perform transformations when we hit the computational limitations of TikZ.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
The code for all diagrams below is on my &lt;a href="https://github.com/anilzen/anilzen.github.io/tree/main/content/post/2022/drawing-penrose-diagrams/tikz" target="_blank" rel="noopener">website repository&lt;/a>.
&lt;/div>
&lt;/div>
&lt;h3 id="the-basic-idea-behind-penrose-diagrams">The basic idea behind Penrose diagrams&lt;/h3>
&lt;p>The Penrose diagram is an extension, or better, a &lt;em>completion&lt;/em> of the &lt;a href="https://en.wikipedia.org/wiki/Spacetime_diagram" target="_blank" rel="noopener">Minkowski diagram&lt;/a>. Like in the Minkowski diagram, time is vertical, space is horizontal, and null rays are at 45 degrees to the axes. In contrast to the Minkowski diagram, a Penrose diagram includes &amp;ldquo;points at infinity&amp;rdquo; added by compactification, thereby visualizing the rich structure of infinity arising from the unification of space and time.&lt;/p>
&lt;p>Below are two beautiful &lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">TikZ diagrams&lt;/a> by &lt;a href="https://www.physik.uzh.ch/en/researcharea/cms/people/Izaak-Neutelings.html" target="_blank" rel="noopener">Izaak Neutelings&lt;/a>. On the left is the Minkowski diagram. Spacetime extends infinitely in all directions. On the right is the Penrose diagram representing the entire spacetime in a finite square.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-minkowski-diagramhttpstikznetrelativity_penrose_diagram">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Minkowski diagram](https://tikz.net/relativity_penrose_diagram/)" srcset="
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_00daccc019c1c8e7d5b53f79e0e59c99.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_463aa2ce8f62b82dc39936265db730ef.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_00daccc019c1c8e7d5b53f79e0e59c99.webp"
width="760"
height="626"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">Minkowski diagram&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-penrose-diagramhttpstikznetrelativity_penrose_diagram">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Penrose Diagram](https://tikz.net/relativity_penrose_diagram/)" srcset="
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_ac06b1fd002695e2e4dc0081a38f765b.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_6a8d1b9f9b8453134e80447669cc1564.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_ac06b1fd002695e2e4dc0081a38f765b.webp"
width="760"
height="661"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">Penrose Diagram&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Compactification maps the Minkowski diagram to the Penrose diagram by mapping the null directions to a finite interval. Let&amp;rsquo;s see how that works.&lt;/p>
&lt;h3 id="minkowski-spacetime">Minkowski spacetime&lt;/h3>
&lt;p>Consider the 4-dimensional Minkowski spacetime with standard spherical coordinates: $t \in (-\infty,\infty)$, $r \in [0,\infty)$, $\theta\in[0,\pi]$, $\varphi\in[0,2\pi)$ with metric
$$ ds^2 = -dt^2 + dr^2 + r^2 d\sigma^2, $$
where $d\sigma^2=d\theta^2+\sin^2\theta d\varphi^2$. Penrose diagrams are two-dimensional, so we essentially ignore the angular coordinates represented by the $d\sigma^2$ piece. Each point on the diagram, except the line at the origin, represents a sphere. Spherical light rays propagate along the directions $t+r$ and $t-r$. These directions are called &lt;a href="https://en.wikipedia.org/wiki/Null_vector" target="_blank" rel="noopener">null&lt;/a> because they are in the &lt;a href="https://en.wikipedia.org/wiki/Kernel_%28linear_algebra%29" target="_blank" rel="noopener">nullspace&lt;/a> of the metric.&lt;/p>
&lt;p>Underlying the Penrose diagrams are coordinates, $T$ and $R$, which compactify these null directions:
$$ T+R = \frac{2}{\pi}\textcolor{DarkOrchid}{\arctan}(t+r),$$
$$ T-R = \frac{2}{\pi}\textcolor{DarkOrchid}{\arctan}(t-r). $$
The scale factor $2/\pi$ is to map the range of the coordinates to $T\pm R \in(-1,1)$. Note that I&amp;rsquo;m not including the endpoints in the intervals yet. The Penrose compactification procedure involves the conformal completion of the spacetime, which adds the points at infinity after a regularizing rescaling. This detail is not relevant for the diagrams but is important for geometry.&lt;/p>
&lt;p>The Penrose coordinates $(T,R)$ read in terms of the standard $(t,r)$ as:
$$ T(t,r) = \frac{1}{\pi}\left( \arctan(t+r) + \arctan(t-r) \right)$$
$$ R(t,r) = \frac{1}{\pi}\left( \arctan(t+r) - \arctan(t-r) \right)$$&lt;/p>
&lt;p>Let&amp;rsquo;s make some plots using these coordinates. I&amp;rsquo;ll demonstrate plotting time surfaces (level sets of a time coordinate). Say you&amp;rsquo;d like to draw constant $t$ surfaces in the Penrose diagram. You might be tempted to solve the above relationships for a graph function $T(R;t)$ with constant $t$. But it is easier to let the machine do the work and plot the surfaces parametrically. Define the functions $T(t,r)$ and $R(t,r)$ in TikZ as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl">&lt;span class="k">\tikzset&lt;/span>&lt;span class="nb">{&lt;/span>declare function=&lt;span class="nb">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> T(&lt;span class="k">\t&lt;/span>,&lt;span class="k">\r&lt;/span>) = &lt;span class="k">\fpeval&lt;/span>&lt;span class="nb">{&lt;/span>1/pi*(atan(&lt;span class="k">\t&lt;/span>+&lt;span class="k">\r&lt;/span>) + atan(&lt;span class="k">\t&lt;/span>-&lt;span class="k">\r&lt;/span>))&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> R(&lt;span class="k">\t&lt;/span>,&lt;span class="k">\r&lt;/span>) = &lt;span class="k">\fpeval&lt;/span>&lt;span class="nb">{&lt;/span>1/pi*(atan(&lt;span class="k">\t&lt;/span>+&lt;span class="k">\r&lt;/span>) - atan(&lt;span class="k">\t&lt;/span>-&lt;span class="k">\r&lt;/span>))&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">}}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, plot the lines parametrically.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl">&lt;span class="k">\def\Nlines&lt;/span>&lt;span class="nb">{&lt;/span>6&lt;span class="nb">}&lt;/span> &lt;span class="c">% total number of lines is 2\Nlines+1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="k">\newcommand\samp&lt;/span>&lt;span class="na">[1]&lt;/span>&lt;span class="nb">{&lt;/span> tan(90*#1) &lt;span class="nb">}&lt;/span> &lt;span class="c">% for equidistant sampling
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="k">\foreach&lt;/span> &lt;span class="k">\i&lt;/span> [evaluate=&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>=&lt;span class="k">\i&lt;/span>/(&lt;span class="k">\Nlines&lt;/span>+1);&lt;span class="nb">}&lt;/span>] in &lt;span class="nb">{&lt;/span>-&lt;span class="k">\Nlines&lt;/span>,...,&lt;span class="k">\Nlines&lt;/span>&lt;span class="nb">}{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\message&lt;/span>&lt;span class="nb">{&lt;/span>Drawing i=&lt;span class="k">\i&lt;/span>...&lt;span class="nb">^^&lt;/span>J&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\draw&lt;/span>&lt;span class="na">[line width=0.3,samples=30,smooth,variable=\r,domain=0.001:1]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> plot(&lt;span class="nb">{&lt;/span> R(&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>&lt;span class="nb">}&lt;/span>,&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\r&lt;/span>&lt;span class="nb">}&lt;/span>) &lt;span class="nb">}&lt;/span>, &lt;span class="nb">{&lt;/span> T(&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>&lt;span class="nb">}&lt;/span>,&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\r&lt;/span>&lt;span class="nb">}&lt;/span>) &lt;span class="nb">}&lt;/span>);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;re sampling points fairly evenly by incorporating the compactification into the plot function. As you can see in the diagrams below, the time surfaces are equally separated from each other at the origin. The function &lt;code>samp&lt;/code> controls the separation of points in the plot.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-compactification-with-arctan-filetikzminkmink_arctantex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Compactification with arctan [(file)](tikz/mink/mink_arctan.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_7c87c5b70173cea05b03cb6ef62b45e7.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_4eb7334e9c0a75b4d551c2668a6b69ba.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_7c87c5b70173cea05b03cb6ef62b45e7.webp"
width="438"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Compactification with arctan &lt;a href="tikz/mink/mink_arctan.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-compactification-with-tanh-filetikzminkmink_tanhtex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Compactification with tanh [(file)](tikz/mink/mink_tanh.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_843cb384a6441847b2660c6b3d4b8f27.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_b04b00b0785f30a374e685c6722f9527.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_843cb384a6441847b2660c6b3d4b8f27.webp"
width="438"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Compactification with tanh &lt;a href="tikz/mink/mink_tanh.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The two versions of the diagram illustrate a coordinate-dependent feature that confuses even the experts. The $t$ surfaces intersect at spatial infinity, $i^0$, on both diagrams. On the left, they are tangent to each other, while on the right, they are not. So this seems to be a coordinate-dependent feature. Penrose used the inverse tangent function in his original papers. Many other choices exist. For example, using the hyperbolic tangent, the mapping reads
$$ T+R = \textcolor{DarkOrchid}{\tanh}(t+r),$$
$$ T-R = \textcolor{DarkOrchid}{\tanh}(t-r). $$
This mapping gives the diagram on the right where $t$ surfaces are not tangential at spatial infinity. There&amp;rsquo;s a subtlety here. Conformal compactification with hyperbolic tangent is not regular when you include the angular dimensions. So be careful what you choose for the mapping.&lt;/p>
&lt;p>I draw Penrose diagrams typically to present the causal structure of &lt;a href="https://hyperboloid.al" target="_blank" rel="noopener">hyperboloidal&lt;/a> surfaces. Such surfaces behave like spacetime hyperboloids: they are spacelike everywhere, including null horizons. Spacetime hyperboloids are typically defined by the level sets of $\tau$ as
$$ t^2 - r^2 = \tau^2 \implies \tau = \pm \sqrt{t^2 - r^2}. $$
This construction appears in many models, such as the &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne model&lt;/a> of cosmology, &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">Dirac&amp;rsquo;s point-form&lt;/a> of relativistic dynamics, the &lt;a href="https://arxiv.org/abs/hep-th/0303006" target="_blank" rel="noopener">de Boer-Solodukhin&lt;/a> holography of Minkowski spacetime, and the &lt;a href="https://arxiv.org/abs/1304.2794" target="_blank" rel="noopener">Buchholz-Roberts framework&lt;/a> of relativistic QED. However, these surfaces are not generally useful for studying evolution in time because they intersect at future null infinity. This may not be immediately obvious from their definition, but you can see it right away on the Penrose diagram (left panel below).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-intersecting-hyperbolic-slicing-filetikzminkmink_hypal_intersecttex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Intersecting hyperbolic slicing [(file)](./tikz/mink/mink_hypal_intersect.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_5332b674093f643b42a9357e1afcba7a.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_85dcb9c9c06ca82a3c6b36b23a9bc33a.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_5332b674093f643b42a9357e1afcba7a.webp"
width="440"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Intersecting hyperbolic slicing &lt;a href="./tikz/mink/mink_hypal_intersect.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-smooth-hyperboloidal-foliation-filetikzminkmink_hypal_foliationtex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Smooth hyperboloidal foliation [(file)](tikz/mink/mink_hypal_foliation.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_18631d5485c16d6c7c2bc7be7aaf3eb5.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_a4bae7e0f37eb8922bfaa8622f12bf45.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_18631d5485c16d6c7c2bc7be7aaf3eb5.webp"
width="457"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Smooth hyperboloidal foliation &lt;a href="tikz/mink/mink_hypal_foliation.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>A better option for a foliation of Minkowski spacetime, illustrated on the right panel above, is to use time-shifted hyperboloids. We pick one spacetime hyperboloid, say, with unit radius, and shift it in time by $\tau$, like this
$$ (t-\tau)^2 - r^2 = 1 \implies \tau = t \pm \sqrt{1+r^2}. $$
Choosing the minus sign gives a future hyperboloidal foliation. The surfaces do not intersect but provide a smooth foliation of future null infinity.&lt;/p>
&lt;p>This example demonstrates how causal structures that may not be immediately clear from the formulas can be illustrated and understood with Penrose diagrams. Other examples include the counterintuitive causal structure of hyperboloidal surfaces or the intersection of standard time surfaces at spatial infinity. We can demonstrate such properties by calculations, but it&amp;rsquo;s much easier to understand them with the help of a Penrose diagram.&lt;/p>
&lt;h4 id="reading-data-into-tikz">Reading data into TikZ&lt;/h4>
&lt;p>TikZ is limited in its data processing capabilities. As a consequence, the evaluation of the lines in the plots takes a while. Once you move away from Minkowski spacetime, the transformations become too complicated for TikZ. To handle complicated mathematical transformations, you can generate the data for the plots in Python, write them in a file, and plot them with TikZ. Below is the Python code to generate the data files for each time surface.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mf">2.&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">14&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">endpoint&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">30&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">penrose_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">penrose_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t_val&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">savetxt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;arctan_data&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;.csv&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">delimiter&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;,&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fmt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;R,T&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">comments&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You&amp;rsquo;ll need the &lt;a href="https://numpy.org/" target="_blank" rel="noopener">numpy&lt;/a> library installed in your environment (use &lt;a href="https://xkcd.com/1987/" target="_blank" rel="noopener">local environments for Python!&lt;/a>). The array &lt;code>t&lt;/code> contains the constant values of the time surfaces, the array &lt;code>r&lt;/code> includes the samples of the plot parameter. The domains stop at $\pi/2$ because we use the $\tan$ function to sample the points for a relatively even distribution. Then, for each value of &lt;code>t&lt;/code>, we write the $R,T$ coordinates into a CSV file with headers and some formatting.&lt;/p>
&lt;p>On the TikZ side, we read these points from the respective files and plot them. To plot data points, I use the library &lt;code>pgfplots&lt;/code>. The nodes must be drawn with respect to the axis of the plot, which is achieved with the &lt;code>axis cs:&lt;/code> directive. The relevant part of the code is below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\begin&lt;/span>&lt;span class="nb">{&lt;/span>axis&lt;span class="nb">}&lt;/span>[axis lines=none, xmin=-.1,xmax=1.1,ymin=-1.2,ymax=1.2,width=0.5&lt;span class="k">\textwidth&lt;/span>,height=0.8&lt;span class="k">\textwidth&lt;/span>]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[right]&lt;/span> at (axis cs:0.6,1) &lt;span class="nb">{&lt;/span>&lt;span class="k">\small&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\arctan&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="nv">\cdot&lt;/span>&lt;span class="o">)&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\coordinate&lt;/span> (O) at (axis cs:0,0) ; &lt;span class="c">% center: origin (r,t) = (0,0)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (S) at ( axis cs:0,-1); &lt;span class="c">% south: t=-infty, i-
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (N) at ( axis cs:0, 1); &lt;span class="c">% north: t=+infty, i+
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (E) at ( axis cs:1, 0); &lt;span class="c">% east: r=+infty, i0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\draw&lt;/span>&lt;span class="na">[thick]&lt;/span> (N) -- (E) -- (S) -- cycle;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\newcommand&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\scri&lt;/span>&lt;span class="nb">}{&lt;/span>&lt;span class="k">\mathscr&lt;/span>&lt;span class="nb">{&lt;/span>I&lt;span class="nb">}}&lt;/span> &lt;span class="c">% null infinity
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\node&lt;/span>&lt;span class="na">[right]&lt;/span> at (E) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above]&lt;/span> at (N) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[below]&lt;/span> at (S) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above, rotate=90]&lt;/span> at (O) &lt;span class="nb">{&lt;/span>&lt;span class="k">\small&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">r&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above right]&lt;/span> at (axis cs: 0.5,0.5) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\scri&lt;/span>&lt;span class="nb">^&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[below right]&lt;/span> at (axis cs: 0.5,-0.5) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\scri&lt;/span>&lt;span class="nb">^&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\foreach&lt;/span> &lt;span class="k">\file&lt;/span> in &lt;span class="nb">{{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data0.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data1.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data2.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data3.csv&lt;span class="nb">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data4.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data5.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data6.csv&lt;span class="nb">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">{&lt;/span>&lt;span class="k">\addplot&lt;/span>&lt;span class="na">[domain={-1,1}]&lt;/span> table [x=R, y=T, col sep=comma] &lt;span class="nb">{&lt;/span>&lt;span class="k">\file&lt;/span>&lt;span class="nb">}&lt;/span>;&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">\end&lt;/span>&lt;span class="nb">{&lt;/span>axis&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is probably not the most elegant solution; you switch platforms to draw the plots. If you&amp;rsquo;re up for it, you might prefer using a Python library such as &lt;a href="https://pypi.org/project/tikzplotlib/" target="_blank" rel="noopener">tikzplotlib&lt;/a> to avoid going back and forth between Python and TikZ.&lt;/p>
&lt;h3 id="schwarzschild-spacetime">Schwarzschild spacetime&lt;/h3>
&lt;p>The Schwarzschild line element in standard (Droste) coordinates reads
$$ ds^2 = - f(r) dt^2 + \frac{1}{f(r)} dr^2 + r^2 d\sigma^2, \tag{SS} \label{1}$$
where
$$ f(r) = 1-\frac{2 M}{r},$$
or in dimensionless coordinates
$$ f(r) = 1-\frac{1}{r}.$$
Dimensionless coordinates are similar to setting $M=1/2$.&lt;/p>
&lt;p>The metric is singular where $f(r)$ vanishes, at $r=1$. This surface is the &lt;a href="https://en.wikipedia.org/wiki/Event_horizon" target="_blank" rel="noopener">event horizon&lt;/a>, but the metric singularity is a coordinate artifact. There are regular coordinates across the event horizon, such as the &lt;a href="https://en.wikipedia.org/wiki/Gullstrand%E2%80%93Painlev%C3%A9_coordinates" target="_blank" rel="noopener">Gullstrand–Painlevé&lt;/a> or the &lt;a href="https://en.wikipedia.org/wiki/Eddington%E2%80%93Finkelstein_coordinates" target="_blank" rel="noopener">Eddington–Finkelstein&lt;/a> coordinates. In fact, we must use such regular coordinates to draw Penrose diagrams.&lt;/p>
&lt;p>Penrose diagrams for Schwarzschild spacetime are traditionally drawn using a compactification of &lt;a href="https://en.wikipedia.org/wiki/Kruskal%E2%80%93Szekeres_coordinates" target="_blank" rel="noopener">Kruskal coordinates&lt;/a>. Let&amp;rsquo;s copy them from Wikipedia (for a derivation, see, for example, the Appendix of my &lt;a href="https://anilzen.github.io/publication/zenginoglu-2007-conformal/" target="_blank" rel="noopener">thesis&lt;/a>):
$$ \tau = (r-1) e^r \sinh \tfrac{t}{2}, $$
$$ \rho = (r-1) e^r \cosh \tfrac{t}{2}. $$
The coordinates of the Penrose diagram are compactified along the null directions just as in the Minkowski case:
$$ T = \frac{1}{\pi}\left( \arctan(\tau+\rho) + \arctan(\tau-\rho) \right)$$
$$ R = \frac{1}{\pi}\left( \arctan(\tau+\rho) - \arctan(\tau-\rho) \right)$$
The transformations are more complicated, so we will use Python codes to generate the lines and plot them with TikZ. Here&amp;rsquo;s the Python function to construct the compactified Kruskal coordinates.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">kruskal_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rho&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cosh&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tau&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sinh&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I construct Schwarzschild radial points from tortoise coordinates that are on Chebyshev nodes because I want higher density near the horizon and infinity. There are different, possibly more effective ways of achieving such a beneficial point distribution, but this method works well enough.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">t_vals&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">3.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">7&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r_tort&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">))[::&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r_schw&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lambertw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r_tort&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t_vals&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kruskal_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r_schw&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;data/ss&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;.csv&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">savetxt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fn&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">delimiter&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;,&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fmt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;R,T&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">comments&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;{&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">fn&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;},&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Schwarzschild time slices intersect at the bifurcation sphere, $\mathcal{B}$, and at spatial infinity $i^0$. When you&amp;rsquo;re interested in the behavior of fields near the black hole and far away from it (say, to study gravitational waves), it&amp;rsquo;s &lt;a href="https://anilzen.github.io/publication/zenginoglu-2011-geometric/" target="_blank" rel="noopener">better&lt;/a> to use non-intersecting time slices.&lt;/p>
&lt;p>Most useful time functions are related to the Schwarzschild time by a &amp;ldquo;height&amp;rdquo; shift that depends only on the radial coordinate:
$$ t \to t + h(r). $$
For example, the height function for &lt;a href="https://en.wikipedia.org/wiki/Gullstrand%E2%80%93Painlev%C3%A9_coordinates" target="_blank" rel="noopener">Gullstrand–Painlevé&lt;/a> coordinates is
$$ h_{\rm GP} (r) = -2 \sqrt{r} + \ln\frac{\sqrt{r}+1}{\sqrt{r}-1}. $$
Note that the height function is singular at the horizon. This singularity is needed to counteract the singularity of Schwarzschild time slices near the bifurcation sphere. Gullstrand–Painlevé gives a nice foliation of the future event horizon, $\mathcal{H}^+$, but the time slices still intersect at spatial infinity.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-schwarzschild-droste-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Schwarzschild-Droste time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_e40ae1b1bcf9878a0ebd29d8891af4da.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_ac19cab7f13834bfd457c9df7e2072af.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_e40ae1b1bcf9878a0ebd29d8891af4da.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Schwarzschild-Droste time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-gullstrandpainlevé-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Gullstrand–Painlevé time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_627235e3a36de0231d5f308663c9288a.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_e6bc86a8e8efe39e055b87f72c278f9b.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_627235e3a36de0231d5f308663c9288a.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Gullstrand–Painlevé time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We need a height function that&amp;rsquo;s singular both near and far from the black hole. This idea underlies the construction of hyperboloidal coordinates. For example,
$$ h_{\rm Hyp} (r) = \sqrt{1+ r_\ast^2}, $$
gives the hyperboloidal foliation on the left panel below. The slices do not intersect. Instead, you get a nice, smooth foliation of the full exterior domain. Another example with this nice behavior is the minimal gauge of &lt;a href="https://arxiv.org/abs/1809.02837" target="_blank" rel="noopener">Ansorg, Jaramillo, and Macedo&lt;/a>
$$ h_{\rm MG} (r) = r + 2 \ln r - \ln(r - 1). $$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-hyperbolic-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperbolic time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_619c70a07d8ef7fd1fa8eac3ec505535.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_cf6b5d0752b76d74c305fe585553582f.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_619c70a07d8ef7fd1fa8eac3ec505535.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Hyperbolic time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-minimal-gauge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Minimal gauge" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_edd5824f8e851782d2c357d381a0ccf6.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_c34845ff30f7a4ae6610769aa013eeb2.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_edd5824f8e851782d2c357d381a0ccf6.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Minimal gauge
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Similar constructions can also be made for Kerr, Reissner–Nordström, or Schwarzschild-de Sitter metrics.&lt;/p>
&lt;p>That&amp;rsquo;s it! I hope this helps for the handful of people out there who draw Penrose diagrams. If you have any questions about the diagrams or the source files, email me at &lt;a href="mailto:anil@umd.edu">anil@umd.edu&lt;/a>. I plan to write two more posts on Penrose compactification that should be of broader interest. Stay tuned!&lt;/p>
&lt;!-- These coordinates are good for plotting the maximally extended Schwarzschild solution but they are too complicated if you're primarily interested in the exterior region. You can draw a Penrose diagram for Schwarzschild just like in the Minkowski case by using the tortoise coordinate. The reason is that the $t,r$ portion of the Schwarzschild metric is conformally flat in the tortoise coordinate. Since Penrose diagrams are conformal diagrams, all the usual tricks apply. -->
&lt;!-- The Schwarzschild case reduces to the Minkowski case when we switch to the tortoise coordinate (up to a constant)
$$ r_\ast = \int \frac{dr}{f(r)} = r + 2 M \log \left(\frac{r}{2M}-1\right).$$
The Schwarzschild metric \eqref{1} becomes
$$ ds^2 = f(r(r_\ast)) (- dt^2 + dr_\ast^2) + r(r_\ast)^2 d\sigma^2. $$
Ignoring the angular part, the spacetime metric is just a conformal Minkowski metric in coordinates $(t,r_\ast)$ with in- and outgoing coordinates $t+r_\ast$ and $t-r_\ast$. For a Penrose diagram, you can consider the Schwarzschild metric to be simply $ -dt^2 + dr_\ast^2$. The main difference is the domain of $r_\ast$. While the domain of $r$ in Minkowski is $[0,\infty)$, the domain of the tortoise coordinate $r_\ast$ in Schwarzschild is $(-\infty, \infty)$. This difference is why the Minkowski conformal diagram is a triangle, whereas the conformal diagram for the exterior Schwarzschild spacetime is a diamond. -->
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>For a rigorous, mathematical definition of Penrose diagrams, see the Appendix C.2 in a &lt;a href="https://arxiv.org/abs/gr-qc/0309115" target="_blank" rel="noopener">paper&lt;/a> by Dafermos and Rodnianski.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The curious capitalization of TikZ derives from the capitalization of the German expression &amp;ldquo;TikZ is kein Zeichenprogramm.&amp;rdquo;&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Jealous of hyperbolic fish</title><link>https://anilzen.github.io/post/2022/jealous-of-fish/</link><pubDate>Sun, 23 Jan 2022 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/jealous-of-fish/</guid><description>&lt;p>Inspired by the following tweet from the great &lt;a href="https://www.quantamagazine.org/" target="_blank" rel="noopener">Quanta Magazine&lt;/a>.
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Physicists are jealous of these fish drawn by M.C. Escher. If we lived in a world with a geometry like theirs, we may have a much simpler time formulating theories about the quantum nature of gravity. &lt;a href="https://t.co/RuZFkM4pCO">https://t.co/RuZFkM4pCO&lt;/a> &lt;a href="https://t.co/2LAHKuGW8C">pic.twitter.com/2LAHKuGW8C&lt;/a>&lt;/p>&amp;mdash; Quanta Magazine (@QuantaMagazine) &lt;a href="https://twitter.com/QuantaMagazine/status/1478861128553971718?ref_src=twsrc%5Etfw">January 5, 2022&lt;/a>&lt;/blockquote>
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;/p>
&lt;h3 id="kusturica-against-wallace">Kusturica against Wallace&lt;/h3>
&lt;p>The featured image on the tweet above shows &lt;a href="https://en.wikipedia.org/wiki/M._C._Escher" target="_blank" rel="noopener">Escher&lt;/a>&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Circle_Limit_III" target="_blank" rel="noopener">Circle Limit III&lt;/a> depicting the Poincaré model of hyperbolic geometry. Escher was interested in constructing an infinitely repeating pattern in a finite figure. When the brilliant geometer &lt;a href="https://en.wikipedia.org/wiki/Harold_Scott_MacDonald_Coxeter" target="_blank" rel="noopener">Coxeter&lt;/a> shared with him a reprint of one of his lectures on hyperbolic geometry, Escher figured out how to create such &lt;a href="https://en.wikipedia.org/wiki/Tessellation" target="_blank" rel="noopener">tessalations&lt;/a>. The &lt;a href="http://www.ams.org/publicoutreach/feature-column/fcarc-circle-limit" target="_blank" rel="noopener">story&lt;/a> of how Escher managed to create these &lt;a href="https://mathstat.slu.edu/escher/index.php/Circle_Limit_Exploration" target="_blank" rel="noopener">four woodcuts&lt;/a> is fascinating.&lt;/p>
&lt;p>The original &lt;a href="https://www.quantamagazine.org/cosmologists-close-in-on-logical-laws-for-the-big-bang-20211110/" target="_blank" rel="noopener">Quanta article&lt;/a> in the tweet above reports the difficulties in constructing something like the &lt;a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence" target="_blank" rel="noopener">AdS/CFT correspondence&lt;/a> for physical spacetimes. While we have a realization of holographic duality based on anti-de Sitter spacetimes, a similar construction on flat or de Sitter spacetimes has been elusive, supposedly because these spacetimes do not possess hyperbolic geometry. A week after the tweet above, there was another &lt;a href="https://www.quantamagazine.org/symmetries-reveal-clues-about-the-holographic-universe-20220112/" target="_blank" rel="noopener">Quanta article&lt;/a> with hyperbolic geometry as its featured image. Clearly, there is some hype around hyperbolic geometry.&lt;/p>
&lt;p>The main argument is that spatial slices of AdS are hyperbolic, which seems to be essential for AdS/CFT, so living in hyperbolic geometry makes it easier for the fish in Escher&amp;rsquo;s drawing to know quantum gravity.&lt;/p>
&lt;blockquote>
&lt;p>The fish doesn&amp;rsquo;t think because the fish knows everything.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Emir_Kusturica" target="_blank" rel="noopener">Kusturica&lt;/a> (1993) from the movie &lt;a href="https://en.wikipedia.org/wiki/Arizona_Dream" target="_blank" rel="noopener">Arizona Dream&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>We are, however, not like those fish; we want to discover quantum gravity. The obstacle remains that our physical models for spacetime are either asymptotically flat (for isolated systems) or de Sitter (for cosmology).&lt;/p>
&lt;p>I argue below that there is no need to be jealous of the fish. We do live in hyperbolic geometry; we just don&amp;rsquo;t know it yet.&lt;/p>
&lt;blockquote>
&lt;p>There are these two young fish swimming along and they happen to meet an older fish swimming the other way, who nods at them and says &amp;ldquo;Morning, boys. How&amp;rsquo;s the water?&amp;rdquo; And the two young fish swim on for a bit, and then eventually one of them looks over at the other and goes &amp;ldquo;What the hell is water?&amp;rdquo;&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/David_Foster_Wallace" target="_blank" rel="noopener">Wallace&lt;/a> (2005) from his commencement speech &lt;a href="https://en.wikipedia.org/wiki/This_Is_Water" target="_blank" rel="noopener">This Is Water&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="little-fish-big-fish">Little fish, big fish&lt;/h3>
&lt;p>Let&amp;rsquo;s start with a short review of the hyperbolic fish in Escher&amp;rsquo;s drawing. Hyperbolic geometry is typically introduced in a fictitious higher dimensional Minkowski space using the equation for a &lt;a href="https://en.wikipedia.org/wiki/Hyperboloid" target="_blank" rel="noopener">hyperboloid&lt;/a> in Cartesian coordinates. I&amp;rsquo;ll be using spherical coordinates foreshadowing an application below, so we have the Minkowski space with line element
$$ ds^2_{\rm{Mink}} = -dt^2 + dr^2 + r^2 d\sigma^2,$$
where $d\sigma^2=d\sigma^2+\sin^2\theta \ d\varphi^2$, the usual metric on the unit sphere.&lt;/p>
&lt;p>Spheres and hyperboloids may seem very different, but they are analogous to each other. Just like the unit sphere is the set of points at unit Euclidean distance from the origin, the unit hyperboloid is the set of points at unit Minkowski distance from the origin: the hyperboloid is simply a (pseudo)sphere of Minkowski space
$$t^2-r^2=1. \tag{1} \label{1}$$&lt;/p>
&lt;p>We already have the first hint that we live in hyperbolic geometry! The hyperboloid describes the &lt;a href="../../hyperbolic-relativity/">kinematic space of special relativity&lt;/a>. But this was rather anticlimactic so let&amp;rsquo;s go a bit further.&lt;/p>
&lt;p>We can derive and represent the intrinsic metric of hyperbolic geometry in various equivalent ways. For example, setting $t=\cosh \xi$ and $r=\sinh \xi$ satisfies the equation for the hyperboloid (\ref{1}) identically, and gives us the hyperbolic line element
$$ ds^2_{\mathbb{H}^3} = d\xi^2+ \sinh^2\xi \ d\sigma^2.$$
Note the similarity of this hyperbolic metric with the metric on the unit sphere.
Coordinates are labels and it&amp;rsquo;s helpful to see hyperbolic geometry in different representations. Instead of introducing hyperbolic functions, let&amp;rsquo;s solve (\ref{1}) directly
$$ t = \sqrt{1+r^2}. \tag{2} \label{2} $$
Replacing the $dt$ term with the differential of this relation in the Minkowski metric, we get
$$ ds^2_{\mathbb{H}^3} = \textcolor{purple}{\frac{1}{1+r^2}dr^2 + r^2 d\sigma^2}. \tag{3} \label{3} $$&lt;/p>
&lt;p>To get Escher&amp;rsquo;s image of a disk, we need to map the hyperboloid model with its infinite range in $r$ to the unit &lt;a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model" target="_blank" rel="noopener">Poincaré disk model&lt;/a>. We relabel the radial points using
$$ r = \frac{2\rho}{1-\rho^2}, \tag{4}\label{4}$$
and obtain the familiar &lt;a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model#Metric_and_curvature" target="_blank" rel="noopener">line element for the Poincaré ball&lt;/a>&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>
$$ ds^2_{\mathbb{H}^3} = \textcolor{darkorange}{\frac{4}{(1-\rho^2)^2}\left( d\rho^2 + \rho^2 d\sigma^2 \right)}. $$
The metric blows up near the boundary, which is how Escher packs the infinitely many fish in a repeating pattern into the disk. Remember that the fish have the same size. The Poincaré disk is a conformal model of hyperbolic geometry distorting sizes but keeping the shapes (or rather local angles) invariant, so the fish actually don&amp;rsquo;t know that they are packed so densely, contrary to claims about their omniscient nature. As far as the fish are concerned, they don&amp;rsquo;t see the distortions and might even think they live in flat space&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &amp;#x1f609;.&lt;/p>
&lt;h3 id="swimming-in-anti-de-sitter">Swimming in anti-de Sitter&lt;/h3>
&lt;p>The hype around hyperbolic geometry among quanta is because cuts of AdS are hyperbolic spaces. In fact, AdS is introduced very similarly to hyperbolic geometry&amp;mdash;starting with a fictitious higher dimensional Minkowski space. This Minkowski space, however, has two time dimensions (which is rather awkward): $ds^2 = -dt_1^2 - dt_2^2 + dr^2 + r^2 d\sigma^2$. We write the hyperboloid as $-t_1^2-t_2^2+r^2=-1$. This relation can be solved identically by $t_1 = \sqrt{1+r^2} \cos t$ and $t_2 = \sqrt{1+r^2} \sin t$, giving the &lt;a href="https://en.wikipedia.org/wiki/Anti-de_Sitter_space#Global_coordinates" target="_blank" rel="noopener">AdS metric in global coordinates&lt;/a>
$$ ds^2_{\rm{AdS}} = - \left(1+r^2\right) dt^2 + \textcolor{purple}{\frac{1}{1+r^2} dr^2 + r^2 d\sigma^2}.$$
AdS spacetime has many &lt;a href="https://arxiv.org/abs/1611.01118" target="_blank" rel="noopener">counterintuitive aspects&lt;/a>; it&amp;rsquo;s not even stable against small &lt;a href="https://arxiv.org/abs/1312.5544" target="_blank" rel="noopener">perturbations&lt;/a>. But you can immediately see that its $t$-slices have hyperbolic geometry by comparing them to the metric (\ref{3}), which is the crucial aspect for quantum gravity.&lt;/p>
&lt;p>I promised you to end your jealousy, but so far, I&amp;rsquo;ve just been pressing on the wound. The question remains: how do we go from our usual flat spacetime with line element
$$ ds^2_{\rm{Mink}} = -dt^2 + dr^2 + r^2 d\sigma^2$$
to the hyperbolic three-metric as time slices? Yes, we&amp;rsquo;ve encountered the kinematic space of special relativity, but that&amp;rsquo;s not a spacetime metric&amp;mdash;time is missing! Considering the usual time slices, there is nothing hyperbolic about the flat geometry of $dr^2 + r^2 d\sigma^2$.&lt;/p>
&lt;h3 id="time-heals-all-wounds">Time heals all wounds&lt;/h3>
&lt;p>In relativity, there is no notion of absolute time; we can develop our own simultaneity to connect observers and call it a proper day. Therefore, the geometry of the three-space connecting these observers is also not invariant but depends on a choice of time. Many familiar notions such as simultaneity, volume, area become unfamiliar through such reparametrizations of spacetime.&lt;/p>
&lt;p>The choice of time also determines whether the geometry of the three-space is Euclidean or hyperbolic. We are free to pick our preferred time coordinate, so let&amp;rsquo;s pick a time that makes space hyperbolic and end the jealousy.&lt;/p>
&lt;p>We&amp;rsquo;ve already seen this when we encountered the kinematic space of special relativity in our derivation of the hyperboloid model of hyperbolic geometry. Now, instead of finding intrinsic coordinates as in (\ref{2}), we introduce a new time coordinate as
$$ \tau = t - \sqrt{1+r^2}. \tag{5} \label{5} $$
The resulting metric is not a three-dimensional intrinsic spatial metric but a transformed Minkowski metric with timelike, null, and spacelike directions
$$ ds^2_{\rm{Mink}} = -d\tau^2 - \frac{2r}{\sqrt{1+r^2}} d\tau dr + \textcolor{purple}{\frac{1}{1+r^2} dr^2 + r^2 d\sigma^2}.$$
Voilà! The $\tau$ slices have hyperbolic geometry. Relabeling the radial points as in (\ref{4}) brings us to a foliation whose sections are the Poincaré balls
$$ ds^2_{\rm{Mink}} = -d\tau^2 + \frac{8\rho}{(1-\rho^2)^2} d\tau d\rho + \textcolor{darkorange}{\frac{4}{(1-\rho^2)^2} \left( d\rho^2 + \rho^2 d\sigma^2\right)}.$$
Note that this metric is still the flat Minkowski metric with all its usual geometric properties, such as vanishing curvature. The important difference is that the points along the &lt;a href="https://en.wikipedia.org/wiki/End_%28topology%29" target="_blank" rel="noopener">ideal boundary&lt;/a> do not sit at spatial infinity but null infinity. The cuts $\rho=1$ along each $\tau$ slice corresponds to what is called a celestial sphere. This allows us to make the connection back to quantum gravity, because the celestial sphere is indeed the stage for an attempt at a &lt;a href="../../hyperboloidal-holography/">holographic description&lt;/a> called &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">celestial holography&lt;/a>.&lt;/p>
&lt;p>The ideal boundary seems like an ideal place for quantum gravity.&lt;/p>
&lt;h3 id="hyperbolic-de-sitter">Hyperbolic de Sitter&lt;/h3>
&lt;p>The &lt;a href="https://www.quantamagazine.org/cosmologists-close-in-on-logical-laws-for-the-big-bang-20211110/" target="_blank" rel="noopener">Quanta article&lt;/a> discusses the difficulties in constructing an AdS/CFT correspondence in de Sitter spacetimes. Again, we can use coordinates to make the time slices of de Sitter hyperbolic, but the transformations are a bit tricky because de Sitter space has a different conformal geometry. For example, conformal infinity is spacelike (as opposed to timelike for AdS and null for Minkowski).&lt;/p>
&lt;p>There is another infinity in de Sitter that depends on coordinates and is useful for us. Consider the de Sitter metric in static coordinates
$$ ds^2_{\rm{dS}} = - \left(1-r^2\right) dt^2 + \frac{1}{1-r^2} dr^2 + r^2 d\sigma^2.$$
The singularity at $r=1$ corresponds to the &lt;a href="https://en.wikipedia.org/wiki/Cosmological_horizon" target="_blank" rel="noopener">cosmological horizon&lt;/a>. The horizon singularity is a coordinate singularity similar to the singularity at the black hole horizon of Schwarzschild spacetime.&lt;/p>
&lt;p>Here&amp;rsquo;s the transformation that gets us to hyperbolic time slices
$$ r = \sinh \xi \sinh \tau, \qquad \cosh t = \frac{\cosh \tau}{\sqrt{1-\sinh^2 \xi \sinh^2\tau}} $$
The transformed metric is comparatively simple
$$ ds^2_{\rm{dS}} = - d\tau^2 + \sinh ^2 \tau \left( d\xi^2 + \sinh^2\xi\ d\sigma^2 \right).$$
We recognize the hyperbolic geometry on $\tau=$const. slices. Alternatively, we can abuse notation and use the same letter for $r=\sinh\xi$ to get the purple representation of the hyperbolic metric
$$ ds^2_{\rm{dS}} = - d\tau^2 + \sinh ^2 \tau \left( \textcolor{purple}{\frac{1}{1+r^2} dr^2 + r^2 d\sigma^2} \right).$$
The Poincaré disk follows from a simple relabeling of the radial points (\ref{4}). This representation of the de Sitter metric is called &lt;a href="https://en.wikipedia.org/wiki/De_Sitter_space#Open_slicing" target="_blank" rel="noopener">open slicing&lt;/a>. I&amp;rsquo;m ignoring subtleties, such as the domain of transformations and the region covered by coordinates in the conformal diagram. We still see that we can choose to live in hyperbolic geometry if we want to, even in de Sitter spacetimes.&lt;/p>
&lt;p>The problem with the de Sitter metric in open slicing is its time-dependence. Especially for numerical studies, coordinates in which the metric is explicitly static are preferable. A better approach to construct a hyperbolic slicing is to consider the static patch and make a transformation similar to the flat case. First we push the horizon to infinity
$$ r_\ast = \int \frac{1}{1-r^2}dr = \mathrm{arctanh} \ r. $$
The de Sitter metric on the static patch becomes
$$ ds^2_{\mathrm{dS}} = \frac{1}{\cosh^2 r_\ast} \left( -dt^2 + dr_\ast^2 \right) + \tanh^2 r_\ast d\sigma^2. $$
Now we can do the same transformation as in (\ref{5}), take $\tau=$const., and get the following metric on the time slices
$$ \frac{1}{\cosh^2 r_\ast} \left(\frac{1}{1+r_\ast^2} dr_\ast^2 + \sinh^2 r_\ast d\sigma^2 \right). $$
This metric is not quite the hyperbolic metric, but has other nice properties for numerical calculations that I&amp;rsquo;ll write about in another post.&lt;/p>
&lt;h3 id="this-is-water">This is Water&lt;/h3>
&lt;p>The point of this post is that you can find suitable coordinates that make space hyperbolic. There is nothing to be jealous about Escher&amp;rsquo;s fish. It may well be that physically motivated time slices of flat spacetime are naturally hyperbolic but we don&amp;rsquo;t experience it because the speed of light is so large.&lt;/p>
&lt;p>Are these time transformations physically motivated? What do they mean? Doesn&amp;rsquo;t a preferred time slice contradict the principle of covariance? I&amp;rsquo;ll leave those questions for another post.&lt;/p>
&lt;p>Enjoy the swim.&lt;/p>
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>To get the disk model, just replace $d\sigma^2$ with $d\theta^2$.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The fish could, in principle, find out that they do live in hyperbolic space by making local measurements of &lt;em>intrinsic&lt;/em> curvature.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Special relativity as hyperbolic geometry</title><link>https://anilzen.github.io/post/hyperbolic-relativity/</link><pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/hyperbolic-relativity/</guid><description>&lt;p>Space and time bend and stretch with every move.&lt;/p>
&lt;p>I was blown away by this as a young high schooler interested in physics and philosophy. To me, the recognition that natural laws dynamically govern space and time was a triumph of physics over philosophy, of Einstein over Kant, of empirical determination over &lt;a href="https://en.wikipedia.org/wiki/Analytic%E2%80%93synthetic_distinction#Kant%27s_version_and_the_a_priori_/_a_posteriori_distinction" target="_blank" rel="noopener">synthetic a priori&lt;/a>.&lt;/p>
&lt;p>As fascinated as I was by special relativity, the formalism seemed relatively obscure. I recognized patterns in the &lt;a href="https://en.wikipedia.org/wiki/Lorentz_transformation" target="_blank" rel="noopener">Lorentz transformations&lt;/a> but missed a deeper insight into the origins of this structure. At college, I considered various standard derivations of relativistic effects, such as the &lt;a href="https://en.wikipedia.org/wiki/Relativistic_aberration" target="_blank" rel="noopener">relativistic aberration&lt;/a> or the &lt;a href="https://en.wikipedia.org/wiki/Thomas_precession" target="_blank" rel="noopener">Thomas precession&lt;/a>, tedious and boring.&lt;/p>
&lt;p>A simple example is Einstein&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Velocity-addition_formula" target="_blank" rel="noopener">velocity-addition formula&lt;/a>. In Galilean relativity, which corresponds to our everyday experience, we can just add velocities. In special relativity, the speed of light is constant, so the addition of velocities must be such that no composition of velocities exceeds the speed of light. In the simplest case of collinear motion (velocities pointing in the same direction), velocity addition becomes speed addition and looks like this
$$ \tag{1} \label{1}
u = \frac{u_1+u_2}{1+\tfrac{u_1 u_2}{c^2}} $$
The speed of light, $c$, can never be exceeded with this composition law. While we can understand the result, it is not intuitively clear how this addition rule is related to the structure of space and time.&lt;/p>
&lt;p>It turns out that relativistic calculations have natural interpretations in hyperbolic geometry. This viewpoint not only simplifies the calculations but also provides deeper insights into the theory. Its power extends beyond special relativity into machine learning and general relativity. And it all starts with Minkowski.&lt;/p>
&lt;h3 id="hyperbole">Hyperbole&lt;/h3>
&lt;p>In 1908, &lt;a href="https://en.wikipedia.org/wiki/Hermann_Minkowski" target="_blank" rel="noopener">Minkowski&lt;/a> gave a &lt;a href="https://www.math.nyu.edu/~tschinke/papers/yuri/14minkowski/raum-und-zeit.pdf" target="_blank" rel="noopener">lecture&lt;/a> about a new mathematical model for space and time. In his opening remarks, he stated boldly that, from now on, space and time are mere shadows of a more fundamental union: spacetime.&lt;/p>
&lt;blockquote>
&lt;p>Henceforth space by itself, and time by itself, are doomed to fade away into mere shadows, and only a kind of union of the two will preserve an independent reality.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Hermann_Minkowski" target="_blank" rel="noopener">Minkowski&lt;/a> (1908)&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;/blockquote>
&lt;p>The quote is by now famous. It has made its way to various outlets since appearing on the cover page of Synge&amp;rsquo;s &lt;a href="https://www.amazon.com/Relativity-Special-J-L-Synge/dp/B000GP7PX4" target="_blank" rel="noopener">textbook on relativity&lt;/a> in 1956. But when Minkowski made the statement, it was considered hyperbole and encountered resistance from physicists and mathematicians alike.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The spacetime view became widely celebrated a few years after the lecture because of its central role in general relativity. In the same lecture, Minkowski also promoted a non-Euclidean approach to spacetime. As a Göttingen mathematics professor, he recognized the non-Euclidean geometry of special relativity. This idea, however, didn&amp;rsquo;t gain traction as widely as the idea of spacetime. Only in the last 20 years has there been renewed interest in the hyperbolic nature of special relativity.&lt;/p>
&lt;p>To see why hyperbolic geometry is the natural geometry for special relativity, consider a two-dimensional spacetime with coordinates $(t,x)$ and Minkowski metric
$$ ds^2 = -dt^2 + dx^2. $$
A natural object for a metric is the set of points at unit distance from the origin. In two-dimensional Euclidean space, we get a circle, $x^2+y^2=1$. In two-dimensional Minkowski space, we get a hyperbola&lt;br>
$$ \tag{2}\label{2} t^2 - x^2 = 1. $$
Minkowski&amp;rsquo;s drawing below shows this hyperbola for $t&amp;gt;0$. In higher dimensions, you get the one-sheeted &lt;a href="https://en.wikipedia.org/wiki/Hyperboloid" target="_blank" rel="noopener">hyperboloid&lt;/a>, also called hyperbolic hyperboloid.&lt;/p>
&lt;p>
&lt;figure id="figure-minkowskis-drawing-depicts-hyperbola-as-the-invariant-space-under-lorentz-transformations-from-minkowskis-modern-worldhttpshalshsarchives-ouvertesfrhalshs-01234434-by-scott-walter">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Minkowski Drawing" srcset="
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_b89ce1e1b459df492f891c8a09eaf42e.webp 400w,
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_4324b0371c578b6a70ed333dddf1eab4.webp 760w,
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_b89ce1e1b459df492f891c8a09eaf42e.webp"
width="473"
height="500"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Minkowski&amp;rsquo;s drawing depicts hyperbola as the invariant space under Lorentz transformations. &lt;br>From &lt;a href="https://halshs.archives-ouvertes.fr/halshs-01234434/" target="_blank" rel="noopener">Minkowski&amp;rsquo;s modern world&lt;/a> by Scott Walter.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>In his lecture, Minkowski demonstrated that Lorentz transformations are hyperbolic rotations that leave the hyperbola (\ref{2}) invariant. He died in January 1909 before he could develop this idea further, but it was embraced by a few other scientists, particularly by &lt;a href="https://en.wikipedia.org/wiki/Arnold_Sommerfeld" target="_blank" rel="noopener">Sommerfeld&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Vladimir_Vari%C4%87ak" target="_blank" rel="noopener">Varičak&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Alfred_Robb" target="_blank" rel="noopener">Robb&lt;/a>, and &lt;a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel" target="_blank" rel="noopener">Borel&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>The principle of relativity corresponds to the hypothesis that the kinematic space is a space of constant negative curvature, the space of Lobachevsky and Bolyai. The value of the radius of curvature is the speed of light.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel" target="_blank" rel="noopener">Borel&lt;/a> (1913)&lt;/p>
&lt;/blockquote>
&lt;p>The mathematical formalism for a hyperbolic rotation is just like a regular rotation, except with hyperbolic functions instead of trigonometric functions. &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">Hyperbolic functions&lt;/a> are related to their trigonometric counterparts through &lt;a href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Complex_trigonometric_definitions" target="_blank" rel="noopener">complex angles&lt;/a>. An intuitive way to think about the hyperbolic rotation is the &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">squeeze mapping&lt;/a>, which should be familiar to anyone who has seen a Lorentz transformation on a spacetime diagram.&lt;/p>
&lt;p>
&lt;figure id="figure-lorentz-boosts-are-squeeze-mappingshttpsenwikipediaorgwikisqueeze_mapping-from-wikimediahttpscommonswikimediaorgwikifileminkboost2gif">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Squeeze Mapping"
src="https://anilzen.github.io/post/hyperbolic-relativity/MinkBoost.gif"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Lorentz boosts are &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">squeeze mappings&lt;/a>. &lt;br>From &lt;a href="https://commons.wikimedia.org/wiki/File:MinkBoost2.gif" target="_blank" rel="noopener">Wikimedia&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Consider the Lorentz boost with relative speed $v$ between frames
$$ L(v)= \begin{pmatrix}
\gamma &amp;amp; -\gamma \beta \\
-\gamma \beta &amp;amp; \gamma
\end{pmatrix}, $$
where we defined the rescaled speed $\beta:=\tfrac{v}{c}$ and the Lorentz factor $\gamma:=1/\sqrt{1-\beta^2}$. We can write it as a hyperbolic rotation with angle $w$:
$$ L(w)= \begin{pmatrix}
\cosh w &amp;amp; -\sinh w \\
-\sinh w &amp;amp; \cosh w
\end{pmatrix}, $$
where the Lorentz factor becomes $\gamma=\cosh w$ and the rescaled speed becomes $\beta=\tanh w$. So the Lorentz transformation is simply a rotation in hyperbolic space with an angle, $w$, related to the observer&amp;rsquo;s speed as $v=c \ \mathrm{arctanh}\ w$. Its inverse is just a rotation with the inverse angle, $L(w)^{-1}=L(-w)$. We can directly add the hyperbolic angles instead of dealing with weird addition formulas because composition of rotations along the same dimension is additive $L(w_1)L(w_2)=L(w_1+w_2)$.&lt;/p>
&lt;p>Physicists usually think in terms of velocities and speeds, not hyperbolic angles. Writing the angle addition in terms of speeds, we get
$$ \beta = \tanh (w_1+w_2) = \frac{\tanh w_1+\tanh w_2}{1+\tanh w_1 \tanh w_2} = \frac{\beta_1+\beta_2}{1+\beta_1 \beta_2}. $$
Einstein&amp;rsquo;s velocity addition (\ref{1}) becomes a consequence of hyperbolic identities.&lt;/p>
&lt;p>There is much more that one can simplify or understand with this viewpoint. For example, the formula for the Doppler shift is a simple $e^w$. The non-commutativity of general Lorentz boosts in higher dimensions becomes immediately apparent: Lorentz boosts do not commute just like rotations do not commute. The Thomas precession becomes easier to visualize as the consequence of parallel transport of vectors in the curved velocity space (&lt;a href="https://en.wikipedia.org/wiki/Holonomy" target="_blank" rel="noopener">holonomy transformation&lt;/a>). Many other calculations and ideas in special relativity translate to simple exercises in hyperbolic geometry.&lt;/p>
&lt;p>If you&amp;rsquo;re intrigued, follow the rabbit. Scott Walter&amp;rsquo;s papers are excellent for the historical developments: &lt;a href="https://halshs.archives-ouvertes.fr/halshs-00319209/" target="_blank" rel="noopener">Hermann Minkowski and the scandal of spacetime&lt;/a>, &lt;a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.6918&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="noopener">The Non-Euclidean Style of Minkowskian Relativity&lt;/a>, and &lt;a href="https://halshs.archives-ouvertes.fr/halshs-01234434/" target="_blank" rel="noopener">Minkowski&amp;rsquo;s modern world&lt;/a>. There are some gems in Barrett&amp;rsquo;s &lt;a href="https://arxiv.org/abs/1102.0462" target="_blank" rel="noopener">The Hyperbolic Theory of Special Relativity&lt;/a>. Rhodes and Semon go into those tedious calculations in &lt;a href="https://arxiv.org/abs/gr-qc/0501070" target="_blank" rel="noopener">Relativistic velocity space, Wigner rotation and Thomas precession&lt;/a>. Tevian Dray&amp;rsquo;s textbook on &lt;a href="https://www.amazon.com/Geometry-Special-Relativity-Tevian-Dray/dp/1466510471" target="_blank" rel="noopener">The Geometry of Special Relativity&lt;/a> from 2012 takes the hyperbolic viewpoint to teach special relativity.&lt;/p>
&lt;p>Ungar probably has the most influential and extensive work in this space. His books on &lt;a href="https://en.wikipedia.org/wiki/Gyrovector_space" target="_blank" rel="noopener">gyrovector space&lt;/a> present relativistic calculations in hyperbolic geometry from a solid group-theoretical viewpoint. Two of his relevant books are &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/6625" target="_blank" rel="noopener">Analytic Hyperbolic Geometry and Albert Einstein&amp;rsquo;s Special Theory of Relativity&lt;/a> from 2008 and &lt;a href="https://link.springer.com/book/10.1007/0-306-47134-5" target="_blank" rel="noopener">Beyond the Einstein Addition Law and its Gyroscopic Thomas Precession&lt;/a> from 2012. This work has even found applications in machine learning.&lt;/p>
&lt;h3 id="beyond-special">Beyond special&lt;/h3>
&lt;p>The hyperbolic view opens a new window to a universe beyond rewriting calculations in special relativity.&lt;/p>
&lt;p>Hyperbolic geometry efficiently represents &lt;a href="https://arxiv.org/abs/1006.5169" target="_blank" rel="noopener">hierarchical relationships and complex networks&lt;/a>. &lt;a href="https://arxiv.org/abs/1705.08039" target="_blank" rel="noopener">Nickel and Kiela&lt;/a> demonstrated in 2017 that hyperbolic geometry is better than Euclidean geometry when applying machine learning to complex data structures. In a &lt;a href="https://arxiv.org/abs/1806.03417" target="_blank" rel="noopener">follow-up paper&lt;/a>, they compared different hyperbolic models for learning hierarchical relationships and found that the Lorentz model is more efficient than the Poincaré model. Around the same time, &lt;a href="https://arxiv.org/abs/1805.09112" target="_blank" rel="noopener">Ganea, Bécigneul, and Hofmann&lt;/a> derived hyperbolic versions of deep learning tools by using Ungar&amp;rsquo;s gyrovector formalism.&lt;/p>
&lt;p>These papers opened up a new research direction in machine learning on &lt;a href="https://arxiv.org/abs/2101.04562" target="_blank" rel="noopener">hyperbolic deep neural networks&lt;/a>. The researchers use tools developed specifically for special relativity, such as Lorentz transformations and gyrovector spaces, to improve machine learning algorithms. Physicists usually associate velocities and universal speed limits with Lorentz transformations. It is fascinating that Lorentz transformations arise in problems without a notion of motion.&lt;/p>
&lt;p>Another application closer to my heart is hyperbolic geometry in general relativity. The spacetime of special relativity is special: we can identify Minkowski space with its tangent space which allows us to talk of spacetime events as four-vectors and apply global Lorentz transformations. These properties do not generalize to arbitrary Lorentzian manifolds. It&amp;rsquo;s not immediately evident that the hyperbolic viewpoint is still valuable when the spacetime is curved.&lt;/p>
&lt;p>As we have seen above, Minkowski&amp;rsquo;s hyperboloid (\ref{2}) consists of points at unit proper distance from the origin. Hyperboloids are as natural in Minkowski space as spheres are in Euclidean space. So &lt;strong>hyperboloidal&lt;/strong> coordinates may be as natural in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds.&lt;/p>
&lt;p>How can we construct such hyperboloidal coordinates? In many applications, we need to separate spacetime into space and time. Let&amp;rsquo;s separate spacetime into hyperbolic space and time in non-Euclidean style. One approach is to take a hint from spherical coordinates. Generalize the hyperbola from (\ref{2}) to an arbitrary radius of curvature
$$ t^2-x^2 = \tau^2, $$
and use $\tau$ as a new coordinate. Setting $x=\tau \ \sinh \rho$ gives us the &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne universe&lt;/a> with metric
$$ ds^2 = -d\tau^2 + \tau^2 d\xi^2.$$
These coordinates have various use cases today, including &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">quantum field theory&lt;/a>, &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">holography&lt;/a>, and the theory of &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/9427" target="_blank" rel="noopener">partial differential equations&lt;/a>.&lt;/p>
&lt;p>Another approach is to shift the hyperbola (\ref{2}) along the time direction by $\tau$ recognizing the special role of time
$$ (\tau - t)^2 - x^2 = 1 $$
Using $\tau$ as the new time coordinate, the Minkowski metric becomes
$$ ds^2 = -d\tau^2 + \frac{2x}{\sqrt{1+x^2}} d\tau dx + \frac{1}{1+x^2} dx^2.$$
Level sets of $\tau$ are hyperbolic spaces. We can bring the metric into the more recognizable Poincaré form by the spatial transformation $x=2 \rho/(1-\rho^2)$.
$$ ds^2 = -d\tau^2 + \frac{4}{(1-\rho^2)^2} \left(2\rho d\tau d\rho + d\rho^2\right).$$
This construction generalizes to &lt;a href="../../publication/zenginoglu-2008-hyperboloidal/">curved spacetimes&lt;/a>. It&amp;rsquo;s favorable for numerically solving wave equations because the coefficients don&amp;rsquo;t depend on time. Some of the problems appearing in standard coordinates, such as the outer boundary and radiation extraction problems, &lt;a href="../../publication/zenginoglu-2011-hyperboloidal/">don&amp;rsquo;t arise with this approach&lt;/a> because we can solve the wave equation on the infinite spatial domain $\rho\in[0,1]$.&lt;/p>
&lt;p>Hyperboloidal coordinates may seem unnatural at first, but that&amp;rsquo;s because we are used to thinking about space and time in Newtonian terms. Once you spend some time with them, you see that hyperboloidal coordinates are as valuable in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds. I devoted most of my &lt;a href="../../publication/">work&lt;/a> exploring properties of these surfaces and I find their relation to special relativity fascinating.&lt;/p>
&lt;h3 id="hyperclusion">Hyperclusion&lt;/h3>
&lt;p>Physics doesn&amp;rsquo;t care about how we make calculations in special relativity. The Wikipedia page for the &lt;a href="https://en.wikipedia.org/wiki/History_of_Lorentz_transformations" target="_blank" rel="noopener">history of Lorentz transformations&lt;/a> lists many equivalent formalisms. We could keep the usual relativistic formulas using velocities and Lorentz factors, or employ &lt;a href="https://en.wikipedia.org/wiki/Bondi_k-calculus" target="_blank" rel="noopener">Bondi&amp;rsquo;s $k$-calculus&lt;/a>, or &lt;a href="https://en.wikipedia.org/wiki/Gyrovector_space" target="_blank" rel="noopener">Ungar&amp;rsquo;s gyrovector space&lt;/a>. Why choose one over the other?&lt;/p>
&lt;p>It may be just personal preference. How do you understand? I understand by analogy, so it&amp;rsquo;s helpful to me when formalisms lend themselves to connections to other fields.&lt;/p>
&lt;p>There is another, much more relevant reason to prefer the hyperbolic viewpoint. As scientists, we want powerful descriptions. The same thinking style should apply to a broad class of phenomena. In other words, we want to avoid overfitting our formalism to the phenomena at hand. Hyperbolic calculations translate into machine learning, spacetime curvature, and maybe other areas to be discovered. The non-Euclidean approach with hyperbolic angles seems more powerful than the usual approach with velocities.&lt;/p>
&lt;!-- [^3]: Scott Walter wrote a wonderful historical account of these developments in ["The Non-Euclidean Style of Minkowskian Relativity"](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.6918&amp;rep=rep1&amp;type=pdf).
[^4]: Walter, Scott. ["Minkowski’s modern world."](https://halshs.archives-ouvertes.fr/halshs-01234434/) Minkowski Spacetime: A Hundred Years Later. Springer, Dordrecht, 2010. 43-61. -->
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The lecture was given on September 21, 1908 in Köln during a meeting of researchers of nature (the lovely German compound word for such a meeting is Naturforscherversammlung). The &lt;a href="https://www.math.nyu.edu/~tschinke/papers/yuri/14minkowski/raum-und-zeit.pdf" target="_blank" rel="noopener">German version&lt;/a> sounds stronger in its proclamation: &lt;em>&amp;ldquo;Von Stund&amp;rsquo; an sollen Raum für sich und Zeit für sich völlig zu Schatten herabsinken und nur noch eine Art Union der beiden soll Selbständigkeit bewahren.&amp;rdquo;&lt;/em>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Walter, Scott. &amp;ldquo;&lt;a href="https://halshs.archives-ouvertes.fr/halshs-00319209/" target="_blank" rel="noopener">Hermann Minkowski and the scandal of spacetime.&lt;/a>&amp;rdquo; ESI News 3.1 (2008): 6-8.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Hyperboloidal Holography</title><link>https://anilzen.github.io/post/hyperboloidal-holography/</link><pubDate>Thu, 07 Oct 2021 00:00:16 -0400</pubDate><guid>https://anilzen.github.io/post/hyperboloidal-holography/</guid><description>&lt;h3 id="brief-history-of-bekenstein-hawking-black-hole-entropy">Brief History of Bekenstein-Hawking Black-Hole Entropy&lt;/h3>
&lt;p>When physicists want to understand something, they consider idealizations and extremes. To understand spacetime then, consider black holes.&lt;/p>
&lt;blockquote>
&lt;p>The black holes of nature are the most perfect macroscopic objects there are in the universe:
the only elements in their construction are our concepts of space and time.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Subrahmanyan_Chandrasekhar" target="_blank" rel="noopener">Chandrasekhar&lt;/a> (1983)&lt;/p>
&lt;/blockquote>
&lt;p>Black holes are simple with just two properties: mass and rotation. By dropping an object into a black hole, you can increase its mass and change its rotation.&lt;/p>
&lt;p>The simplicity of black holes contradicts the second law of thermodynamics, which states that &lt;em>total entropy never decreases&lt;/em>. If black holes are that simple, you can reduce total entropy by dropping a high-entropy object, say a hot teacup, into the black hole.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/John_Archibald_Wheeler" target="_blank" rel="noopener">Wheeler&lt;/a>, best known for coining the term &amp;ldquo;black hole,&amp;rdquo; tasked one of his graduate students at Princeton, the young &lt;a href="https://en.wikipedia.org/wiki/Jacob_Bekenstein" target="_blank" rel="noopener">Bekenstein&lt;/a>, with resolving this paradox. The &lt;a href="https://en.wikipedia.org/wiki/History_of_general_relativity#Golden_age" target="_blank" rel="noopener">golden age&lt;/a> of relativity was in full swing around 1970, and Princeton was among the best places to be. &lt;a href="https://en.wikipedia.org/wiki/Demetrios_Christodoulou" target="_blank" rel="noopener">Christodoulou&lt;/a>, another student of Wheeler, and &lt;a href="https://en.wikipedia.org/wiki/Stephen_Hawking" target="_blank" rel="noopener">Hawking&lt;/a> had just shown that &lt;em>the area of a black hole horizon never decreases&lt;/em>. To resolve the entropy paradox, Bekenstein conjectured that black holes must have entropy and that the entropy must be proportional to the area of the black hole horizon in Planck units. The proportionality constant was determined by the discovery of &lt;a href="https://en.wikipedia.org/wiki/Hawking_radiation" target="_blank" rel="noopener">Hawking temperature&lt;/a>, which is why we call it the &lt;a href="http://www.scholarpedia.org/article/Bekenstein-Hawking_entropy" target="_blank" rel="noopener">Bekenstein-Hawking&lt;/a> Black-Hole entropy
$$ S_{BH} = \frac{A}{4 L_P^2}$$
where $A$ is the horizon area, $L_P$ is the &lt;a href="https://en.wikipedia.org/wiki/Planck_length" target="_blank" rel="noopener">Planck length&lt;/a>, $S$ stands for entropy (probably after &lt;a href="https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot" target="_blank" rel="noopener">Sadi Carnot&lt;/a>), and BH stands for &lt;a href="#brief-history-of-bekenstein-hawking-black-hole-entropy">Brief History&lt;/a>.&lt;/p>
&lt;p>You may think this is a curious result about curious objects called black holes with little relevance to our notion of spacetime. But consider the following question: what is the maximum entropy in a finite, say spherical region? If you take some high-entropy matter and squeeze it spherically (with your mind), eventually, you hit the Bekenstein-Hawking entropy, and the compressed matter forms a black hole. This thought experiment leads to the surprising conclusion that the entropy of any matter system is bound by the surface area of the smallest sphere that encloses it&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We started with black holes and arrived at the&lt;/p>
&lt;h3 id="holographic-principle">Holographic Principle&lt;/h3>
&lt;p>Counterintuitively, the maximal entropy of a region scales with the area of its boundary, not its volume. The &lt;a href="https://en.wikipedia.org/wiki/Holographic_principle" target="_blank" rel="noopener">holographic principle&lt;/a> pushes this area-dependence further, suggesting that the dynamics in a volume of space is encoded on the boundary of that space. In other words, the information content of a spacetime region is encoded on the surface of that region.&lt;/p>
&lt;p>Take a break and think about this for a moment. How can our three-dimensional world be encoded on a two-dimensional surface? What does it mean? Is the universe a &lt;a href="https://en.wikipedia.org/wiki/Simulation_hypothesis" target="_blank" rel="noopener">game&lt;/a> running on the screen of an alien child? Are we trapped in &lt;a href="https://en.wikipedia.org/wiki/The_Matrix" target="_blank" rel="noopener">The Matrix&lt;/a>? Are we &lt;a href="https://www.youtube.com/watch?v=7nqcL0mjMjw" target="_blank" rel="noopener">Livin&amp;rsquo; on the Edge&lt;/a>?&lt;/p>
&lt;p>
&lt;figure id="figure-aerosmith-livin-on-the-edge-video-1993">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Living on the Edge" srcset="
/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_c7a240253d106fdf4dedfdaa3a7ac972.webp 400w,
/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_207a77ccf2abf58e87d6f804fcaac0b9.webp 760w,
/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/living_on_the_edge_hu173cdc4707bba8c72fbdcae6744ed276_37858_c7a240253d106fdf4dedfdaa3a7ac972.webp"
width="722"
height="531"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Aerosmith: Livin&amp;rsquo; on the Edge (Video 1993)
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="adscft-correspondence">AdS/CFT correspondence&lt;/h3>
&lt;p>The most famous realization of the holographic principle is the &lt;a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence" target="_blank" rel="noopener">AdS/CFT correspondence&lt;/a>. Conjectured by &lt;a href="https://en.wikipedia.org/wiki/Juan_Mart%C3%ADn_Maldacena" target="_blank" rel="noopener">Maldacena&lt;/a> in 1997, the correspondence suggests an equivalence between a &lt;a href="https://en.wikipedia.org/wiki/String_theory" target="_blank" rel="noopener">string theory&lt;/a> in curved anti-de Sitter spacetime (AdS) and a strongly coupled &lt;a href="https://en.wikipedia.org/wiki/Conformal_field_theory" target="_blank" rel="noopener">conformal field theory&lt;/a> in a lower-dimensional flat spacetime. Aside from the details, the remarkable aspect of the conjecture is the duality between the bulk and the boundary of AdS.&lt;/p>
&lt;p>Below is a visual image for the AdS$_3$/CFT$_2$ correspondence. The cylinder represents the three-dimensional AdS$_3$ spacetime. On the boundary of the cylinder lives the 1+1 dimensional CFT$_2$. Slices of constant time are &lt;a href="https://en.wikipedia.org/wiki/Hyperbolic_manifold" target="_blank" rel="noopener">hyperbolic manifolds&lt;/a> with negative curvature, drawn as &lt;a href="https://en.wikipedia.org/wiki/Poincar%C3%A9_disk_model" target="_blank" rel="noopener">Poincaré disks&lt;/a>. A gravity theory within the disks is dual to a gravity-free theory on the bounding circle. The boundary of space encodes the bulk.&lt;/p>
&lt;p>
&lt;figure id="figure-ads-is-hyperbolic-space-with-a-time-direction-wikimediahttpscommonswikimediaorgwikifileads3svg">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="AdS3" srcset="
/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_61970cad7d4d717edc3a67b3ac88af73.webp 400w,
/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_352cb1eb5025a0154dc7b8a7e0e7b971.webp 760w,
/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/AdS3_hu87bd1888389a6c14d9cc1117d0109a96_71192_61970cad7d4d717edc3a67b3ac88af73.webp"
width="472"
height="480"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
AdS is hyperbolic space with a time direction. &lt;a href="https://commons.wikimedia.org/wiki/File:AdS3.svg" target="_blank" rel="noopener">Wikimedia&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The negative curvature of AdS is a feature and a bug. It allows the realization of the holographic principle due to its hyperbolic slices, but it also makes AdS an unphysical model for our universe. At cosmological scales, the universe has positive curvature and resembles de Sitter space. At astrophysical and smaller scales, isolated systems are modeled by asymptotically flat spacetimes. Either way, anti-de Sitter spacetime, with all its &lt;a href="https://arxiv.org/abs/1611.01118" target="_blank" rel="noopener">bizarre&lt;/a> features, is not a good model for physical reality.&lt;/p>
&lt;p>Can we establish the holographic principle for isolated systems?&lt;/p>
&lt;h3 id="hyperbolic-hyperboloidal">Hyperbolic Hyperboloidal&lt;/h3>
&lt;p>Consider the AdS$_3$ metric in static coordinates with curvature scale $L$
$$ ds^2_{\rm{AdS}} = - \left(L^2+r^2\right) d\tau^2 + \textcolor{RedViolet}{\frac{L^2}{L^2+r^2} dr^2 + r^2 d\theta^2},$$
Slices of constant time are hyperbolic spaces with negative curvature. Hyperbolic geometry has much more space near its conformal boundary than Euclidean space. It seems that flat spacetime with its Euclidean time slices and vanishing curvature would not allow such a description
$$ ds^2_{\rm{Mink}} = -dt^2 + dr^2 + r^2 d\theta^2 . $$&lt;/p>
&lt;p>But now consider the time-shifted spacetime hyperboloid with curvature radius $L$
$$ (\tau-t)^2 - r^2 = L^2 $$
This hypersurface is spacelike everywhere and extends to null infinity. Such surfaces are called hyperboloidal. Solving for $\tau$, we choose the negative square root, so the surfaces extend to future null infinity.
$$ \tau = t - \sqrt{L^2+r^2}.$$
In this hyperboloidal time, Minkowski metric has hyperbolic spatial slices
$$ ds^2_{\rm{Mink}} = -d\tau^2 - \frac{2 r}{\sqrt{L^2+r^2}} d\tau dr + \textcolor{RedViolet}{\frac{L^2}{L^2+r^2} dr^2 + r^2 d\theta^2}. $$
Just like AdS, flat spacetime takes the form of hyperbolic space with a time direction. You can draw the spacetime geometry as a cylinder, which is what Penrose did in his first publication on conformal infinity in 1963.&lt;/p>
&lt;p>
&lt;figure id="figure-the-cylinders-in-penroses-drawing-are-hyperbolic-spaces-with-a-time-direction-penrose-prl-1963httpsjournalsapsorgprlabstract101103physrevlett1066">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Penrose cylinder" srcset="
/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_5c28c60ce0a2024da2163000044e7e0a.webp 400w,
/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_dd8192eb1d14d6f9877e737a76389855.webp 760w,
/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/featured_hu7eb71e76e4f9e823419f039672d26a8d_30573_5c28c60ce0a2024da2163000044e7e0a.webp"
width="377"
height="365"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
The cylinders in Penrose&amp;rsquo;s drawing are hyperbolic spaces with a time direction. &lt;a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.10.66" target="_blank" rel="noopener">Penrose, PRL (1963)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The agreement of the geometry of hyperboloidal and AdS slices is suggestive. Calculations in the AdS/CFT literature performed on time slices may carry over to flat spacetime in hyperboloidal coordinates. One such example is the Ryu-Takayanagi conjecture.&lt;/p>
&lt;h3 id="rt-if-you-agree">RT if you agree&lt;/h3>
&lt;p>Ryu and Takayanagi (RT) have &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">proposed&lt;/a> a relation between geometry in AdS and entropy in CFT.&lt;/p>
&lt;p>One can interpret the Bekenstein-Hawking entropy as a measure of information lost to external observers. The black-hole horizon acts as a screen dividing space into two subsystems inside and outside the horizon. &lt;a href="https://en.wikipedia.org/wiki/Entropy_of_entanglement" target="_blank" rel="noopener">Entanglement entropy&lt;/a> is proportional to the area of this screen. The RT conjecture relates the entanglement entropy on CFT to the surface area of a screen in AdS that divides the AdS boundary into subdomains.&lt;/p>
&lt;p>Words don&amp;rsquo;t do it justice, so let&amp;rsquo;s look at a picture. Below is a diagram from the &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">RT paper&lt;/a> for AdS$_3$ /CFT$_2$.&lt;/p>
&lt;p>
&lt;figure id="figure-diagram-demonstrating-rt-proposal-on-ads_3cft_2-ryu-and-takayanagi-arxiv-2006httpsarxivorgabshep-th0603001">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="RT conjecture" srcset="
/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_3d67cdaa312f19f9d0c16a749eafda23.webp 400w,
/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_d2c76ec2eb6abc02b8c34de0f7348e36.webp 760w,
/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/RT_diagram_huc187e2868a67d5cdf87ff9d98ee019fe_90520_3d67cdaa312f19f9d0c16a749eafda23.webp"
width="760"
height="487"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Diagram demonstrating RT proposal on AdS$_3$/CFT$_2$. &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">Ryu and Takayanagi, arXiv (2006)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The two subsystems $A$ and $B$ of CFT$_2$ live on the boundary of AdS$_3$. They are separated by a screen $\gamma_A$ required to be an extremal surface in the bulk AdS$_3$. An extremal &amp;ldquo;surface&amp;rdquo; on a slice of AdS$_3$ is a geodesic line connecting the meeting points of the domains $A$ and $B$ on the bounding circle. The RT proposal states that the entanglement entropy of subsystem $A$ is proportional to the area of the screen $\gamma_A$:
$$ S_A \sim \rm{Area\ of\ }\gamma_A $$
In this case, we are talking about the length of a geodesic on a time slice. You can easily see that the calculation does not change whether you are in AdS or hyperboloidal Minkowski. In the &lt;a href="https://en.wikipedia.org/wiki/Ryu%E2%80%93Takayanagi_conjecture#Example" target="_blank" rel="noopener">example on Wikipedia&lt;/a>, the AdS$_3$ metric is written in the spatial coordinate $\rho$ defined through $r=\sinh \rho$, so
$$ ds^2_{\rm{AdS}} = -\cosh^2\rho\ d\tau^2 + \textcolor{Blue}{d\rho^2 + \sinh^2\rho\ d\theta^2}. $$
The same spatial geometry is obtained in hyperboloidal coordinates in Minkowski&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>
$$ ds^2_{\rm{Mink}} = -d\tau^2 - \sinh\rho\ d\tau d\rho + \textcolor{Blue}{d\rho^2 + \sinh^2\rho\ d\theta^2}. $$
This flat metric describes a stack of hyperbolic disks, just like the AdS metric. The length of the geodesic and its relation to entanglement entropy are the same in hyperboloidal coordinates. The original RT proposal applies directly to flat spacetime!&lt;/p>
&lt;p>The RT proposal reveals an interesting interpretation for the area dependence of entropy. It is easier to understand that entanglement entropy should be proportional to the area of a screen and not to the volume behind it. But maybe we should ask first: What is volume?&lt;/p>
&lt;h3 id="thinking-outside-the-ball">Thinking outside the ball&lt;/h3>
&lt;p>When we think of volume, we typically envision a box or a ball and consider the space inside. Relativity modifies our intuition about space and time, so it is reasonable that our notion of volume is modified as well. The volume of a region $B$ is the integral of the volume form
$$ V_B = \int_{B} \sqrt{\det h} \ d\Sigma, $$
and depends on the spatial metric $h$. In flat space we have $\sqrt{\det h} =$ $r^2 \sin\theta$. The volume of a ball of radius $R$ gives us the usual formula that we memorize at school
$$ V_{B(R)} = \int_{0}^{R} \int_0^{\pi} \int_0^{2 \pi} r^2 \sin\theta \ dr d\theta d\varphi= 4\pi \int_{0}^{R} r^2 dr = \frac{4\pi}{3}R^3. $$
For the hyperboloidal foliation with $L=1$ we have
$$ h = \frac{1}{1+r^2}dr^2 + r^2 d\theta^2 + r^2 \sin^2\theta d\varphi^2,\quad \sqrt{\det h} = \frac{r^2 \sin\theta}{\sqrt{1+r^2}}. $$
The volume expression changes to
$$ V_{B(R)} = 4\pi \int_{0}^{R} \frac{r^2}{\sqrt{1+r^2}} dr. $$
We can get a sense for it using Taylor expansions. For small spheres with $R \ll 1$, we get the usual result
$$ V_{B(R)}=\frac{4\pi}{3}R^3+O(R^5).$$
For large spheres, however, the series expansion for $R\gg 1$ gives
$$ V_{B(R)}=2\pi R^2 + O(\ln R).$$
Remarkably, volume and area scale the same way near the asymptotic boundary in hyperboloidal coordinates.&lt;/p>
&lt;p>These are suggestive observations about the potential usefulness of hyperboloidal surfaces in holography. And indeed, there is a current effort using hyperboloidal surfaces for holography.&lt;/p>
&lt;h3 id="celestial-holography">Celestial Holography&lt;/h3>
&lt;p>In the context of quantum gravity, the holographic principle suggests duality between a gravity theory in the bulk and a quantum theory on the boundary. &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">Celestial holography&lt;/a> suggests that asymptotically flat spacetimes are dual to a theory living on the &amp;ldquo;celestial sphere&amp;rdquo; at infinity.&lt;/p>
&lt;p>The theory uses Bondi coordinates along null infinity to discuss asymptotic symmetries, and switches to a hyperboloidal slicing to discuss massive particles. The slices they use have been first proposed as a cosmological model by &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne&lt;/a> in 1935. Milne slices are level sets of
$$ \tau^2 = t^2 - r^2.$$
Introducing the spatial coordinate
$$ \rho = \frac{r}{\sqrt{t^2-r^2}}, $$
the Minkowski metric takes the form
$$ ds^2_{\rm{Mink}} = -d\tau^2 + \tau^2 \left( \textcolor{RedViolet}{\frac{1}{1+\rho^2} d\rho^2 + \rho^2 d\sigma^2}\right). $$
Again, we recognize the representation of Minkowski spacetime as a stack of hyperbolic disks. But in this case, the metric coefficients depend on time $\tau$. And the mean extrinsic curvature of Milne slices vanishes asymptotically in time as $K=3/\tau$.&lt;/p>
&lt;p>I suspect that the hyperboloidal foliation we discussed &lt;a href="#hyperbolic-hyperboloidal">above&lt;/a> is better than Milne slicing for holography. Consider the Penrose diagram of Milne slices.
&lt;figure id="figure-penrose-diagram-of-milne-slicing-strominger-arxiv-2017httpsarxivorgabshep-th0603001">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperbolic slices" srcset="
/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_db8958ccf0594bed0168ca2fed9d8f85.webp 400w,
/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_67332c32c03e6a61abc8f80af55f0e95.webp 760w,
/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/hyperbolic_hu07438a4e6e3cb093e518d73c9c90a40d_74988_db8958ccf0594bed0168ca2fed9d8f85.webp"
width="444"
height="453"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Penrose diagram of Milne slicing. &lt;a href="https://arxiv.org/abs/hep-th/0603001" target="_blank" rel="noopener">Strominger, arXiv (2017)&lt;/a>
&lt;/figcaption>&lt;/figure>
The Penrose diagram and the metric reveal some undesirable features of Milne slices compared to the hyperboloidal foliation:&lt;/p>
&lt;ul>
&lt;li>The slices intersect at null infinity.&lt;/li>
&lt;li>The coordinates are time-dependent.&lt;/li>
&lt;li>There is no analog of the AdS curvature scale.&lt;/li>
&lt;/ul>
&lt;p>In contrast, the hyperboloidal foliation has constant mean curvature, $K=3/L$, playing the same role as the curvature scale in AdS geometry. The coordinates are independent of time and provide a smooth foliation of null infinity. Below are Penrose diagrams for such hyperboloidal, constant-mean-curvature foliations with different values of the mean extrinsic curvature, which acts as a dial between characteristic and Cauchy surfaces.&lt;/p>
&lt;p>
&lt;figure id="figure-penrose-diagrams-of-hyperboloidal-foliations-with-different-mean-extrinsic-curvatures-k632-zenginoğlu-arxiv-2008httpsarxivorgabs07124333">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperboloidal foliation" srcset="
/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_3b2e6e1d322650da3e63826c855b461a.webp 400w,
/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_f6ff2e6e1abd3e3ba1c347060783a487.webp 760w,
/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperboloidal-holography/hyperboloidal_huddf2caadb7366214d7f4d1665843ed97_102448_3b2e6e1d322650da3e63826c855b461a.webp"
width="760"
height="338"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Penrose diagrams of hyperboloidal foliations with different mean extrinsic curvatures $K={6,3,2}$. &lt;a href="https://arxiv.org/abs/0712.4333" target="_blank" rel="noopener">Zenginoğlu, arXiv (2008)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Milne coordinates have one advantage that a hyperboloidal foliation of null infinity cannot provide. A massive particle in constant motion will asymptote to a point on the Milne slices. Dirac used this property in his 1949 paper on &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">Forms of Relativistic Dynamics&lt;/a> and it appears repeatedly in celestial holography. I&amp;rsquo;m not sure how important this is. But Milne coordinates are also hyperboloidal. So in that sense, celestial holography is already using hyperboloidal holography.&lt;/p>
&lt;p>Celestial holography is relatively new and developing rapidly. We don&amp;rsquo;t know how the final version will look like. That&amp;rsquo;s the excitement of research.&lt;/p>
&lt;h3 id="wrap-up">Wrap up&lt;/h3>
&lt;p>A few curious observations suggest that hyperboloidal coordinates are interesting for holography.&lt;/p>
&lt;ul>
&lt;li>Flat spacetime becomes a stack of hyperbolic disks, with a &lt;a href="#hyperbolic-hyperboloidal">cylinder&lt;/a> representing the global picture.&lt;/li>
&lt;li>The original &lt;a href="#rt-if-you-agree">Ryu-Takayanagi calculations&lt;/a> apply to Minkowski spacetime.&lt;/li>
&lt;li>&lt;a href="#thinking-outside-the-ball">Volume scales as area&lt;/a> near the conformal boundary.&lt;/li>
&lt;li>Spacetime hyperboloids are essential in &lt;a href="#celestial-holography">celestial holography&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Are these observations indicative of a profound duality, an underlying realization of the holographic principle for isolated systems in hyperboloidal coordinates? I don&amp;rsquo;t know. The similarity between Minkowski in hyperboloidal coordinates and AdS is interesting, but it can also be misleading.&lt;/p>
&lt;p>The observations here are, by their very nature, coordinate-dependent. Suitable coordinates may ease calculations and suggest research directions, but the results should not depend on them. A significant, coordinate-independent difference to the AdS case is the null conformal boundary of asymptotically flat spacetimes. Any realization of the holographic principle will need to incorporate the null boundary and the associated &lt;a href="https://en.wikipedia.org/wiki/Bondi%E2%80%93Metzner%E2%80%93Sachs_group" target="_blank" rel="noopener">symmetries&lt;/a> as celestial holography is attempting to do.&lt;/p>
&lt;p>It can be misleading to take similarities in specific coordinates too far, but it can also be a missed opportunity to ignore them. I think hyperbolic geometry and hyperboloidal surfaces will keep playing an essential role in holography. &lt;a href="https://www.urbandictionary.com/define.php?term=Watch%20this%20Space" target="_blank" rel="noopener">Watch this space!&lt;/a>&lt;/p>
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>There are various subtleties here that I&amp;rsquo;m glossing over. You can go down the rabbit hole following the &lt;a href="http://www.scholarpedia.org/article/Bekenstein_bound" target="_blank" rel="noopener">Bekenstein bound&lt;/a>.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Setting the curvature radius to unity for simplicity, $L=1$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How far is infinity?</title><link>https://anilzen.github.io/post/empirical-infinity/</link><pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/empirical-infinity/</guid><description>&lt;p>Most of my research uses infinity in numerical computations. When presenting my work, I often encounter the objection that infinity is not physical. But not all infinities are created equal, and some are more physical than others. In fact, we can &lt;strong>verify by experiment&lt;/strong> that we are at infinity with respect to distant sources of radiation.&lt;/p>
&lt;p>Below I collect some thoughts on infinity and describe the experiment. Feel free to skip my ramblings and go directly to the &lt;a href="#a-thought-experiment">experiment&lt;/a>.&lt;/p>
&lt;h3 id="we-dont-need-no-infinity">We don&amp;rsquo;t need no infinity&lt;/h3>
&lt;p>Physicists are generally uncomfortable with infinity. First, it is argued, infinity cannot be the outcome of a physical experiment. Second, infinity as the outcome of a calculation indicates problems with the theory. Resolving such problems is important because they may reveal new research directions such as &lt;a href="https://en.wikipedia.org/wiki/Renormalization" target="_blank" rel="noopener">renormalization techniques&lt;/a>. Since infinity doesn&amp;rsquo;t make sense in experiment and should be avoided in theory, physicists are skeptical when it appears in mathematical descriptions of physical phenomena. A few examples: Max Tegmark argues that infinity is an idea that should be &lt;a href="https://www.edge.org/response-detail/25344" target="_blank" rel="noopener">retired from physics&lt;/a>; Nicolas Gisin aims to &lt;a href="https://www.quantamagazine.org/does-time-really-flow-new-clues-come-from-a-century-old-approach-to-math-20200407/" target="_blank" rel="noopener">understand time&lt;/a> without relying on infinitely precise numbers; and here is a quote from &lt;a href="https://www.nature.com/articles/s41567-018-0238-1" target="_blank" rel="noopener">The physics of infinity&lt;/a> by Ellis, Meissner, and Nicolai:&lt;/p>
&lt;blockquote>
&lt;p>We maintain that in each physical case where $\infty$ is used in a discussion, greater insight is attained by considering what large number $N$ will suffice instead because real physics is embodied in that number.&lt;/p>
&lt;/blockquote>
&lt;p>This uneasiness of physicists with infinity is also present in mathematics. There are mathematical philosophies and formalisms without infinity, such as &lt;a href="https://en.wikipedia.org/wiki/Finitism" target="_blank" rel="noopener">finitism&lt;/a> or &lt;a href="https://en.wikipedia.org/wiki/Intuitionism" target="_blank" rel="noopener">intuitionism&lt;/a>. I was attracted to similar ideas when I was in college, so I went down the rabbit hole of &lt;a href="https://en.wikipedia.org/wiki/Fuzzy_number" target="_blank" rel="noopener">fuzzy numbers&lt;/a>. They seemed to &amp;ldquo;naturally&amp;rdquo; represent physical measurements, which are not real numbers with infinite precision but fuzzy numbers with uncertainties. The more I learned about fuzzy numbers, however, the less I was convinced of their general utility. They were cumbersome and did not improve calculations in most cases. I learned, by way of fuzzy numbers, that successful mathematical representations do not necessarily match our physical intuitions. The map is not the territory.&lt;/p>
&lt;h3 id="empirical-infinity">Empirical infinity&lt;/h3>
&lt;p>Aristotle distinguishes &lt;a href="https://en.wikipedia.org/wiki/Actual_infinity" target="_blank" rel="noopener">two notions of infinity&lt;/a>: actual and potential. Potential infinity is what children learn first: numbers just keep going &amp;ldquo;forever&amp;rdquo;. Actual infinity is the completion of this process: the set of all numbers as a mathematical object.&lt;/p>
&lt;p>We do not observe infinity directly but we use it to represent our impressions of the natural world. In a painting, both sides of a road may meet at a &lt;a href="https://en.wikipedia.org/wiki/Vanishing_point" target="_blank" rel="noopener">vanishing point&lt;/a>. The road ends somewhere but the painting suggests that the meeting point is beyond the horizon of perception. The representation of &amp;ldquo;far&amp;rdquo; on the canvas is infinity.&lt;/p>
&lt;p>The infinity on the canvas is neither potential (it&amp;rsquo;s a finite, completed representation of infinity) nor actual (the horizon is actually at a finite distance). Call it empirical infinity. Empirical concepts are abstracted from individual perceptions. Empirical infinity is the best canvas representation of your perception looking down a long, seemingly unending road. You don&amp;rsquo;t know how far you can see, and you don&amp;rsquo;t care about representing that particular detail in your painting. You just want to express that it&amp;rsquo;s very far.&lt;/p>
&lt;h3 id="a-thought-experiment">A thought experiment&lt;/h3>
&lt;p>Black hole perturbations have a property that is different at infinity and at a finite distance. We can use this property to determine where we are with respect to the black hole. We would need very accurate detectors to measure it, which we don&amp;rsquo;t have yet, so consider this a reasonable thought experiment.&lt;/p>
&lt;p>Perturbations of a black hole go through an oscillatory period, and subsequently decay with a power-law $ \sim t^p$, where $t$ is time as measured by the observer and $p$ is the negative power&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Surprisingly, the power $p$ is different depending on where you observe the perturbations: -3 at finite distances, -2 at infinity&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. The slower decay at infinity is somewhat unexpected. One could argue that the power-law decay, which was first discovered for finite distances, should be non-radiative; it shouldn&amp;rsquo;t even be detected at infinity. As demonstrated by Gundlach, Price, and Pullin in 1994, it is not only radiative but decays slower at infinity than at finite distances&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. There are explanations of this slower decay in terms of accumulation of backscattering off of curvature towards infinity, but this is not our main concern&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. Given that the rates at finite distances and infinity are different, we ask the following question:&lt;/p>
&lt;p>&lt;em>Which decay rate would we observe?&lt;/em>&lt;/p>
&lt;p>We are clearly a finite distance away from the black hole, so the intuitive answer is that we would measure the finite distance rate. However, the correct answer is the infinity rate.&lt;/p>
&lt;p>Below is a figure from a &lt;a href="https://anilzen.github.io/publication/zenginoglu-2008-tail/">paper of mine&lt;/a> where I studied various decay rates. The figure shows the evolution of the decay rate in time for various distances to the black hole.&lt;/p>
&lt;p>
&lt;figure id="figure-decay-rates-in-time-at-various-distances-from-the-source">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Decay rates by distance" srcset="
/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_bf581f31ae27b9bf532bd87e03782aca.webp 400w,
/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_12160c7bebd609602e4ccf6e0e0f676c.webp 760w,
/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/empirical-infinity/featured_hucb430a8324f06a774b153a0b9634005a_69007_bf581f31ae27b9bf532bd87e03782aca.webp"
width="711"
height="440"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Decay rates in time at various distances from the source.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>The long-time decay rate is -3 for any finite distance as the theory demonstrates. But the theory is valid for the asymptotic limit. Initially, the decay rate for distant observers is much closer to -2 than to -3. The top curve in the figure is the measurement at infinity, and the next closest curve is at 40,000 units of black hole mass (in these units, a solar mass corresponds to a distance of 1.5 km). For a small black hole of 5 solar masses, this observer is about 300,000 km away. This may sound far, but it&amp;rsquo;s just one light second, which is about the distance between the Earth and the Moon. To compare, typical sources of gravitational waves are thousands and millions of light years away. The decay rate of perturbations from such a distant source would be indistinguishable from the infinity value.&lt;/p>
&lt;p>Infinity in astrophysics may be closer than the Moon.&lt;/p>
&lt;h3 id="wrap-up">Wrap-up&lt;/h3>
&lt;p>If we ever get to measure the power-law decay from black-hole perturbations, we will measure the infinity value and not the finite distance value, even though we are a finite distance away from the sources. Given that the distances are literally astronomical, it&amp;rsquo;s more practical to use infinity both in our calculations and in our discussions where it&amp;rsquo;s needed, instead of artificially avoiding it.&lt;/p>
&lt;p>I think most people agree that actual infinity doesn&amp;rsquo;t exist in the real world. But the number 3 also doesn&amp;rsquo;t exist in the real world. Numbers are useful concepts abstracted from observations and represented in mathematical models. The decay rate experiment demonstrates that infinity is a useful concept in describing astrophysical observations. We should embrace empirical infinity and its appearance in our formalisms as long as it improves our calculations.&lt;/p>
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>This was first discussed in detail by Richard Price in his 1972 paper &lt;a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.5.2419" target="_blank" rel="noopener">Nonspherical Perturbations of Relativistic Gravitational Collapse. I. Scalar and Gravitational Perturbations&lt;/a>. The decay rate depends on initial data and mode of perturbation.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The observation of a radiative field at infinity gives the part of the field that falls off as $1/r$. In other words, for a radiative field $\phi$, its observation at infinity is $\lim_{r\to\infty} r \phi$.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>C. Gundlach, R. H. Price, and J. Pullin. &lt;a href="https://journals.aps.org/prd/abstract/10.1103/PhysRevD.49.883" target="_blank" rel="noopener">Late-time behavior of stellar collapse and explosions. I. Linearized perturbations.&lt;/a> Physical Review D 49.2, 883 (1994).&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>For those of you who are wondering about the mathematical form of such a function, here is an example:
$$ \phi(t,r) \sim \frac{C_1}{(C_2+t+r)(C_3+t-r)^2} $$
The constants $C_i$ are arbitrary values determined by the initial perturbation. For fixed $r$ we have $\phi\sim 1/t^3$. Along null directions with constant $t+r$, the function decays as $\phi\sim 1/(t-r)^2$. Therefore, an observer measures the decay rate $3$ or $2$ depending on location.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>