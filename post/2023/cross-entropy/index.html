<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=stylesheet href=/css/wowchemy.b9992bc39f58597ce0fecaf651999475.css><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=google-site-verification content="CVDtdnmvDztdvP8XA_Fh0StEBOfqy3DM5L6GyojfVgk"><script async src="https://www.googletagmanager.com/gtag/js?id=G-1Q5LP68RP1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-1Q5LP68RP1",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Anıl Zenginoğlu"><meta name=description content="Cross-entropy is a measure of the difference between two probability distributions. It also plays a role in the information-theoretic formulation of the Second Law of Thermodynamics, thereby providing a bridge between physics and machine learning."><link rel=alternate hreflang=en-us href=https://anilzen.github.io/post/2023/cross-entropy/><link rel=canonical href=https://anilzen.github.io/post/2023/cross-entropy/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu6b147281c14d31ddea9d4b1ec87dcb1f_1135_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu6b147281c14d31ddea9d4b1ec87dcb1f_1135_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@AnilZenginoglu"><meta property="twitter:creator" content="@AnilZenginoglu"><meta property="twitter:image" content="https://anilzen.github.io/post/2023/cross-entropy/featured.webp"><meta property="og:type" content="article"><meta property="og:site_name" content="Anıl Zenginoğlu"><meta property="og:url" content="https://anilzen.github.io/post/2023/cross-entropy/"><meta property="og:title" content="Cross-entropy | Anıl Zenginoğlu"><meta property="og:description" content="Cross-entropy is a measure of the difference between two probability distributions. It also plays a role in the information-theoretic formulation of the Second Law of Thermodynamics, thereby providing a bridge between physics and machine learning."><meta property="og:image" content="https://anilzen.github.io/post/2023/cross-entropy/featured.webp"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-04-02T00:01:00+00:00"><meta property="article:modified_time" content="2023-04-02T00:01:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://anilzen.github.io/post/2023/cross-entropy/"},"headline":"Cross-entropy","image":["https://anilzen.github.io/post/2023/cross-entropy/featured.webp"],"datePublished":"2023-04-02T00:01:00Z","dateModified":"2023-04-02T00:01:00Z","author":{"@type":"Person","name":"Anıl Zenginoğlu"},"publisher":{"@type":"Organization","name":"Anıl Zenginoğlu","logo":{"@type":"ImageObject","url":"https://anilzen.github.io/media/icon_hu6b147281c14d31ddea9d4b1ec87dcb1f_1135_192x192_fill_lanczos_center_3.png"}},"description":"Cross-entropy is a measure of the difference between two probability distributions. It also plays a role in the information-theoretic formulation of the Second Law of Thermodynamics, thereby providing a bridge between physics and machine learning."}</script><title>Cross-entropy | Anıl Zenginoğlu</title>
<link rel=me href=https://mathstodon.xyz/@anilzen></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=687e8c76166c990ef7a943163e2b27e6><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Anıl Zenginoğlu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Anıl Zenginoğlu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/nsf-award-2309084><span>NSF Award</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Cross-entropy</h1><p class=page-subtitle>Crossing the bridge between physics and machine learning.</p><div class=article-metadata><div><span class=author-highlighted>Anıl Zenginoğlu</span></div><span class=article-date>Apr 2, 2023
</span><span class=middot-divider></span>
<span class=article-reading-time>13 min read</span></div></div><div class=article-container><div class=article-style><p>We&rsquo;re all gonna die.</p><p>Blame the Second Law of Thermodynamics. Entropy increases, we get older, and we die.<figure id=figure-mark-thompson-new-yorker-magazine><div class="d-flex justify-content-center"><div class=w-100><img src=./entropy_new_yorker_cartoon.jpg alt="I blame entropy - New Yorker Cartoon" loading=lazy data-zoomable></div></div><figcaption>Mark Thompson, New Yorker Magazine</figcaption></figure>This perception of entropy, prevalent in popular culture, presents it as the driving force for decay and disorder. But this cannot be the whole story. The incredible abundance of life on our planet would not have been possible without the self-organization of complex systems. The story of <a href=https://en.wikipedia.org/wiki/Entropy_and_life target=_blank rel=noopener>entropy and life</a> is more complicated than the boy in the above cartoon implies. To understand these larger questions about Life, the Universe, and Everything, we need to first clarify what entropy is.</p><p>This post is about a variant of entropy&mdash;called cross-entropy&mdash;that I wrote about in a post on <a href=../../2022/learning-machine-learning/>machine learning</a>. There, I presented cross-entropy as a measure of <a href=http://localhost:1313/post/2022/learning-machine-learning/#fn:3 target=_blank rel=noopener>the difference between two probability distributions</a>. Most explanations of the concept, including its <a href=https://en.wikipedia.org/wiki/Cross_entropy target=_blank rel=noopener>Wikipedia entry</a>, mainly focus on its relevance in information theory, not physics.</p><p>I learned in a <a href="https://www.youtube.com/watch?v=x9COqqqsFtc" target=_blank rel=noopener>talk</a> by <a href=https://www.preposterousuniverse.com/ target=_blank rel=noopener>Sean Carroll</a> during the <a href=https://qtd-hub.umd.edu/event/symposium-2023/ target=_blank rel=noopener>Maryland Quantum-Thermodynamics Symposium</a> that cross-entropy plays a central role in the informational reformulation of the Second Law. This way of thinking about entropy and the Second Law builds a fascinating bridge between machine learning and physics. Before we cross that bridge, let&rsquo;s talk about plain old entropy.</p><h2 id=entropy-without-a-cross>Entropy Without a Cross</h2><p>Entropy is one of the most important concepts in physics. It&rsquo;s the main character of the Second Law of Thermodynamics, which states that the entropy of an isolated system increases over time.</p><p>Despite its importance, entropy is not as widely known or used as energy. Whether you&rsquo;re trying to count your calories, arguing about the geopolitics of natural gas, or worrying about climate change, energy seems to be the main character. But it doesn&rsquo;t quite make sense. We know that energy is conserved; all we do is transform energy from one form to another. Yet we sense that something is irreversibly lost when we &ldquo;spend energy.&rdquo; What exactly are we spending when we burn food or natural gas? Check out the next paragraph. The answer will <a href=#surpriiise>surprise</a> you!</p><p>Well, it&rsquo;s entropy. And its story starts with heat.</p><h3 id=the-birth-of-heat>The Birth of Heat</h3><p>Thermodynamics is the study of heat, energy, and work. It was born in the 19th century during the Industrial Revolution from the desire to understand how to efficiently convert heat energy into mechanical work.</p><p><a href=https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot target=_blank rel=noopener>Sadi Carnot</a> showed that the efficiency of a heat engine depends only on the temperature difference between the hot and cold reservoirs and not on the specific working substance or the details of the engine design.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> While this observation had huge practical implications, his main contribution for our purposes is the distinction between reversible and irreversible processes, which led to the notion of entropy.</p><p>The <a href=https://en.wikipedia.org/wiki/Entropy#Etymology target=_blank rel=noopener>term entropy</a> was coined by the German physicist <a href=https://en.wikipedia.org/wiki/Rudolf_Clausius target=_blank rel=noopener>Rudolf Clausius</a> in 1865 as a counterpart to the term energy. The 19th-century German intellectuals were enamored with <a href=https://en.wikipedia.org/wiki/Hellenism_%28neoclassicism%29 target=_blank rel=noopener>neoclassical hellenism</a>, which resulted in lots of Greek words in scientific literature. <em>Entropia</em> means &ldquo;transformation to&rdquo; in Greek. So the German word &ldquo;Entropie&rdquo; is the germanized Greek translation of the German word &ldquo;Verwandlungsinhalt,&rdquo; which Clausius used to describe the transformational content of energy.</p><p>When you burn natural gas to generate heat, you spend the transformational content of the natural gas. The heat that results in this process cannot be transformed back; entropy increases. Clausius formulated the observation that heat flows naturally from a hot body to a cooler one through the inequality
$$ dS ≥ \frac{\delta Q}{T}. $$
Clausius used $S$ for entropy in honor of Sadi Carnot, so $dS$ denotes a small change in entropy, $\delta Q$ is the heat the system absorbs from its surroundings, and $T$ is the temperature at which the heat is absorbed. In an adiabatic process without heat exchange, we have $\delta Q=0$, and entropy can never decrease in accordance with the Second Law.</p><p>Entropy encapsulates the irreversible processes that we typically associate with energy usage. Concepts like <a href=https://en.wikipedia.org/wiki/Energy_crisis target=_blank rel=noopener>energy crisis</a> actually refer to entropy crisis: we need a continuous supply of low entropy to keep the world running.</p><p>Clausius&rsquo; inequality doesn&rsquo;t give an origin story or an explanation for entropy. For that, we need statistical physics.</p><h3 id=atoms>Atoms</h3><p>The famous equality that describes entropy is engraved in Boltzmann&rsquo;s <a href=https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#/media/File:Boltzmann_equation.JPG target=_blank rel=noopener>tombstone</a><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>
$$ S = k \log W. \tag{1} \label{1} $$
In this expression, $k$ is the Boltzmann constant, $W$ is the number of microstates corresponding to a particular macrostate of the system. To understand what $W$ represents, think of a system composed of many parts, say, tiny atoms. Our description of the system uses a few variables, such as heat and pressure. This macroscopic description is clearly underdetermined: there are gazillions of atomic configurations that result in a given value for heat and pressure. A macrostate is a collection of $W$ individual microstates that are macroscopically indistinguishable. It&rsquo;s the number of equivalent ways the subsystems (atoms) can be arranged without changing the macroscopic state.</p><p>The logarithm in the formula arises from known observations about entropy and simple combinatorics. Consider two systems. It was known that their total entropy is the <em>sum</em> of their entropies, $S=S_1+S_2$, but the total number of microstates for the full system is the <em>product</em> of its parts, $W=W_1*W_2$. The only function that converts a product into a sum is the $\log$ which tells us that $S\sim \log W$. The Boltzmann constant in \eqref{1} makes the units work.</p><p>The Second Law is then a probabilistic statement: among different macrostates, the system evolves towards a more probable configuration, one with a larger number of microstates. In this picture, we don&rsquo;t expect entropy to <em>always</em> increase. It just happens to be more probable. You will run into fluctuations where entropy goes down if you wait enough.<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>
A rather outrageous extrapolation of this idea is the <a href=https://en.wikipedia.org/wiki/Boltzmann_brain target=_blank rel=noopener>Boltzmann brain</a>: a self-aware brain that spontaneously appears in a universe through random fluctuations rather than through biological evolution.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><p><figure id=figure-boltzmann-brainhttpsenwikipediaorgwikiboltzmann_brain-generated-with-midjourneyhttpswwwmidjourneycom><div class="d-flex justify-content-center"><div class=w-100><img src=./featured.webp alt="Boltzmann Brain generated with Midjourney" loading=lazy data-zoomable></div></div><figcaption><a href=https://en.wikipedia.org/wiki/Boltzmann_brain target=_blank rel=noopener>Boltzmann brain</a> generated with <a href=https://www.midjourney.com target=_blank rel=noopener>Midjourney</a>.</figcaption></figure></p><h3 id=surpriiise>Surpriiise!</h3><p>With the rise of calculators, computers, and communication devices in the 20th century, information started to play a fundamental role in our description of physical phenomena.<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>
<a href=https://en.wikipedia.org/wiki/Claude_Shannon target=_blank rel=noopener>Shannon&rsquo;s</a> reformulation of entropy in <a href=https://ieeexplore.ieee.org/abstract/document/6773024 target=_blank rel=noopener>The Mathematical Theory of Communication</a> relates information to surprise.</p><p>What is surprise? To be surprised, you must have a prior expectation, some sense that things happen in a certain way. The more you expect something, the less surprised you are to see it, and vice versa. Therefore, surprise $s$ should be a decreasing function of probability $p\in [0,1]$. Specifically, we&rsquo;re looking for an expression $s(p)$ that satisfies the following reasonable conditions:</p><ul><li>If you&rsquo;re absolutely certain of $x$, then $p(x)=1$ and you&rsquo;re not surprised: $s(1)=0$.</li><li>If you&rsquo;re absolutely certain that $x$ can never happen, then $p(x)=0$ and its occurence surprises you infinitely: $s(0) \to \infty$.<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup></li><li>Surprise should be additive: the total surprise for multiple events should be the addition of the surprise associated with each event. For two events $x_1$ and $x_2$, the combined probability is $p=p_1 * p_2$ and the total surprise should be $s(p) = s(p_1*p_2) = s(p_1) + s(p_2)$</li></ul><p>These conditions are satisfied by a formula that depends logarithmically on the inverse of $p$:
$$ s(p) = \log \frac{1}{p} = - \log p. $$</p><p>Entropy is then the probability-weighted sum of surprise. In other words, entropy is expected surprise:</p><p>$$ S = \sum p_i\ s(p_i) = - \sum p_i \log p_i. \label{2} \tag{2}$$</p><p>Boltzmann used a similar formula <a href=https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#Generalization target=_blank rel=noopener>already in 1866</a>, yet the expression is named after Gibbs and Shannon. It reduces to Boltzmann&rsquo;s first formula \eqref{1} when the probabilities of all microstates are equal, which can then be used to derive Clausius&rsquo; inequality.<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup></p><p>An increase in entropy means that the expected surprise increases. This might sound a bit counterintuitive. We learned that entropy is a measure of disorder. How are disorder and surprise related?</p><p>It may be simpler to understand that patterns reduce <em>total</em> expected surprise. Let&rsquo;s say every time I order a taxi, I get a yellow cab. Over time, the total expected surprise about the color of the taxi cab will be low even though I might get a blue cab once in a blue moon. If, however, the color of the taxi cab is different every single time, those little surprises add up and maximize the total expected surprise. Disorder increases total expected surprise over a collection of events. It&rsquo;s highest when the events are random.</p><h2 id=the-cross-of-entropy>The Cross of Entropy</h2><p>Boltzmann&rsquo;s entropy \eqref{1} generalizes to Gibbs-Shannon entropy \eqref{2}, allowing different probabilities for the microstates. The next generalization<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> includes a distinction between the expected and observed probabilities and is commonly used to define loss functions in supervised machine learning.</p><h3 id=learning-to-expect-the-unexpected>Learning to Expect the Unexpected</h3><p>In Gibbs-Shannon entropy \eqref{2}, the weights of the sum, $p_i$, are from the same probability distribution that quantifies surprise, $s(p_i)$. But those two distributions are not necessarily the same. Our surprise arises from our assumed expectations, let&rsquo;s call it $q_i$, which may need to be corrected or updated. A good example is climate change, when 100-year storms start happening every decade. The probability distribution for heavy storms has shifted, so we need to adjust our expectations.</p><p>We may formally use the true distribution, $p_i$, for the weights, which are unknown a priori and must be learned from observations. The cross-entropy, $H$, accounts for the difference between true and assumed expectations.
$$ H(p,q) = - \sum p_i \log q_i. $$
The cross-entropy $H$ is the true expected value of our assumed surprise. In other words, it&rsquo;s the expected value, with respect to the true distribution $p$, of our surprise, with respect to the assumed distribution $q$. It measures how likely we are to be surprised (and therefore learn something) if we were told the actual probability distribution of the system.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup> It obtains a minimum when the two distributions are equal.</p><p>This property is why it&rsquo;s so useful in machine learning where cross-entropy is used to construct the <a href=../../2022/learning-machine-learning/#layer-and-loss-build-the-model>loss function</a> in multiclass classification tasks. The true labels of the training samples serve as the true distribution; the output labels of the neural network serve as the assumed distribution. The cross-entropy loss function is iteratively reduced by numerical optimization. Eventually, the true distribution of the labels matches the predicted distribution from the neural network sufficiently well. At that point, we say the machine learned the training set.</p><h3 id=the-second-coming-of-the-second-law>The Second Coming of the Second Law</h3><p>The Second Law of Thermodynamics has been reformulated using cross-entropy by Bartolotta, Carroll, Leichenauer, and Pollack to &ldquo;incorporate the effects of a measurement of a system at some point in its evolution.&rdquo; <a href=https://arxiv.org/abs/1508.02421 target=_blank rel=noopener>The Bayesian Second Law of Thermodynamics</a> uses an information-theoretic approach. Sean Carroll has a great <a href=https://www.preposterousuniverse.com/blog/2015/08/11/the-bayesian-second-law-of-thermodynamics/ target=_blank rel=noopener>blog post</a> about this paper; you should read it. Here&rsquo;s a short description in our context.</p><p>According to the Bayesian Second Law, the cross-entropy of the updated (&ldquo;true&rdquo;) distribution with respect to the original (&ldquo;assumed&rdquo;) distribution, plus the generalized heat flow, is larger when evaluated at the end of the experiment than at the beginning. For zero heat transfer, the expected amount of information an observer would learn by being told the true microstate of the system is larger at the final time than at the initial one. Therefore, cross-entropy can change over time according to how well our initial assumptions about a system match its true underlying distribution and how much new information we gain through measurements and updates to our assumptions.</p><p>This updated Second Law describes the increase in cross-entropy as
$$ \Delta H(p, q) + \langle Q \rangle \geq 0, $$
where $\langle Q \rangle$ is the expectation value of a generalized heat flow out of the system, similar to the term $\delta Q$ in Clausius&rsquo; inequality (with a different sign).</p><p>When the assumed distribution differs significantly from the correct distribution during time evolution, it can lead to information loss and, therefore, a large increase in cross-entropy. Cross-entropy increases with time even with zero heat transfer. In this interpretation, what happens during optimization in a machine learning model (decreasing cross-entropy) is the opposite of what happens in stochastic evolution (increasing cross-entropy): The act of learning is a revolt against disorder and decay!</p><h2 id=the-death-of-heat>The Death of Heat</h2><p>At the beginning of the post, I mentioned that the relationship between life and entropy is complicated. When it comes to the Universe, however, things are much simpler. The Universe is evolving towards <a href=https://en.wikipedia.org/wiki/Heat_death_of_the_universe target=_blank rel=noopener>heat death</a>.</p><p>As the Universe continues to expand and matter becomes more dispersed, it will become increasingly difficult for matter to interact with other matter, and energy will become more evenly distributed. Eventually, all stars will have exhausted their fuel, and the Universe will be a cold, dark, lifeless place where nothing happens.</p><p>One of my favorite science-fiction short stories is Asimov&rsquo;s <a href=http://users.ece.cmu.edu/~gamvrosi/thelastq.html target=_blank rel=noopener>The Last Question</a> from 1956. It&rsquo;s a story about the heat death of the Universe with the perfect punch line. The story begins with two technicians attending to a giant, self-adjusting, and self-correcting computer, called Multivac that found a way to fulfill the Earth&rsquo;s energy needs by drawing energy from the Sun. The technicians argue that the Sun and all the stars in the Universe will eventually run out. They ask Multivac whether entropy can be reversed, to which Multivac replies, &ldquo;INSUFFICIENT DATA FOR MEANINGFUL ANSWER.&rdquo; The story follows the history of humanity across many eons, through interstellar travel and immortality. The last question remains and is asked repeatedly.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=./multivac.png alt="Asking Multivac" loading=lazy data-zoomable></div></div></figure></p><p>I won&rsquo;t give away the punchline but it does fit into our observation that <em>learning acts against entropy</em>. I posed the last question to ChatGPT, our version of the Multivac. Maybe somewhere among the weights and biases in the billions of its connections, ChatGPT is still thinking about it.</p><hr><h4 id=footnotes>Footnotes</h4><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Unfortunately, Carnot died from cholera at a relatively young age of 36. His book, <a href=https://en.wikipedia.org/wiki/Reflections_on_the_Motive_Power_of_Fire target=_blank rel=noopener>Reflections on the Motive Power of Fire</a>, self-published in 1824, was largely ignored by the scientific community at the time.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Unfortunately, Boltzmann committed suicide while on a beach vacation with his wife and daughter near Trieste, shortly before the experimental verification of his ideas.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>In small systems with a few parts, such fluctuations happen frequently. Their study is a relatively new topic of research that falls under <a href=https://en.wikipedia.org/wiki/Stochastic_thermodynamics target=_blank rel=noopener>stochastic thermodynamics</a>. One of the main results in that area is the <a href=https://en.wikipedia.org/wiki/Jarzynski_equality target=_blank rel=noopener>Jarzynski equality</a> that relates the free energy difference between two equilibrium states to the average work performed on the system during a non-equilibrium process. As the system size increases, however, it becomes increasingly unlikely that such fluctuations reduce entropy and we recover classical thermodynamics.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>It took me about 5 minutes to generate, modify, and upscale this image using <a href=https://www.midjourney.com/ target=_blank rel=noopener>Midjourney</a>. An actual Boltzmann brain would presumably take much longer to form but some people argue that it&rsquo;s more likely than the formation of our entire Universe. Personally, I don&rsquo;t like talking about likelihood in the context of the entire Universe. I rather think <a href=https://en.wikipedia.org/wiki/Tractatus_Logico-Philosophicus target=_blank rel=noopener>darüber muss man schweigen</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>As an example on how fundamental information became in physics, consider that one of the most influential physicists of the 20th century, <a href=https://en.wikipedia.org/wiki/John_Archibald_Wheeler target=_blank rel=noopener>John Wheeler</a>, divided his physics career into <a href=https://plus.maths.org/content/it-bit target=_blank rel=noopener>three phases</a>: &ldquo;Everything is Particles,&rdquo; &ldquo;Everything is Fields,&rdquo; and &ldquo;Everything is Information.&rdquo; These stages may sum up the development of physics in the last four centuries. As we are now fully in the informational stage, it will be fascinating to see how machine learning will impact fundamental developments in physics, not only as a tool, but as a conceptual framework for our quest to understand Nature.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>We could consider setting a maximum here. We now know that, indeed, there is a maximum amount of entropy for a given volume of space. This <a href=https://en.wikipedia.org/wiki/Bekenstein_bound target=_blank rel=noopener>upper bound</a> for entropy is named after John Wheeler&rsquo;s student <a href=https://en.wikipedia.org/wiki/Jacob_Bekenstein target=_blank rel=noopener>Jacob Bekenstein</a> and has to do with black holes. But let&rsquo;s leave the quantization of gravity to a later time.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>There are some subtleties here related to the dimensions and underlying probability distributions. The equivalence of the various formulations of entropy must be demonstrated using certain assumptions. If you notice such subtleties, you probably didn&rsquo;t need to read this post, but I hope you enjoyed it.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>There are other generalizations, such as <a href=https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy target=_blank rel=noopener>Rényi entropy</a>, that are interesting but today&rsquo;s focus is on cross-entropy.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>This interpretation is better understood with the <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence target=_blank rel=noopener>Kullback–Leibler divergence</a> defined by
$$ D(p||q) = \sum p_i \log \frac{p_i}{q_i} = H(p,q) - S(p). $$
This expression vanishes when $p=q$ in accordance with the interpretation that there is nothing left to learn when the true distribution equals our assumed expectation.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=article-tags><a class="badge badge-light" href=/tag/entropy/>Entropy</a>
<a class="badge badge-light" href=/tag/physics/>Physics</a>
<a class="badge badge-light" href=/tag/machine-learning/>Machine Learning</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fanilzen.github.io%2Fpost%2F2023%2Fcross-entropy%2F&amp;text=Cross-entropy" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Fanilzen.github.io%2Fpost%2F2023%2Fcross-entropy%2F&amp;t=Cross-entropy" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Cross-entropy&amp;body=https%3A%2F%2Fanilzen.github.io%2Fpost%2F2023%2Fcross-entropy%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fanilzen.github.io%2Fpost%2F2023%2Fcross-entropy%2F&amp;title=Cross-entropy" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Cross-entropy%20https%3A%2F%2Fanilzen.github.io%2Fpost%2F2023%2Fcross-entropy%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Fanilzen.github.io%2Fpost%2F2023%2Fcross-entropy%2F&amp;title=Cross-entropy" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://anilzen.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/anil/avatar_hubdbe8f760004904350d82355aac46df7_167987_270x270_fill_q75_lanczos_center.jpg alt="Anıl Zenginoğlu"></a><div class=media-body><h5 class=card-title><a href=https://anilzen.github.io/>Anıl Zenginoğlu</a></h5><h6 class=card-subtitle>Assistant Research Scientist</h6><p class=card-text>Research scientist and administrator exploring spacetime infinity.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:anil@umd.edu><i class="fas fa-envelope"></i></a></li><li><a href=https://www.linkedin.com/in/anilzen/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://scholar.google.com/citations?user=M8NnUIQAAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/anilzen target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://orcid.org/0000-0001-7896-6268 target=_blank rel=noopener><i class="fab fa-orcid"></i></a></li><li><a href=https://twitter.com/AnilZenginoglu target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://mathstodon.xyz/@anilzen target=_blank rel=noopener><i class="fab fa-mastodon"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Anıl Zenginoğlu. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.391d344a129df56f7ad674c2c2ed04e8.js></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.7f5ebaff62ae468cff8bb3dd1337bb9b.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>