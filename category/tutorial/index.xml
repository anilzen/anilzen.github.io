<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tutorial | Anıl Zenginoğlu</title><link>https://anilzen.github.io/category/tutorial/</link><atom:link href="https://anilzen.github.io/category/tutorial/index.xml" rel="self" type="application/rss+xml"/><description>Tutorial</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Anıl Zenginoğlu</copyright><lastBuildDate>Thu, 08 Dec 2022 00:01:00 +0000</lastBuildDate><image><url>https://anilzen.github.io/media/sharing.jpg</url><title>Tutorial</title><link>https://anilzen.github.io/category/tutorial/</link></image><item><title>Learning Machine Learning</title><link>https://anilzen.github.io/post/2022/learning-machine-learning/</link><pubDate>Thu, 08 Dec 2022 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/learning-machine-learning/</guid><description>&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>
&lt;h2 id="neural-network-what-is-machine-learning">Neural Network: What is machine learning?&lt;/h2>
&lt;p>The core of a machine learning algorithm is the neural network that maps inputs to desired outputs. When you read &amp;ldquo;neural network,&amp;rdquo; you might think of the human brain with layers of interconnected neurons working together to solve problems. But in machine learning, we are simply talking about a function with parameters. Lots and lots of parameters. &amp;ldquo;Learning&amp;rdquo; is adjusting these parameters until the difference between the desired output and the actual output of the function is sufficiently small. That&amp;rsquo;s it. In a way, machine learning is the rediscovery of the old adage that you can fit any &lt;a href="https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant" target="_blank" rel="noopener">elephant&lt;/a> with sufficient parameters.&lt;/p>
&lt;p>This was a very short summary of what people mean by machine learning. You can imagine that the function, the parameters, and the adjustment process are all rather sophisticated and can get very complicated. I&amp;rsquo;ll expand on this basic idea below with an example of a machine-learning algorithm.&lt;/p>
&lt;p>First, some terminology. Think of each input dimension as a neuron in a neural network. The parameters of the neural network, or the function, are called weights and biases. Weights represent the strength of the connection between neurons; biases shift the activation threshold of a neuron. By adjusting the weights and biases based on the output, the network learns the patterns in the data and makes predictions on new data.&lt;/p>
&lt;p>Let&amp;rsquo;s write this down. I mentioned that the neural network is just a function with parameters. Its output is usually a probability, so we&amp;rsquo;ll call it $p$. The network should look something like $p=f(x; W,b)$, where $x$ is the input array, $p$ is the network&amp;rsquo;s output array, $W$ are the weights, and $b$ are the biases. A simple neural network could then be written like this
$$ p = W \cdot x + b. $$
But wait, you say; this is just a linear transformation! Layering linear transformations on top of each other can only create a linear network. You can&amp;rsquo;t learn complex patterns and make accurate predictions with just linear transformations. To introduce nonlinearity into the model, we throw this into a nonlinear activation function $\sigma$, so the output looks like
$$ p = \sigma(W \cdot x + b). \label{1} \tag{1} $$
There are a few commonly used activation functions that one frequently encounters: the sigmoid function, the hyperbolic tangent (tanh) function, the rectified linear unit (ReLU) function, and so on. We&amp;rsquo;ll use a generalization of the sigmoid (or logistic) function for our experiments.
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
This function maps a real-valued input to a value between 0 and 1, so the output can be interpreted as a probability, making it directly useful in classification problems.&lt;/p>
&lt;p>To get into more detail, we need to understand and prepare the input. I use the MNIST dataset for the demonstration below. We will avoid the powerful machine learning packages and only use NumPy&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, so all operations can be considered elemental. You can follow along on &lt;a href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing" target="_blank" rel="noopener">Colab&lt;/a>.
&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>&lt;/p>
&lt;h2 id="mnist-dataset-prepare-the-input">MNIST Dataset: Prepare the input&lt;/h2>
&lt;p>The MNIST dataset is a large database of handwritten digits consisting of 60,000 training images and 10,000 test images. Each image is a 28x28 grayscale image labeled with the correct digit, from 0 to 9. It&amp;rsquo;s commonly used for training and testing various image processing and machine learning algorithms. The digits in grayscale look like this
&lt;figure id="figure-handwritten-digits-0-and-1-from-the-mnist-dataset">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Handwritten digits from the MNIST dataset" srcset="
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp 400w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_3fc866dcf8d288cdcdc65060faa475c2.webp 760w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp"
width="600"
height="400"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Handwritten digits 0 and 1 from the MNIST dataset.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Our goal is to teach the single-layer network (\ref{1}) to recognize these handwritten digits. The network learns from the training dataset by adjusting its weights to minimize the difference between the desired output and the actual output, that is, the loss. This process is repeated until the loss is sufficiently small, implying that the network has learned the dataset. So we need to define the loss, calculate how it depends on layer parameters, and find a way to minimize it iteratively.&lt;/p>
&lt;h2 id="layer-and-loss-build-the-model">Layer and Loss: Build the model&lt;/h2>
&lt;p>Our model architecture consists of just the one layer in (\ref{1}). So this is an example of shallow learning. But even with shallow networks, it can get confusing with the number of samples, inputs, and outputs. To recap, we have $M=60,000$ samples in the training set; each sample has $N=28\times28=784$ dimensions (one for each pixel in a flattened 1D-array); the output has $K=$10 dimensions (one for each digit). Accounting is worse when you have hidden layers in between. They all live in different spaces, so it makes sense to introduce different types of letters into the tensor notation for each type of space. To clarify each space, I like to define indices with their own ranges&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:
$$ \begin{align}
a,b&amp;amp;=1,2,\dots,M=60,000. \\
\alpha, \beta&amp;amp;=1,2,\dots,N=784. \\
i,j&amp;amp;=1,2,\dots,K=10.
\end{align} $$
We can then write the output of our AI algorithm as
$$ p_{ai} = \sigma(z_{ai}) = \sigma\left( \sum_{\alpha=1}^{N} x_{a\alpha} W_{\alpha i} + b_i \right). \tag{2} \label{2} $$
Here, $\sigma$ is a generalization of the sigmoid function, called the softmax function. In code, we write&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This step is sometimes called the forward pass.&lt;/p>
&lt;p>We do not use the sigmoid function because we have to ensure that the output probabilities sum to one. The softmax function is defined as
$$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}. \tag{3} \label{3} $$
It maps a vector of arbitrary real values, $z_i$, to a vector of values between 0 and 1. The sum of all outputs is 1, so each output can be interpreted as a probability. This makes it a useful activation function for multiclass classification tasks, where the predicted probabilities must sum to 1.&lt;/p>
&lt;p>The softmax function is numerically unstable if implemented naively, so we rewrite it such that the maximum value of the input array is 0.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr />
&lt;p>We now need to devise a way to tell our network when its outputs, $p_{ai}$, are losers. This is done by defining a loss function that measures the difference between the desired output, $y$, and the actual output, $p$. There are many possible choices. For example, when predicting a continuous variable, one typically uses a regression loss function such as mean squared error or mean absolute error. We have a multiclass classification problem (one class for each digit), so we&amp;rsquo;ll use the cross-entropy loss&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> defined as
$$ L = - \frac{1}{M} \sum_{a=1}^M \sum_{i=1}^N y_{ai} \log p_{ai}, \tag{4} \label{4} $$
where $p$ is the predicted probability, $y$ is the actual probability, $M$ is the number of samples, and $N$ is the number of classes. In NumPy&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The cross-entropy loss is often used with a softmax activation function in the output layer of a classification model. One of the main reasons is the beautiful simplification of its derivative with respect to the input. But I&amp;rsquo;m getting ahead of myself here.&lt;/p>
&lt;p>How do we minimize the loss? We can evaluate the function at different locations in the parameter space to search for the minimum, but that&amp;rsquo;s not an efficient approach, especially in high dimensions. In our simple toy example, the parameter space of weights and biases has $784\times 10+10=7,850$ dimensions.&lt;/p>
&lt;p>A better approach is gradient descent. We start at some random point which will invariably have a large loss. It&amp;rsquo;s like starting on a big hill. To go to the bottom, you take small steps downhill, that is, you descend along the negative gradient, until you can&amp;rsquo;t go reasonably further. We need to compute the gradient of the loss function with respect to the weights and biases to determine the downhill direction. Using the chain rule, we obtain the following formula for the derivative of the loss function with respect to bias
$$ \frac{\partial{L}}{\partial b_{j}} = - \frac{1}{M}\sum_{a=1}^M \sum_{i=1}^N \sum_{k=1}^N y_{ai}\frac{\partial \log p_{ai}}{\partial z_{ak}} \frac{\partial z_{ak}}{\partial b_j} . $$
This calculation is where the simplification comes in when you combine the cross-entropy loss with softmax activation. To demonstrate, write the total loss as the mean of the losses of all samples, $L=\tfrac{1}{M}\sum_{a=1}^M \ell_a$. Let&amp;rsquo;s compute the derivative for a single sample, suppressing its index
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = - \sum_{i=1}^N \sum_{k=1}^N y_{i}\frac{\partial \log p_{i}}{\partial z_{k}} \frac{\partial z_{k}}{\partial b_j} . $$
The log-term&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> with the definition of softmax (\ref{3}) reads
$$ \log p_i = \log (\sigma(z_i)) = z_i - \log\left(\sum_{j=1}^K e^{z_j}\right).$$
We get for the $z$-derivative
$$ \frac{\partial \log p_i}{\partial z_k} = \delta_{ik} - \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} = \delta_{ik} - p_k.$$
Combining with the summation over $y$, we obtain this very simple formula
$$ \sum_{i=1}^{N} y_{i} (\delta_{ik} - p_k) = y_k - p_k \sum_{i=1}^{N} y_{i} = y_k - p_k. $$
For the last step, remember that $y_i$ are probabilities that sum up to 1. We then have
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = \sum_{k=1}^N (p_k - y_k)\frac{\partial z_{k}}{\partial b_j} $$
Now we can insert the dependence of $z$ on $b$ and bring back the summation over the samples with index $a$ to get
$$ \frac{\partial{L}}{\partial b_{j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}). $$
Similarly, for the weights
$$ \frac{\partial{L}}{\partial W_{\beta j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}) x_{a\beta}. $$
In NumPy, the gradient of the loss is then calculated by&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We then update the weights and biases based on these gradients with some step size $\lambda$. What mathematicians call step size is called learning rate in machine learning, even though it&amp;rsquo;s not quite a rate. Anyway, we update the weights and biases iteratively as follows
$$ \begin{align}
W^{n+1} &amp;amp;= W^n - \lambda \frac{\partial{L}}{\partial W}, \\
b^{n+1} &amp;amp;= b^n - \lambda \frac{\partial{L}}{\partial b}.
\end{align} $$&lt;/p>
&lt;h2 id="train-and-evaluate">Train and Evaluate&lt;/h2>
&lt;p>We now have everything in place to train the network using the training set. To recap, we initialize the weights and biases randomly and then run multiple epochs&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, during which we get the network output (forward pass), compute the gradient of the associated loss, and update the weights and biases in the direction of the negative gradient with a constant learning rate (backpropagation). Below is all of this in code.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">200&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To evaluate the accuracy of the network on the training set, we compare the network&amp;rsquo;s prediction with the labels&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">accuracy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[];&lt;/span> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">accuracy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count_nonzero&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We get about %90 accuracy with this simple method on both the test and the training sets!&lt;/p>
&lt;p>
&lt;figure id="figure-final-accuracy-and-loss-after-training-for-200-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final result of training" srcset="
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp 400w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_2dc1e69ab398c40bfcd5a110fc5ac088.webp 760w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp"
width="760"
height="380"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final accuracy and loss after training for 200 steps.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>The building block of a neural network is the single layer (\ref{1}). I hope you obtained an understanding of how a neural network layer is trained and what people mean when they say the machine has learned something. My goal was to provide a basic understanding of this procedure. This can get very complicated. Large operational machine learning models, such as &lt;a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noopener">GPT-3&lt;/a>, &lt;a href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval" target="_blank" rel="noopener">Gopher&lt;/a>, or &lt;a href="https://developer.nvidia.com/megatron-turing-natural-language-generation" target="_blank" rel="noopener">Megatron-Turing NLG&lt;/a> use many network layers with hundreds of billions of parameters, but the procedure is similar.&lt;/p>
&lt;p>You now have a few simple tools as a starting point for further inquiry. Here are a couple of directions to go from here:&lt;/p>
&lt;ul>
&lt;li>Use a more complex neural network with at least one hidden layer. The &amp;ldquo;deep&amp;rdquo; in deep learning comes from hidden layers. Each layer dramatically increases the parameter space&amp;rsquo;s dimension, which also increases computation time. But the gain in accuracy may be worth it. You can easily get about %97 accuracy with just one additional layer for the MNIST dataset.&lt;/li>
&lt;li>Batch-process your samples. Batch processing was not necessary for his example, but with higher dimensions and larger training sets, it becomes necessary. There are also indications that batch-processed (stochastic) gradient descent generalizes better.&lt;/li>
&lt;li>Use a better optimization procedure. The step size, $\lambda$, or &lt;code>lr&lt;/code>, is the most important parameter in gradient descent. I chose the rather large value of &lt;code>lr=6&lt;/code>. You can see in the plots that the loss doesn&amp;rsquo;t decrease monotonously. A simple fix for this is to multiply the learning rate with a scalar slightly less than 1, so it gets smaller at every epoch. But more importantly, you should use a better optimizer such as Adams. In fact, my interest in optimization is the reason I wrote this post. With Jingcheng Lu and Eitan Tadmor from the University of Maryland, we constructed a &lt;a href="https://anilzen.github.io/publication/2022-swarm-based-gradient-descent/">swarm-based gradient descent&lt;/a> that avoids getting trapped at local minima. The method beats existing optimizers in certain types of non-convex optimization problems. I&amp;rsquo;m experimenting to see whether it&amp;rsquo;s useful in machine learning applications.&lt;/li>
&lt;/ul>
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>NumPy is a free, open-source Python library for scientific computing and data analysis. It has a lot of functionality, but the main reason for using NumPy here is its speed. It uses vectorization instead of looping through individual elements to perform calculations on an array, allowing faster calculations and more efficient use of memory. NumPy operations are implemented in C using highly optimized libraries that take advantage of modern processor architectures, so we are not slowed down by Python&amp;rsquo;s excruciatingly inefficient loops.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This is common practice in theoretical physics, in particular, in general relativity, where we have maps between and projections onto various different spaces and subspaces. The tensor notation using different types of letters helps keep track of the spaces in the computations. I was tempted to introduce the Einstein summation convention here as well, but it doesn&amp;rsquo;t quite work.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>You might recognize this expression of entropy. It appears commonly as $-\sum p_i \log p_i$ in different entropy formulas such as Gibbs entropy, Shannon entropy, and von Neumann entropy. In our case, it&amp;rsquo;s a way of measuring the difference between two probability distributions. We compare the true distribution of the labeled data, $y$, and the predicted probability distribution from the model, $p$. When the same probability distribution $p$ is used on either side of the log function, for example, in Shannon entropy, it measures the amount of uncertainty or randomness in that given probability distribution, which is useful for encoding information or measuring the level of disorder in a system. The purpose and interpretation are different in those cases, but the underlying similarity is the quantification of disorder, which justifies using the term entropy.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>It seems that in machine learning, NumPy, and many other mathematical software programs, $\log$ refers to the natural logarithm with base $e$, which I adopted for this post. It is confusing because, in information theory, $\log$ commonly refers to base 2. But of course &lt;a href="https://en.wikipedia.org/wiki/Logarithm#Particular_bases" target="_blank" rel="noopener">everybody knows that&lt;/a> $\log$ is base 10, and for the other stuff, you either write $\ln$ for base $e$ or $\log_2$ for base 2. In short, use your $\log$ with caution.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>You might think that the double transpose in &lt;code>np.dot(dL.T,x).T&lt;/code> is unnecessary. Why not write it as &lt;code>np.dot(x.T, dL)&lt;/code>? It turns out that the former dot product is faster because of the way the data is stored in memory.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>These are actually iterations. The term epochs alludes to a bright future where you might want to process your training data in batches. Batch processing is among the many directions you might want to expand the code.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>How to draw Penrose diagrams</title><link>https://anilzen.github.io/post/2022/drawing-penrose-diagrams/</link><pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/drawing-penrose-diagrams/</guid><description>&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Penrose_diagram" target="_blank" rel="noopener">Penrose diagram&lt;/a> is a valuable tool in relativity to illustrate the global causal structure of spacetimes&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. Often, qualitative diagrams are sufficient if you&amp;rsquo;re mainly interested in the essential causal relationships. But sometimes, for numerical work or for your own understanding, you need a quantitatively correct diagram.&lt;/p>
&lt;p>Below, we draw Penrose diagrams using free and open-source software. We&amp;rsquo;ll draw the diagrams using the LaTeX package &lt;a href="https://en.wikipedia.org/wiki/PGF/TikZ" target="_blank" rel="noopener">TikZ&lt;/a> (not a drawing program but a program that draws&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>). We&amp;rsquo;ll use Python to perform transformations when we hit the computational limitations of TikZ.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
The code for all diagrams below is on my &lt;a href="https://github.com/anilzen/anilzen.github.io/tree/main/content/post/2022/drawing-penrose-diagrams/tikz" target="_blank" rel="noopener">website repository&lt;/a>.
&lt;/div>
&lt;/div>
&lt;h3 id="the-basic-idea-behind-penrose-diagrams">The basic idea behind Penrose diagrams&lt;/h3>
&lt;p>The Penrose diagram is an extension, or better, a &lt;em>completion&lt;/em> of the &lt;a href="https://en.wikipedia.org/wiki/Spacetime_diagram" target="_blank" rel="noopener">Minkowski diagram&lt;/a>. Like in the Minkowski diagram, time is vertical, space is horizontal, and null rays are at 45 degrees to the axes. In contrast to the Minkowski diagram, a Penrose diagram includes &amp;ldquo;points at infinity&amp;rdquo; added by compactification, thereby visualizing the rich structure of infinity arising from the unification of space and time.&lt;/p>
&lt;p>Below are two beautiful &lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">TikZ diagrams&lt;/a> by &lt;a href="https://www.physik.uzh.ch/en/researcharea/cms/people/Izaak-Neutelings.html" target="_blank" rel="noopener">Izaak Neutelings&lt;/a>. On the left is the Minkowski diagram. Spacetime extends infinitely in all directions. On the right is the Penrose diagram representing the entire spacetime in a finite square.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-minkowski-diagramhttpstikznetrelativity_penrose_diagram">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Minkowski diagram](https://tikz.net/relativity_penrose_diagram/)" srcset="
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_00daccc019c1c8e7d5b53f79e0e59c99.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_463aa2ce8f62b82dc39936265db730ef.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/neutelings_minkowski_hub48f7d8ad14fbf2ddf097172b8405b3c_351951_00daccc019c1c8e7d5b53f79e0e59c99.webp"
width="760"
height="626"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">Minkowski diagram&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-penrose-diagramhttpstikznetrelativity_penrose_diagram">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="[Penrose Diagram](https://tikz.net/relativity_penrose_diagram/)" srcset="
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_ac06b1fd002695e2e4dc0081a38f765b.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_6a8d1b9f9b8453134e80447669cc1564.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/neutelings_penrose_hud64c81516ee3adbf604a4e1198a47bf6_845479_ac06b1fd002695e2e4dc0081a38f765b.webp"
width="760"
height="661"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://tikz.net/relativity_penrose_diagram/" target="_blank" rel="noopener">Penrose Diagram&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Compactification maps the Minkowski diagram to the Penrose diagram by mapping the null directions to a finite interval. Let&amp;rsquo;s see how that works.&lt;/p>
&lt;h3 id="minkowski-spacetime">Minkowski spacetime&lt;/h3>
&lt;p>Consider the 4-dimensional Minkowski spacetime with standard spherical coordinates: $t \in (-\infty,\infty)$, $r \in [0,\infty)$, $\theta\in[0,\pi]$, $\varphi\in[0,2\pi)$ with metric
$$ ds^2 = -dt^2 + dr^2 + r^2 d\sigma^2, $$
where $d\sigma^2=d\theta^2+\sin^2\theta d\varphi^2$. Penrose diagrams are two-dimensional, so we essentially ignore the angular coordinates represented by the $d\sigma^2$ piece. Each point on the diagram, except the line at the origin, represents a sphere. Spherical light rays propagate along the directions $t+r$ and $t-r$. These directions are called &lt;a href="https://en.wikipedia.org/wiki/Null_vector" target="_blank" rel="noopener">null&lt;/a> because they are in the &lt;a href="https://en.wikipedia.org/wiki/Kernel_%28linear_algebra%29" target="_blank" rel="noopener">nullspace&lt;/a> of the metric.&lt;/p>
&lt;p>Underlying the Penrose diagrams are coordinates, $T$ and $R$, which compactify these null directions:
$$ T+R = \frac{2}{\pi}\textcolor{DarkOrchid}{\arctan}(t+r),$$
$$ T-R = \frac{2}{\pi}\textcolor{DarkOrchid}{\arctan}(t-r). $$
The scale factor $2/\pi$ is to map the range of the coordinates to $T\pm R \in(-1,1)$. Note that I&amp;rsquo;m not including the endpoints in the intervals yet. The Penrose compactification procedure involves the conformal completion of the spacetime, which adds the points at infinity after a regularizing rescaling. This detail is not relevant for the diagrams but is important for geometry.&lt;/p>
&lt;p>The Penrose coordinates $(T,R)$ read in terms of the standard $(t,r)$ as:
$$ T(t,r) = \frac{1}{\pi}\left( \arctan(t+r) + \arctan(t-r) \right)$$
$$ R(t,r) = \frac{1}{\pi}\left( \arctan(t+r) - \arctan(t-r) \right)$$&lt;/p>
&lt;p>Let&amp;rsquo;s make some plots using these coordinates. I&amp;rsquo;ll demonstrate plotting time surfaces (level sets of a time coordinate). Say you&amp;rsquo;d like to draw constant $t$ surfaces in the Penrose diagram. You might be tempted to solve the above relationships for a graph function $T(R;t)$ with constant $t$. But it is easier to let the machine do the work and plot the surfaces parametrically. Define the functions $T(t,r)$ and $R(t,r)$ in TikZ as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl">&lt;span class="k">\tikzset&lt;/span>&lt;span class="nb">{&lt;/span>declare function=&lt;span class="nb">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> T(&lt;span class="k">\t&lt;/span>,&lt;span class="k">\r&lt;/span>) = &lt;span class="k">\fpeval&lt;/span>&lt;span class="nb">{&lt;/span>1/pi*(atan(&lt;span class="k">\t&lt;/span>+&lt;span class="k">\r&lt;/span>) + atan(&lt;span class="k">\t&lt;/span>-&lt;span class="k">\r&lt;/span>))&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> R(&lt;span class="k">\t&lt;/span>,&lt;span class="k">\r&lt;/span>) = &lt;span class="k">\fpeval&lt;/span>&lt;span class="nb">{&lt;/span>1/pi*(atan(&lt;span class="k">\t&lt;/span>+&lt;span class="k">\r&lt;/span>) - atan(&lt;span class="k">\t&lt;/span>-&lt;span class="k">\r&lt;/span>))&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">}}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, plot the lines parametrically.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl">&lt;span class="k">\def\Nlines&lt;/span>&lt;span class="nb">{&lt;/span>6&lt;span class="nb">}&lt;/span> &lt;span class="c">% total number of lines is 2\Nlines+1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="k">\newcommand\samp&lt;/span>&lt;span class="na">[1]&lt;/span>&lt;span class="nb">{&lt;/span> tan(90*#1) &lt;span class="nb">}&lt;/span> &lt;span class="c">% for equidistant sampling
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span>&lt;span class="k">\foreach&lt;/span> &lt;span class="k">\i&lt;/span> [evaluate=&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>=&lt;span class="k">\i&lt;/span>/(&lt;span class="k">\Nlines&lt;/span>+1);&lt;span class="nb">}&lt;/span>] in &lt;span class="nb">{&lt;/span>-&lt;span class="k">\Nlines&lt;/span>,...,&lt;span class="k">\Nlines&lt;/span>&lt;span class="nb">}{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\message&lt;/span>&lt;span class="nb">{&lt;/span>Drawing i=&lt;span class="k">\i&lt;/span>...&lt;span class="nb">^^&lt;/span>J&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\draw&lt;/span>&lt;span class="na">[line width=0.3,samples=30,smooth,variable=\r,domain=0.001:1]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> plot(&lt;span class="nb">{&lt;/span> R(&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>&lt;span class="nb">}&lt;/span>,&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\r&lt;/span>&lt;span class="nb">}&lt;/span>) &lt;span class="nb">}&lt;/span>, &lt;span class="nb">{&lt;/span> T(&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\t&lt;/span>&lt;span class="nb">}&lt;/span>,&lt;span class="k">\samp&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\r&lt;/span>&lt;span class="nb">}&lt;/span>) &lt;span class="nb">}&lt;/span>);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We&amp;rsquo;re sampling points fairly evenly by incorporating the compactification into the plot function. As you can see in the diagrams below, the time surfaces are equally separated from each other at the origin. The function &lt;code>samp&lt;/code> controls the separation of points in the plot.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-compactification-with-arctan-filetikzminkmink_arctantex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Compactification with arctan [(file)](tikz/mink/mink_arctan.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_7c87c5b70173cea05b03cb6ef62b45e7.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_4eb7334e9c0a75b4d551c2668a6b69ba.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_arctan_hu0ed202cf971a20ea79ed5ddc7d6a4c2f_63882_7c87c5b70173cea05b03cb6ef62b45e7.webp"
width="438"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Compactification with arctan &lt;a href="tikz/mink/mink_arctan.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-compactification-with-tanh-filetikzminkmink_tanhtex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Compactification with tanh [(file)](tikz/mink/mink_tanh.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_843cb384a6441847b2660c6b3d4b8f27.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_b04b00b0785f30a374e685c6722f9527.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_tanh_hu39a38b3c1d47f28d8ceb9cd1fe0fce12_61420_843cb384a6441847b2660c6b3d4b8f27.webp"
width="438"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Compactification with tanh &lt;a href="tikz/mink/mink_tanh.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The two versions of the diagram illustrate a coordinate-dependent feature that confuses even the experts. The $t$ surfaces intersect at spatial infinity, $i^0$, on both diagrams. On the left, they are tangent to each other, while on the right, they are not. So this seems to be a coordinate-dependent feature. Penrose used the inverse tangent function in his original papers. Many other choices exist. For example, using the hyperbolic tangent, the mapping reads
$$ T+R = \textcolor{DarkOrchid}{\tanh}(t+r),$$
$$ T-R = \textcolor{DarkOrchid}{\tanh}(t-r). $$
This mapping gives the diagram on the right where $t$ surfaces are not tangential at spatial infinity. There&amp;rsquo;s a subtlety here. Conformal compactification with hyperbolic tangent is not regular when you include the angular dimensions. So be careful what you choose for the mapping.&lt;/p>
&lt;p>I draw Penrose diagrams typically to present the causal structure of &lt;a href="https://hyperboloid.al" target="_blank" rel="noopener">hyperboloidal&lt;/a> surfaces. Such surfaces behave like spacetime hyperboloids: they are spacelike everywhere, including null horizons. Spacetime hyperboloids are typically defined by the level sets of $\tau$ as
$$ t^2 - r^2 = \tau^2 \implies \tau = \pm \sqrt{t^2 - r^2}. $$
This construction appears in many models, such as the &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne model&lt;/a> of cosmology, &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">Dirac&amp;rsquo;s point-form&lt;/a> of relativistic dynamics, the &lt;a href="https://arxiv.org/abs/hep-th/0303006" target="_blank" rel="noopener">de Boer-Solodukhin&lt;/a> holography of Minkowski spacetime, and the &lt;a href="https://arxiv.org/abs/1304.2794" target="_blank" rel="noopener">Buchholz-Roberts framework&lt;/a> of relativistic QED. However, these surfaces are not generally useful for studying evolution in time because they intersect at future null infinity. This may not be immediately obvious from their definition, but you can see it right away on the Penrose diagram (left panel below).&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-intersecting-hyperbolic-slicing-filetikzminkmink_hypal_intersecttex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Intersecting hyperbolic slicing [(file)](./tikz/mink/mink_hypal_intersect.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_5332b674093f643b42a9357e1afcba7a.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_85dcb9c9c06ca82a3c6b36b23a9bc33a.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_hypal_intersect_hu7747645cf07242a4588871dffffb0c08_48436_5332b674093f643b42a9357e1afcba7a.webp"
width="440"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Intersecting hyperbolic slicing &lt;a href="./tikz/mink/mink_hypal_intersect.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-smooth-hyperboloidal-foliation-filetikzminkmink_hypal_foliationtex">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Smooth hyperboloidal foliation [(file)](tikz/mink/mink_hypal_foliation.tex)" srcset="
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_18631d5485c16d6c7c2bc7be7aaf3eb5.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_a4bae7e0f37eb8922bfaa8622f12bf45.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/mink_hypal_foliation_hu3f5b5c63b3b9a07f4845771a0daebf93_45539_18631d5485c16d6c7c2bc7be7aaf3eb5.webp"
width="457"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Smooth hyperboloidal foliation &lt;a href="tikz/mink/mink_hypal_foliation.tex">(file)&lt;/a>
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>A better option for a foliation of Minkowski spacetime, illustrated on the right panel above, is to use time-shifted hyperboloids. We pick one spacetime hyperboloid, say, with unit radius, and shift it in time by $\tau$, like this
$$ (t-\tau)^2 - r^2 = 1 \implies \tau = t \pm \sqrt{1+r^2}. $$
Choosing the minus sign gives a future hyperboloidal foliation. The surfaces do not intersect but provide a smooth foliation of future null infinity.&lt;/p>
&lt;p>This example demonstrates how causal structures that may not be immediately clear from the formulas can be illustrated and understood with Penrose diagrams. Other examples include the counterintuitive causal structure of hyperboloidal surfaces or the intersection of standard time surfaces at spatial infinity. We can demonstrate such properties by calculations, but it&amp;rsquo;s much easier to understand them with the help of a Penrose diagram.&lt;/p>
&lt;h4 id="reading-data-into-tikz">Reading data into TikZ&lt;/h4>
&lt;p>TikZ is limited in its data processing capabilities. As a consequence, the evaluation of the lines in the plots takes a while. Once you move away from Minkowski spacetime, the transformations become too complicated for TikZ. To handle complicated mathematical transformations, you can generate the data for the plots in Python, write them in a file, and plot them with TikZ. Below is the Python code to generate the data files for each time surface.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">t&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mf">2.&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">14&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">endpoint&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">30&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">penrose_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">1.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">penrose_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t_val&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">savetxt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;arctan_data&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;.csv&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">delimiter&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;,&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fmt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;R,T&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">comments&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You&amp;rsquo;ll need the &lt;a href="https://numpy.org/" target="_blank" rel="noopener">numpy&lt;/a> library installed in your environment (use &lt;a href="https://xkcd.com/1987/" target="_blank" rel="noopener">local environments for Python!&lt;/a>). The array &lt;code>t&lt;/code> contains the constant values of the time surfaces, the array &lt;code>r&lt;/code> includes the samples of the plot parameter. The domains stop at $\pi/2$ because we use the $\tan$ function to sample the points for a relatively even distribution. Then, for each value of &lt;code>t&lt;/code>, we write the $R,T$ coordinates into a CSV file with headers and some formatting.&lt;/p>
&lt;p>On the TikZ side, we read these points from the respective files and plot them. To plot data points, I use the library &lt;code>pgfplots&lt;/code>. The nodes must be drawn with respect to the axis of the plot, which is achieved with the &lt;code>axis cs:&lt;/code> directive. The relevant part of the code is below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-latex" data-lang="latex">&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\begin&lt;/span>&lt;span class="nb">{&lt;/span>axis&lt;span class="nb">}&lt;/span>[axis lines=none, xmin=-.1,xmax=1.1,ymin=-1.2,ymax=1.2,width=0.5&lt;span class="k">\textwidth&lt;/span>,height=0.8&lt;span class="k">\textwidth&lt;/span>]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[right]&lt;/span> at (axis cs:0.6,1) &lt;span class="nb">{&lt;/span>&lt;span class="k">\small&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\arctan&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="nv">\cdot&lt;/span>&lt;span class="o">)&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\coordinate&lt;/span> (O) at (axis cs:0,0) ; &lt;span class="c">% center: origin (r,t) = (0,0)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (S) at ( axis cs:0,-1); &lt;span class="c">% south: t=-infty, i-
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (N) at ( axis cs:0, 1); &lt;span class="c">% north: t=+infty, i+
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\coordinate&lt;/span> (E) at ( axis cs:1, 0); &lt;span class="c">% east: r=+infty, i0
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\draw&lt;/span>&lt;span class="na">[thick]&lt;/span> (N) -- (E) -- (S) -- cycle;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\newcommand&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="k">\scri&lt;/span>&lt;span class="nb">}{&lt;/span>&lt;span class="k">\mathscr&lt;/span>&lt;span class="nb">{&lt;/span>I&lt;span class="nb">}}&lt;/span> &lt;span class="c">% null infinity
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">&lt;/span> &lt;span class="k">\node&lt;/span>&lt;span class="na">[right]&lt;/span> at (E) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above]&lt;/span> at (N) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[below]&lt;/span> at (S) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">i^&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above, rotate=90]&lt;/span> at (O) &lt;span class="nb">{&lt;/span>&lt;span class="k">\small&lt;/span>&lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">r&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="m">0&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[above right]&lt;/span> at (axis cs: 0.5,0.5) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\scri&lt;/span>&lt;span class="nb">^&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\node&lt;/span>&lt;span class="na">[below right]&lt;/span> at (axis cs: 0.5,-0.5) &lt;span class="nb">{&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nv">\scri&lt;/span>&lt;span class="nb">^&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="s">$&lt;/span>&lt;span class="nb">}&lt;/span>;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">\foreach&lt;/span> &lt;span class="k">\file&lt;/span> in &lt;span class="nb">{{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data0.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data1.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data2.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data3.csv&lt;span class="nb">}&lt;/span>,
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data4.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data5.csv&lt;span class="nb">}&lt;/span>,&lt;span class="nb">{&lt;/span>arctan&lt;span class="nb">_&lt;/span>data6.csv&lt;span class="nb">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">{&lt;/span>&lt;span class="k">\addplot&lt;/span>&lt;span class="na">[domain={-1,1}]&lt;/span> table [x=R, y=T, col sep=comma] &lt;span class="nb">{&lt;/span>&lt;span class="k">\file&lt;/span>&lt;span class="nb">}&lt;/span>;&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">\end&lt;/span>&lt;span class="nb">{&lt;/span>axis&lt;span class="nb">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is probably not the most elegant solution; you switch platforms to draw the plots. If you&amp;rsquo;re up for it, you might prefer using a Python library such as &lt;a href="https://pypi.org/project/tikzplotlib/" target="_blank" rel="noopener">tikzplotlib&lt;/a> to avoid going back and forth between Python and TikZ.&lt;/p>
&lt;h3 id="schwarzschild-spacetime">Schwarzschild spacetime&lt;/h3>
&lt;p>The Schwarzschild line element in standard (Droste) coordinates reads
$$ ds^2 = - f(r) dt^2 + \frac{1}{f(r)} dr^2 + r^2 d\sigma^2, \tag{SS} \label{1}$$
where
$$ f(r) = 1-\frac{2 M}{r},$$
or in dimensionless coordinates
$$ f(r) = 1-\frac{1}{r}.$$
Dimensionless coordinates are similar to setting $M=1/2$.&lt;/p>
&lt;p>The metric is singular where $f(r)$ vanishes, at $r=1$. This surface is the &lt;a href="https://en.wikipedia.org/wiki/Event_horizon" target="_blank" rel="noopener">event horizon&lt;/a>, but the metric singularity is a coordinate artifact. There are regular coordinates across the event horizon, such as the &lt;a href="https://en.wikipedia.org/wiki/Gullstrand%E2%80%93Painlev%C3%A9_coordinates" target="_blank" rel="noopener">Gullstrand–Painlevé&lt;/a> or the &lt;a href="https://en.wikipedia.org/wiki/Eddington%E2%80%93Finkelstein_coordinates" target="_blank" rel="noopener">Eddington–Finkelstein&lt;/a> coordinates. In fact, we must use such regular coordinates to draw Penrose diagrams.&lt;/p>
&lt;p>Penrose diagrams for Schwarzschild spacetime are traditionally drawn using a compactification of &lt;a href="https://en.wikipedia.org/wiki/Kruskal%E2%80%93Szekeres_coordinates" target="_blank" rel="noopener">Kruskal coordinates&lt;/a>. Let&amp;rsquo;s copy them from Wikipedia (for a derivation, see, for example, the Appendix of my &lt;a href="https://anilzen.github.io/publication/zenginoglu-2007-conformal/" target="_blank" rel="noopener">thesis&lt;/a>):
$$ \tau = (r-1) e^r \sinh \tfrac{t}{2}, $$
$$ \rho = (r-1) e^r \cosh \tfrac{t}{2}. $$
The coordinates of the Penrose diagram are compactified along the null directions just as in the Minkowski case:
$$ T = \frac{1}{\pi}\left( \arctan(\tau+\rho) + \arctan(\tau-\rho) \right)$$
$$ R = \frac{1}{\pi}\left( \arctan(\tau+\rho) - \arctan(\tau-\rho) \right)$$
The transformations are more complicated, so we will use Python codes to generate the lines and plot them with TikZ. Here&amp;rsquo;s the Python function to construct the compactified Kruskal coordinates.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">kruskal_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rho&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cosh&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tau&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sinh&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arctan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tau&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">rho&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I construct Schwarzschild radial points from tortoise coordinates that are on Chebyshev nodes because I want higher density near the horizon and infinity. There are different, possibly more effective ways of achieving such a beneficial point distribution, but this method works well enough.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">t_vals&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linspace&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">3.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">7&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r_tort&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tan&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cos&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">pi&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">))[::&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">r_schw&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">real&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lambertw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r_tort&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">t_vals&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">kruskal_coords&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">r_schw&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">t_val&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">fn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;data/ss&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;.csv&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">savetxt&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">fn&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">R&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">delimiter&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;,&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">fmt&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;R,T&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">comments&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;{&amp;#39;&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">fn&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="s1">&amp;#39;},&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Schwarzschild time slices intersect at the bifurcation sphere, $\mathcal{B}$, and at spatial infinity $i^0$. When you&amp;rsquo;re interested in the behavior of fields near the black hole and far away from it (say, to study gravitational waves), it&amp;rsquo;s &lt;a href="https://anilzen.github.io/publication/zenginoglu-2011-geometric/" target="_blank" rel="noopener">better&lt;/a> to use non-intersecting time slices.&lt;/p>
&lt;p>Most useful time functions are related to the Schwarzschild time by a &amp;ldquo;height&amp;rdquo; shift that depends only on the radial coordinate:
$$ t \to t + h(r). $$
For example, the height function for &lt;a href="https://en.wikipedia.org/wiki/Gullstrand%E2%80%93Painlev%C3%A9_coordinates" target="_blank" rel="noopener">Gullstrand–Painlevé&lt;/a> coordinates is
$$ h_{\rm GP} (r) = -2 \sqrt{r} + \ln\frac{\sqrt{r}+1}{\sqrt{r}-1}. $$
Note that the height function is singular at the horizon. This singularity is needed to counteract the singularity of Schwarzschild time slices near the bifurcation sphere. Gullstrand–Painlevé gives a nice foliation of the future event horizon, $\mathcal{H}^+$, but the time slices still intersect at spatial infinity.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-schwarzschild-droste-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Schwarzschild-Droste time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_e40ae1b1bcf9878a0ebd29d8891af4da.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_ac19cab7f13834bfd457c9df7e2072af.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_standard_hu727494f74e41712379bdfed7d71bfe07_305624_e40ae1b1bcf9878a0ebd29d8891af4da.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Schwarzschild-Droste time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-gullstrandpainlevé-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Gullstrand–Painlevé time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_627235e3a36de0231d5f308663c9288a.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_e6bc86a8e8efe39e055b87f72c278f9b.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_gp_huc45694372c238d974acb5d587f3bbeb8_320514_627235e3a36de0231d5f308663c9288a.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Gullstrand–Painlevé time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We need a height function that&amp;rsquo;s singular both near and far from the black hole. This idea underlies the construction of hyperboloidal coordinates. For example,
$$ h_{\rm Hyp} (r) = \sqrt{1+ r_\ast^2}, $$
gives the hyperboloidal foliation on the left panel below. The slices do not intersect. Instead, you get a nice, smooth foliation of the full exterior domain. Another example with this nice behavior is the minimal gauge of &lt;a href="https://arxiv.org/abs/1809.02837" target="_blank" rel="noopener">Ansorg, Jaramillo, and Macedo&lt;/a>
$$ h_{\rm MG} (r) = r + 2 \ln r - \ln(r - 1). $$&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure id="figure-hyperbolic-time-slices">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Hyperbolic time slices" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_619c70a07d8ef7fd1fa8eac3ec505535.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_cf6b5d0752b76d74c305fe585553582f.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_hyperbolic_hu727494f74e41712379bdfed7d71bfe07_267505_619c70a07d8ef7fd1fa8eac3ec505535.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Hyperbolic time slices
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;td style="text-align:center">
&lt;figure id="figure-minimal-gauge">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Minimal gauge" srcset="
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_edd5824f8e851782d2c357d381a0ccf6.webp 400w,
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_c34845ff30f7a4ae6610769aa013eeb2.webp 760w,
/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/drawing-penrose-diagrams/figures/ss_minimal_hu727494f74e41712379bdfed7d71bfe07_260868_edd5824f8e851782d2c357d381a0ccf6.webp"
width="760"
height="760"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Minimal gauge
&lt;/figcaption>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Similar constructions can also be made for Kerr, Reissner–Nordström, or Schwarzschild-de Sitter metrics.&lt;/p>
&lt;p>That&amp;rsquo;s it! I hope this helps for the handful of people out there who draw Penrose diagrams. If you have any questions about the diagrams or the source files, email me at &lt;a href="mailto:anil@umd.edu">anil@umd.edu&lt;/a>. I plan to write two more posts on Penrose compactification that should be of broader interest. Stay tuned!&lt;/p>
&lt;!-- These coordinates are good for plotting the maximally extended Schwarzschild solution but they are too complicated if you're primarily interested in the exterior region. You can draw a Penrose diagram for Schwarzschild just like in the Minkowski case by using the tortoise coordinate. The reason is that the $t,r$ portion of the Schwarzschild metric is conformally flat in the tortoise coordinate. Since Penrose diagrams are conformal diagrams, all the usual tricks apply. -->
&lt;!-- The Schwarzschild case reduces to the Minkowski case when we switch to the tortoise coordinate (up to a constant)
$$ r_\ast = \int \frac{dr}{f(r)} = r + 2 M \log \left(\frac{r}{2M}-1\right).$$
The Schwarzschild metric \eqref{1} becomes
$$ ds^2 = f(r(r_\ast)) (- dt^2 + dr_\ast^2) + r(r_\ast)^2 d\sigma^2. $$
Ignoring the angular part, the spacetime metric is just a conformal Minkowski metric in coordinates $(t,r_\ast)$ with in- and outgoing coordinates $t+r_\ast$ and $t-r_\ast$. For a Penrose diagram, you can consider the Schwarzschild metric to be simply $ -dt^2 + dr_\ast^2$. The main difference is the domain of $r_\ast$. While the domain of $r$ in Minkowski is $[0,\infty)$, the domain of the tortoise coordinate $r_\ast$ in Schwarzschild is $(-\infty, \infty)$. This difference is why the Minkowski conformal diagram is a triangle, whereas the conformal diagram for the exterior Schwarzschild spacetime is a diamond. -->
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>For a rigorous, mathematical definition of Penrose diagrams, see the Appendix C.2 in a &lt;a href="https://arxiv.org/abs/gr-qc/0309115" target="_blank" rel="noopener">paper&lt;/a> by Dafermos and Rodnianski.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>The curious capitalization of TikZ derives from the capitalization of the German expression &amp;ldquo;TikZ is kein Zeichenprogramm.&amp;rdquo;&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>