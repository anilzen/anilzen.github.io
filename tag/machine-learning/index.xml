<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning | Anıl Zenginoğlu</title><link>https://anilzen.github.io/tag/machine-learning/</link><atom:link href="https://anilzen.github.io/tag/machine-learning/index.xml" rel="self" type="application/rss+xml"/><description>Machine Learning</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><copyright>Anıl Zenginoğlu</copyright><lastBuildDate>Sun, 02 Apr 2023 00:01:00 +0000</lastBuildDate><image><url>https://anilzen.github.io/media/sharing.jpg</url><title>Machine Learning</title><link>https://anilzen.github.io/tag/machine-learning/</link></image><item><title>Cross-entropy</title><link>https://anilzen.github.io/post/2023/cross-entropy/</link><pubDate>Sun, 02 Apr 2023 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2023/cross-entropy/</guid><description>&lt;p>We&amp;rsquo;re all gonna die.&lt;/p>
&lt;p>Blame the Second Law of Thermodynamics. Entropy increases, we get older, and we die.
&lt;figure id="figure-mark-thompson-new-yorker-magazine">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./entropy_new_yorker_cartoon.jpg" alt="I blame entropy - New Yorker Cartoon" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Mark Thompson, New Yorker Magazine
&lt;/figcaption>&lt;/figure>
This perception of entropy, prevalent in popular culture, presents it as the driving force for decay and disorder. But this cannot be the whole story. The incredible abundance of life on our planet would not have been possible without the self-organization of complex systems. The story of &lt;a href="https://en.wikipedia.org/wiki/Entropy_and_life" target="_blank" rel="noopener">entropy and life&lt;/a> is more complicated than the boy in the above cartoon implies. To understand these larger questions about Life, the Universe, and Everything, we need to first clarify what entropy is.&lt;/p>
&lt;p>This post is about a variant of entropy&amp;mdash;called cross-entropy&amp;mdash;that I wrote about in a post on &lt;a href="../../2022/learning-machine-learning/">machine learning&lt;/a>. There, I presented cross-entropy as a measure of &lt;a href="http://localhost:1313/post/2022/learning-machine-learning/#fn:3" target="_blank" rel="noopener">the difference between two probability distributions&lt;/a>. Most explanations of the concept, including its &lt;a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">Wikipedia entry&lt;/a>, mainly focus on its relevance in information theory, not physics.&lt;/p>
&lt;p>I learned in a &lt;a href="https://www.youtube.com/watch?v=x9COqqqsFtc" target="_blank" rel="noopener">talk&lt;/a> by &lt;a href="https://www.preposterousuniverse.com/" target="_blank" rel="noopener">Sean Carroll&lt;/a> during the &lt;a href="https://qtd-hub.umd.edu/event/symposium-2023/" target="_blank" rel="noopener">Maryland Quantum-Thermodynamics Symposium&lt;/a> that cross-entropy plays a central role in the informational reformulation of the Second Law. This way of thinking about entropy and the Second Law builds a fascinating bridge between machine learning and physics. Before we cross that bridge, let&amp;rsquo;s talk about plain old entropy.&lt;/p>
&lt;h2 id="entropy-without-a-cross">Entropy Without a Cross&lt;/h2>
&lt;p>Entropy is one of the most important concepts in physics. It&amp;rsquo;s the main character of the Second Law of Thermodynamics, which states that the entropy of an isolated system increases over time.&lt;/p>
&lt;p>Despite its importance, entropy is not as widely known or used as energy. Whether you&amp;rsquo;re trying to count your calories, arguing about the geopolitics of natural gas, or worrying about climate change, energy seems to be the main character. But it doesn&amp;rsquo;t quite make sense. We know that energy is conserved; all we do is transform energy from one form to another. Yet we sense that something is irreversibly lost when we &amp;ldquo;spend energy.&amp;rdquo; What exactly are we spending when we burn food or natural gas? Check out the next paragraph. The answer will &lt;a href="#surpriiise">surprise&lt;/a> you!&lt;/p>
&lt;p>Well, it&amp;rsquo;s entropy. And its story starts with heat.&lt;/p>
&lt;h3 id="the-birth-of-heat">The Birth of Heat&lt;/h3>
&lt;p>Thermodynamics is the study of heat, energy, and work. It was born in the 19th century during the Industrial Revolution from the desire to understand how to efficiently convert heat energy into mechanical work.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Nicolas_L%C3%A9onard_Sadi_Carnot" target="_blank" rel="noopener">Sadi Carnot&lt;/a> showed that the efficiency of a heat engine depends only on the temperature difference between the hot and cold reservoirs and not on the specific working substance or the details of the engine design.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> While this observation had huge practical implications, his main contribution for our purposes is the distinction between reversible and irreversible processes, which led to the notion of entropy.&lt;/p>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Entropy#Etymology" target="_blank" rel="noopener">term entropy&lt;/a> was coined by the German physicist &lt;a href="https://en.wikipedia.org/wiki/Rudolf_Clausius" target="_blank" rel="noopener">Rudolf Clausius&lt;/a> in 1865 as a counterpart to the term energy. The 19th-century German intellectuals were enamored with &lt;a href="https://en.wikipedia.org/wiki/Hellenism_%28neoclassicism%29" target="_blank" rel="noopener">neoclassical hellenism&lt;/a>, which resulted in lots of Greek words in scientific literature. &lt;em>Entropia&lt;/em> means &amp;ldquo;transformation to&amp;rdquo; in Greek. So the German word &amp;ldquo;Entropie&amp;rdquo; is the germanized Greek translation of the German word &amp;ldquo;Verwandlungsinhalt,&amp;rdquo; which Clausius used to describe the transformational content of energy.&lt;/p>
&lt;p>When you burn natural gas to generate heat, you spend the transformational content of the natural gas. The heat that results in this process cannot be transformed back; entropy increases. Clausius formulated the observation that heat flows naturally from a hot body to a cooler one through the inequality
$$ dS ≥ \frac{\delta Q}{T}. $$
Clausius used $S$ for entropy in honor of Sadi Carnot, so $dS$ denotes a small change in entropy, $\delta Q$ is the heat the system absorbs from its surroundings, and $T$ is the temperature at which the heat is absorbed. In an adiabatic process without heat exchange, we have $\delta Q=0$, and entropy can never decrease in accordance with the Second Law.&lt;/p>
&lt;p>Entropy encapsulates the irreversible processes that we typically associate with energy usage. Concepts like &lt;a href="https://en.wikipedia.org/wiki/Energy_crisis" target="_blank" rel="noopener">energy crisis&lt;/a> actually refer to entropy crisis: we need a continuous supply of low entropy to keep the world running.&lt;/p>
&lt;p>Clausius&amp;rsquo; inequality doesn&amp;rsquo;t give an origin story or an explanation for entropy. For that, we need statistical physics.&lt;/p>
&lt;h3 id="atoms">Atoms&lt;/h3>
&lt;p>The famous equality that describes entropy is engraved in Boltzmann&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#/media/File:Boltzmann_equation.JPG" target="_blank" rel="noopener">tombstone&lt;/a>&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>
$$ S = k \log W. \tag{1} \label{1} $$
In this expression, $k$ is the Boltzmann constant, $W$ is the number of microstates corresponding to a particular macrostate of the system. To understand what $W$ represents, think of a system composed of many parts, say, tiny atoms. Our description of the system uses a few variables, such as heat and pressure. This macroscopic description is clearly underdetermined: there are gazillions of atomic configurations that result in a given value for heat and pressure. A macrostate is a collection of $W$ individual microstates that are macroscopically indistinguishable. It&amp;rsquo;s the number of equivalent ways the subsystems (atoms) can be arranged without changing the macroscopic state.&lt;/p>
&lt;p>The logarithm in the formula arises from known observations about entropy and simple combinatorics. Consider two systems. It was known that their total entropy is the &lt;em>sum&lt;/em> of their entropies, $S=S_1+S_2$, but the total number of microstates for the full system is the &lt;em>product&lt;/em> of its parts, $W=W_1*W_2$. The only function that converts a product into a sum is the $\log$ which tells us that $S\sim \log W$. The Boltzmann constant in \eqref{1} makes the units work.&lt;/p>
&lt;p>The Second Law is then a probabilistic statement: among different macrostates, the system evolves towards a more probable configuration, one with a larger number of microstates. In this picture, we don&amp;rsquo;t expect entropy to &lt;em>always&lt;/em> increase. It just happens to be more probable. You will run into fluctuations where entropy goes down if you wait enough.&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>
A rather outrageous extrapolation of this idea is the &lt;a href="https://en.wikipedia.org/wiki/Boltzmann_brain" target="_blank" rel="noopener">Boltzmann brain&lt;/a>: a self-aware brain that spontaneously appears in a universe through random fluctuations rather than through biological evolution.&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>&lt;/p>
&lt;p>
&lt;figure id="figure-boltzmann-brainhttpsenwikipediaorgwikiboltzmann_brain-generated-with-midjourneyhttpswwwmidjourneycom">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./featured.webp" alt="Boltzmann Brain generated with Midjourney" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
&lt;a href="https://en.wikipedia.org/wiki/Boltzmann_brain" target="_blank" rel="noopener">Boltzmann brain&lt;/a> generated with &lt;a href="https://www.midjourney.com" target="_blank" rel="noopener">Midjourney&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h3 id="surpriiise">Surpriiise!&lt;/h3>
&lt;p>With the rise of calculators, computers, and communication devices in the 20th century, information started to play a fundamental role in our description of physical phenomena.&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>
&lt;a href="https://en.wikipedia.org/wiki/Claude_Shannon" target="_blank" rel="noopener">Shannon&amp;rsquo;s&lt;/a> reformulation of entropy in &lt;a href="https://ieeexplore.ieee.org/abstract/document/6773024" target="_blank" rel="noopener">The Mathematical Theory of Communication&lt;/a> relates information to surprise.&lt;/p>
&lt;p>What is surprise? To be surprised, you must have a prior expectation, some sense that things happen in a certain way. The more you expect something, the less surprised you are to see it, and vice versa. Therefore, surprise $s$ should be a decreasing function of probability $p\in [0,1]$. Specifically, we&amp;rsquo;re looking for an expression $s(p)$ that satisfies the following reasonable conditions:&lt;/p>
&lt;ul>
&lt;li>If you&amp;rsquo;re absolutely certain of $x$, then $p(x)=1$ and you&amp;rsquo;re not surprised: $s(1)=0$.&lt;/li>
&lt;li>If you&amp;rsquo;re absolutely certain that $x$ can never happen, then $p(x)=0$ and its occurence surprises you infinitely: $s(0) \to \infty$.&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>&lt;/li>
&lt;li>Surprise should be additive: the total surprise for multiple events should be the addition of the surprise associated with each event. For two events $x_1$ and $x_2$, the combined probability is $p=p_1 * p_2$ and the total surprise should be $s(p) = s(p_1*p_2) = s(p_1) + s(p_2)$&lt;/li>
&lt;/ul>
&lt;p>These conditions are satisfied by a formula that depends logarithmically on the inverse of $p$:
$$ s(p) = \log \frac{1}{p} = - \log p. $$&lt;/p>
&lt;p>Entropy is then the probability-weighted sum of surprise. In other words, entropy is expected surprise:&lt;/p>
&lt;p>$$ S = \sum p_i\ s(p_i) = - \sum p_i \log p_i. \label{2} \tag{2}$$&lt;/p>
&lt;p>Boltzmann used a similar formula &lt;a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula#Generalization" target="_blank" rel="noopener">already in 1866&lt;/a>, yet the expression is named after Gibbs and Shannon. It reduces to Boltzmann&amp;rsquo;s first formula \eqref{1} when the probabilities of all microstates are equal, which can then be used to derive Clausius&amp;rsquo; inequality.&lt;sup id="fnref:7">&lt;a href="#fn:7" class="footnote-ref" role="doc-noteref">7&lt;/a>&lt;/sup>&lt;/p>
&lt;p>An increase in entropy means that the expected surprise increases. This might sound a bit counterintuitive. We learned that entropy is a measure of disorder. How are disorder and surprise related?&lt;/p>
&lt;p>It may be simpler to understand that patterns reduce &lt;em>total&lt;/em> expected surprise. Let&amp;rsquo;s say every time I order a taxi, I get a yellow cab. Over time, the total expected surprise about the color of the taxi cab will be low even though I might get a blue cab once in a blue moon. If, however, the color of the taxi cab is different every single time, those little surprises add up and maximize the total expected surprise. Disorder increases total expected surprise over a collection of events. It&amp;rsquo;s highest when the events are random.&lt;/p>
&lt;h2 id="the-cross-of-entropy">The Cross of Entropy&lt;/h2>
&lt;p>Boltzmann&amp;rsquo;s entropy \eqref{1} generalizes to Gibbs-Shannon entropy \eqref{2}, allowing different probabilities for the microstates. The next generalization&lt;sup id="fnref:8">&lt;a href="#fn:8" class="footnote-ref" role="doc-noteref">8&lt;/a>&lt;/sup> includes a distinction between the expected and observed probabilities and is commonly used to define loss functions in supervised machine learning.&lt;/p>
&lt;h3 id="learning-to-expect-the-unexpected">Learning to Expect the Unexpected&lt;/h3>
&lt;p>In Gibbs-Shannon entropy \eqref{2}, the weights of the sum, $p_i$, are from the same probability distribution that quantifies surprise, $s(p_i)$. But those two distributions are not necessarily the same. Our surprise arises from our assumed expectations, let&amp;rsquo;s call it $q_i$, which may need to be corrected or updated. A good example is climate change, when 100-year storms start happening every decade. The probability distribution for heavy storms has shifted, so we need to adjust our expectations.&lt;/p>
&lt;p>We may formally use the true distribution, $p_i$, for the weights, which are unknown a priori and must be learned from observations. The cross-entropy, $H$, accounts for the difference between true and assumed expectations.
$$ H(p,q) = - \sum p_i \log q_i. $$
The cross-entropy $H$ is the true expected value of our assumed surprise. In other words, it&amp;rsquo;s the expected value, with respect to the true distribution $p$, of our surprise, with respect to the assumed distribution $q$. It measures how likely we are to be surprised (and therefore learn something) if we were told the actual probability distribution of the system.&lt;sup id="fnref:9">&lt;a href="#fn:9" class="footnote-ref" role="doc-noteref">9&lt;/a>&lt;/sup> It obtains a minimum when the two distributions are equal.&lt;/p>
&lt;p>This property is why it&amp;rsquo;s so useful in machine learning where cross-entropy is used to construct the &lt;a href="../../2022/learning-machine-learning/#layer-and-loss-build-the-model">loss function&lt;/a> in multiclass classification tasks. The true labels of the training samples serve as the true distribution; the output labels of the neural network serve as the assumed distribution. The cross-entropy loss function is iteratively reduced by numerical optimization. Eventually, the true distribution of the labels matches the predicted distribution from the neural network sufficiently well. At that point, we say the machine learned the training set.&lt;/p>
&lt;h3 id="the-second-coming-of-the-second-law">The Second Coming of the Second Law&lt;/h3>
&lt;p>The Second Law of Thermodynamics has been reformulated using cross-entropy by Bartolotta, Carroll, Leichenauer, and Pollack to &amp;ldquo;incorporate the effects of a measurement of a system at some point in its evolution.&amp;rdquo; &lt;a href="https://arxiv.org/abs/1508.02421" target="_blank" rel="noopener">The Bayesian Second Law of Thermodynamics&lt;/a> uses an information-theoretic approach. Sean Carroll has a great &lt;a href="https://www.preposterousuniverse.com/blog/2015/08/11/the-bayesian-second-law-of-thermodynamics/" target="_blank" rel="noopener">blog post&lt;/a> about this paper; you should read it. Here&amp;rsquo;s a short description in our context.&lt;/p>
&lt;p>According to the Bayesian Second Law, the cross-entropy of the updated (&amp;ldquo;true&amp;rdquo;) distribution with respect to the original (&amp;ldquo;assumed&amp;rdquo;) distribution, plus the generalized heat flow, is larger when evaluated at the end of the experiment than at the beginning. For zero heat transfer, the expected amount of information an observer would learn by being told the true microstate of the system is larger at the final time than at the initial one. Therefore, cross-entropy can change over time according to how well our initial assumptions about a system match its true underlying distribution and how much new information we gain through measurements and updates to our assumptions.&lt;/p>
&lt;p>This updated Second Law describes the increase in cross-entropy as
$$ \Delta H(p, q) + \langle Q \rangle \geq 0, $$
where $\langle Q \rangle$ is the expectation value of a generalized heat flow out of the system, similar to the term $\delta Q$ in Clausius&amp;rsquo; inequality (with a different sign).&lt;/p>
&lt;p>When the assumed distribution differs significantly from the correct distribution during time evolution, it can lead to information loss and, therefore, a large increase in cross-entropy. Cross-entropy increases with time even with zero heat transfer. In this interpretation, what happens during optimization in a machine learning model (decreasing cross-entropy) is the opposite of what happens in stochastic evolution (increasing cross-entropy): The act of learning is a revolt against disorder and decay!&lt;/p>
&lt;h2 id="the-death-of-heat">The Death of Heat&lt;/h2>
&lt;p>At the beginning of the post, I mentioned that the relationship between life and entropy is complicated. When it comes to the Universe, however, things are much simpler. The Universe is evolving towards &lt;a href="https://en.wikipedia.org/wiki/Heat_death_of_the_universe" target="_blank" rel="noopener">heat death&lt;/a>.&lt;/p>
&lt;p>As the Universe continues to expand and matter becomes more dispersed, it will become increasingly difficult for matter to interact with other matter, and energy will become more evenly distributed. Eventually, all stars will have exhausted their fuel, and the Universe will be a cold, dark, lifeless place where nothing happens.&lt;/p>
&lt;p>One of my favorite science-fiction short stories is Asimov&amp;rsquo;s &lt;a href="http://users.ece.cmu.edu/~gamvrosi/thelastq.html" target="_blank" rel="noopener">The Last Question&lt;/a> from 1956. It&amp;rsquo;s a story about the heat death of the Universe with the perfect punch line. The story begins with two technicians attending to a giant, self-adjusting, and self-correcting computer, called Multivac that found a way to fulfill the Earth&amp;rsquo;s energy needs by drawing energy from the Sun. The technicians argue that the Sun and all the stars in the Universe will eventually run out. They ask Multivac whether entropy can be reversed, to which Multivac replies, &amp;ldquo;INSUFFICIENT DATA FOR MEANINGFUL ANSWER.&amp;rdquo; The story follows the history of humanity across many eons, through interstellar travel and immortality. The last question remains and is asked repeatedly.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="./multivac.png" alt="Asking Multivac" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;p>I won&amp;rsquo;t give away the punchline but it does fit into our observation that &lt;em>learning acts against entropy&lt;/em>. I posed the last question to ChatGPT, our version of the Multivac. Maybe somewhere among the weights and biases in the billions of its connections, ChatGPT is still thinking about it.&lt;/p>
&lt;hr>
&lt;h4 id="footnotes">Footnotes&lt;/h4>
&lt;!-- As techniques from machine learning are applied to solve problems in physics, and vice versa, the connection between the two areas is likely to become even stronger in the future.
Cross-entropy builds a connection between machine learning and fundamental physics through information theory. There are While not ground-breaking, the information-theoretic reformulation of the Second Law may become preferable once it's more widely known. -->
&lt;!-- When you think of a thing, you have what physicists refer to as an isolated system in your mind. In that sense, we are not things. We constantly breath, drink, eat. We are absolutely and critically dependent on a nurturing Universe that keeps us alive.
It is true that everything eventually dies because of entropy, but this is a statement about the universe. It doesn't quite explain why you and me have to die. And why in 100 years. Entropy doesn't prohibit us from living for another 5 billion years. -->&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>Unfortunately, Carnot died from cholera at a relatively young age of 36. His book, &lt;a href="https://en.wikipedia.org/wiki/Reflections_on_the_Motive_Power_of_Fire" target="_blank" rel="noopener">Reflections on the Motive Power of Fire&lt;/a>, self-published in 1824, was largely ignored by the scientific community at the time.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Unfortunately, Boltzmann committed suicide while on a beach vacation with his wife and daughter near Trieste, shortly before the experimental verification of his ideas.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>In small systems with a few parts, such fluctuations happen frequently. Their study is a relatively new topic of research that falls under &lt;a href="https://en.wikipedia.org/wiki/Stochastic_thermodynamics" target="_blank" rel="noopener">stochastic thermodynamics&lt;/a>. One of the main results in that area is the &lt;a href="https://en.wikipedia.org/wiki/Jarzynski_equality" target="_blank" rel="noopener">Jarzynski equality&lt;/a> that relates the free energy difference between two equilibrium states to the average work performed on the system during a non-equilibrium process. As the system size increases, however, it becomes increasingly unlikely that such fluctuations reduce entropy and we recover classical thermodynamics.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>It took me about 5 minutes to generate, modify, and upscale this image using &lt;a href="https://www.midjourney.com/" target="_blank" rel="noopener">Midjourney&lt;/a>. An actual Boltzmann brain would presumably take much longer to form but some people argue that it&amp;rsquo;s more likely than the formation of our entire Universe. Personally, I don&amp;rsquo;t like talking about likelihood in the context of the entire Universe. I rather think &lt;a href="https://en.wikipedia.org/wiki/Tractatus_Logico-Philosophicus" target="_blank" rel="noopener">darüber muss man schweigen&lt;/a>.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>As an example on how fundamental information became in physics, consider that one of the most influential physicists of the 20th century, &lt;a href="https://en.wikipedia.org/wiki/John_Archibald_Wheeler" target="_blank" rel="noopener">John Wheeler&lt;/a>, divided his physics career into &lt;a href="https://plus.maths.org/content/it-bit" target="_blank" rel="noopener">three phases&lt;/a>: &amp;ldquo;Everything is Particles,&amp;rdquo; &amp;ldquo;Everything is Fields,&amp;rdquo; and &amp;ldquo;Everything is Information.&amp;rdquo; These stages may sum up the development of physics in the last four centuries. As we are now fully in the informational stage, it will be fascinating to see how machine learning will impact fundamental developments in physics, not only as a tool, but as a conceptual framework for our quest to understand Nature.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>We could consider setting a maximum here. We now know that, indeed, there is a maximum amount of entropy for a given volume of space. This &lt;a href="https://en.wikipedia.org/wiki/Bekenstein_bound" target="_blank" rel="noopener">upper bound&lt;/a> for entropy is named after John Wheeler&amp;rsquo;s student &lt;a href="https://en.wikipedia.org/wiki/Jacob_Bekenstein" target="_blank" rel="noopener">Jacob Bekenstein&lt;/a> and has to do with black holes. But let&amp;rsquo;s leave the quantization of gravity to a later time.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:7">
&lt;p>There are some subtleties here related to the dimensions and underlying probability distributions. The equivalence of the various formulations of entropy must be demonstrated using certain assumptions. If you notice such subtleties, you probably didn&amp;rsquo;t need to read this post, but I hope you enjoyed it.&amp;#160;&lt;a href="#fnref:7" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:8">
&lt;p>There are other generalizations, such as &lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy" target="_blank" rel="noopener">Rényi entropy&lt;/a>, that are interesting but today&amp;rsquo;s focus is on cross-entropy.&amp;#160;&lt;a href="#fnref:8" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:9">
&lt;p>This interpretation is better understood with the &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank" rel="noopener">Kullback–Leibler divergence&lt;/a> defined by
$$ D(p||q) = \sum p_i \log \frac{p_i}{q_i} = H(p,q) - S(p). $$
This expression vanishes when $p=q$ in accordance with the interpretation that there is nothing left to learn when the true distribution equals our assumed expectation.&amp;#160;&lt;a href="#fnref:9" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Learning Machine Learning</title><link>https://anilzen.github.io/post/2022/learning-machine-learning/</link><pubDate>Thu, 08 Dec 2022 00:01:00 +0000</pubDate><guid>https://anilzen.github.io/post/2022/learning-machine-learning/</guid><description>&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>
&lt;h2 id="neural-network-what-is-machine-learning">Neural Network: What is machine learning?&lt;/h2>
&lt;p>The core of a machine learning algorithm is the neural network that maps inputs to desired outputs. When you read &amp;ldquo;neural network,&amp;rdquo; you might think of the human brain with layers of interconnected neurons working together to solve problems. But in machine learning, we are simply talking about a function with parameters. Lots and lots of parameters. &amp;ldquo;Learning&amp;rdquo; is adjusting these parameters until the difference between the desired output and the actual output of the function is sufficiently small. That&amp;rsquo;s it. In a way, machine learning is the rediscovery of the old adage that you can fit any &lt;a href="https://en.wikipedia.org/wiki/Von_Neumann%27s_elephant" target="_blank" rel="noopener">elephant&lt;/a> with sufficient parameters.&lt;/p>
&lt;p>This was a very short summary of what people mean by machine learning. You can imagine that the function, the parameters, and the adjustment process are all rather sophisticated and can get very complicated. I&amp;rsquo;ll expand on this basic idea below with an example of a machine-learning algorithm.&lt;/p>
&lt;p>First, some terminology. Think of each input dimension as a neuron in a neural network. The parameters of the neural network, or the function, are called weights and biases. Weights represent the strength of the connection between neurons; biases shift the activation threshold of a neuron. By adjusting the weights and biases based on the output, the network learns the patterns in the data and makes predictions on new data.&lt;/p>
&lt;p>Let&amp;rsquo;s write this down. I mentioned that the neural network is just a function with parameters. Its output is usually a probability, so we&amp;rsquo;ll call it $p$. The network should look something like $p=f(x; W,b)$, where $x$ is the input array, $p$ is the network&amp;rsquo;s output array, $W$ are the weights, and $b$ are the biases. A simple neural network could then be written like this
$$ p = W \cdot x + b. $$
But wait, you say; this is just a linear transformation! Layering linear transformations on top of each other can only create a linear network. You can&amp;rsquo;t learn complex patterns and make accurate predictions with just linear transformations. To introduce nonlinearity into the model, we throw this into a nonlinear activation function $\sigma$, so the output looks like
$$ p = \sigma(W \cdot x + b). \label{1} \tag{1} $$
There are a few commonly used activation functions that one frequently encounters: the sigmoid function, the hyperbolic tangent (tanh) function, the rectified linear unit (ReLU) function, and so on. We&amp;rsquo;ll use a generalization of the sigmoid (or logistic) function for our experiments.
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
This function maps a real-valued input to a value between 0 and 1, so the output can be interpreted as a probability, making it directly useful in classification problems.&lt;/p>
&lt;p>To get into more detail, we need to understand and prepare the input. I use the MNIST dataset for the demonstration below. We will avoid the powerful machine learning packages and only use NumPy&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, so all operations can be considered elemental. You can follow along on &lt;a href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing" target="_blank" rel="noopener">Colab&lt;/a>.
&lt;a target="_blank" href="https://colab.research.google.com/drive/1A80Z_o55cJI-PkHjsg-j2sUiwsx3A9EY?usp=sharing">
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
&lt;/a>&lt;/p>
&lt;h2 id="mnist-dataset-prepare-the-input">MNIST Dataset: Prepare the input&lt;/h2>
&lt;p>The MNIST dataset is a large database of handwritten digits consisting of 60,000 training images and 10,000 test images. Each image is a 28x28 grayscale image labeled with the correct digit, from 0 to 9. It&amp;rsquo;s commonly used for training and testing various image processing and machine learning algorithms. The digits in grayscale look like this
&lt;figure id="figure-handwritten-digits-0-and-1-from-the-mnist-dataset">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Handwritten digits from the MNIST dataset" srcset="
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp 400w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_3fc866dcf8d288cdcdc65060faa475c2.webp 760w,
/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/digits_hu2b0dc1340f2bd946add58c7b992aeb19_6240_1188d41ad3fd793b572412ff33b047b3.webp"
width="600"
height="400"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Handwritten digits 0 and 1 from the MNIST dataset.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Our goal is to teach the single-layer network (\ref{1}) to recognize these handwritten digits. The network learns from the training dataset by adjusting its weights to minimize the difference between the desired output and the actual output, that is, the loss. This process is repeated until the loss is sufficiently small, implying that the network has learned the dataset. So we need to define the loss, calculate how it depends on layer parameters, and find a way to minimize it iteratively.&lt;/p>
&lt;h2 id="layer-and-loss-build-the-model">Layer and Loss: Build the model&lt;/h2>
&lt;p>Our model architecture consists of just the one layer in (\ref{1}). So this is an example of shallow learning. But even with shallow networks, it can get confusing with the number of samples, inputs, and outputs. To recap, we have $M=60,000$ samples in the training set; each sample has $N=28\times28=784$ dimensions (one for each pixel in a flattened 1D-array); the output has $K=$10 dimensions (one for each digit). Accounting is worse when you have hidden layers in between. They all live in different spaces, so it makes sense to introduce different types of letters into the tensor notation for each type of space. To clarify each space, I like to define indices with their own ranges&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:
$$ \begin{align}
a,b&amp;amp;=1,2,\dots,M=60,000. \\
\alpha, \beta&amp;amp;=1,2,\dots,N=784. \\
i,j&amp;amp;=1,2,\dots,K=10.
\end{align} $$
We can then write the output of our AI algorithm as
$$ p_{ai} = \sigma(z_{ai}) = \sigma\left( \sum_{\alpha=1}^{N} x_{a\alpha} W_{\alpha i} + b_i \right). \tag{2} \label{2} $$
Here, $\sigma$ is a generalization of the sigmoid function, called the softmax function. In code, we write&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This step is sometimes called the forward pass.&lt;/p>
&lt;p>We do not use the sigmoid function because we have to ensure that the output probabilities sum to one. The softmax function is defined as
$$\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}. \tag{3} \label{3} $$
It maps a vector of arbitrary real values, $z_i$, to a vector of values between 0 and 1. The sum of all outputs is 1, so each output can be interpreted as a probability. This makes it a useful activation function for multiclass classification tasks, where the predicted probabilities must sum to 1.&lt;/p>
&lt;p>The softmax function is numerically unstable if implemented naively, so we rewrite it such that the maximum value of the input array is 0.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">z&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)[:,&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">newaxis&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr />
&lt;p>We now need to devise a way to tell our network when its outputs, $p_{ai}$, are losers. This is done by defining a loss function that measures the difference between the desired output, $y$, and the actual output, $p$. There are many possible choices. For example, when predicting a continuous variable, one typically uses a regression loss function such as mean squared error or mean absolute error. We have a multiclass classification problem (one class for each digit), so we&amp;rsquo;ll use the cross-entropy loss&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> defined as
$$ L = - \frac{1}{M} \sum_{a=1}^M \sum_{i=1}^N y_{ai} \log p_{ai}, \tag{4} \label{4} $$
where $p$ is the predicted probability, $y$ is the actual probability, $M$ is the number of samples, and $N$ is the number of classes. In NumPy&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The cross-entropy loss is often used with a softmax activation function in the output layer of a classification model. One of the main reasons is the beautiful simplification of its derivative with respect to the input. But I&amp;rsquo;m getting ahead of myself here.&lt;/p>
&lt;p>How do we minimize the loss? We can evaluate the function at different locations in the parameter space to search for the minimum, but that&amp;rsquo;s not an efficient approach, especially in high dimensions. In our simple toy example, the parameter space of weights and biases has $784\times 10+10=7,850$ dimensions.&lt;/p>
&lt;p>A better approach is gradient descent. We start at some random point which will invariably have a large loss. It&amp;rsquo;s like starting on a big hill. To go to the bottom, you take small steps downhill, that is, you descend along the negative gradient, until you can&amp;rsquo;t go reasonably further. We need to compute the gradient of the loss function with respect to the weights and biases to determine the downhill direction. Using the chain rule, we obtain the following formula for the derivative of the loss function with respect to bias
$$ \frac{\partial{L}}{\partial b_{j}} = - \frac{1}{M}\sum_{a=1}^M \sum_{i=1}^N \sum_{k=1}^N y_{ai}\frac{\partial \log p_{ai}}{\partial z_{ak}} \frac{\partial z_{ak}}{\partial b_j} . $$
This calculation is where the simplification comes in when you combine the cross-entropy loss with softmax activation. To demonstrate, write the total loss as the mean of the losses of all samples, $L=\tfrac{1}{M}\sum_{a=1}^M \ell_a$. Let&amp;rsquo;s compute the derivative for a single sample, suppressing its index
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = - \sum_{i=1}^N \sum_{k=1}^N y_{i}\frac{\partial \log p_{i}}{\partial z_{k}} \frac{\partial z_{k}}{\partial b_j} . $$
The log-term&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> with the definition of softmax (\ref{3}) reads
$$ \log p_i = \log (\sigma(z_i)) = z_i - \log\left(\sum_{j=1}^K e^{z_j}\right).$$
We get for the $z$-derivative
$$ \frac{\partial \log p_i}{\partial z_k} = \delta_{ik} - \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} = \delta_{ik} - p_k.$$
Combining with the summation over $y$, we obtain this very simple formula
$$ \sum_{i=1}^{N} y_{i} (\delta_{ik} - p_k) = y_k - p_k \sum_{i=1}^{N} y_{i} = y_k - p_k. $$
For the last step, remember that $y_i$ are probabilities that sum up to 1. We then have
$$ \frac{\partial{\ell_a}}{\partial b_{j}} = \sum_{k=1}^N (p_k - y_k)\frac{\partial z_{k}}{\partial b_j} $$
Now we can insert the dependence of $z$ on $b$ and bring back the summation over the samples with index $a$ to get
$$ \frac{\partial{L}}{\partial b_{j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}). $$
Similarly, for the weights
$$ \frac{\partial{L}}{\partial W_{\beta j}} = \frac{1}{M} \sum_{a=1}^M (p_{aj}-y_{aj}) x_{a\beta}. $$
In NumPy, the gradient of the loss is then calculated by&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dL&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dL&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We then update the weights and biases based on these gradients with some step size $\lambda$. What mathematicians call step size is called learning rate in machine learning, even though it&amp;rsquo;s not quite a rate. Anyway, we update the weights and biases iteratively as follows
$$ \begin{align}
W^{n+1} &amp;amp;= W^n - \lambda \frac{\partial{L}}{\partial W}, \\
b^{n+1} &amp;amp;= b^n - \lambda \frac{\partial{L}}{\partial b}.
\end{align} $$&lt;/p>
&lt;h2 id="train-and-evaluate">Train and Evaluate&lt;/h2>
&lt;p>We now have everything in place to train the network using the training set. To recap, we initialize the weights and biases randomly and then run multiple epochs&lt;sup id="fnref:6">&lt;a href="#fn:6" class="footnote-ref" role="doc-noteref">6&lt;/a>&lt;/sup>, during which we get the network output (forward pass), compute the gradient of the associated loss, and update the weights and biases in the direction of the negative gradient with a constant learning rate (backpropagation). Below is all of this in code.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">200&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">p&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">p_ai&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">gradient&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">update&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dW&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">db&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To evaluate the accuracy of the network on the training set, we compare the network&amp;rsquo;s prediction with the labels&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">accuracy&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[];&lt;/span> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">...&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">accuracy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">count_nonzero&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">entropy_loss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We get about %90 accuracy with this simple method on both the test and the training sets!&lt;/p>
&lt;p>
&lt;figure id="figure-final-accuracy-and-loss-after-training-for-200-steps">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Final result of training" srcset="
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp 400w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_2dc1e69ab398c40bfcd5a110fc5ac088.webp 760w,
/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/2022/learning-machine-learning/final_hu0b401a0507b8fc9c8c4856e1147d7894_127441_bb9fcc0fb6e39bc15072935c58009e4c.webp"
width="760"
height="380"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Final accuracy and loss after training for 200 steps.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;h2 id="wrapping-up">Wrapping up&lt;/h2>
&lt;p>The building block of a neural network is the single layer (\ref{1}). I hope you obtained an understanding of how a neural network layer is trained and what people mean when they say the machine has learned something. My goal was to provide a basic understanding of this procedure. This can get very complicated. Large operational machine learning models, such as &lt;a href="https://en.wikipedia.org/wiki/GPT-3" target="_blank" rel="noopener">GPT-3&lt;/a>, &lt;a href="https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval" target="_blank" rel="noopener">Gopher&lt;/a>, or &lt;a href="https://developer.nvidia.com/megatron-turing-natural-language-generation" target="_blank" rel="noopener">Megatron-Turing NLG&lt;/a> use many network layers with hundreds of billions of parameters, but the procedure is similar.&lt;/p>
&lt;p>You now have a few simple tools as a starting point for further inquiry. Here are a couple of directions to go from here:&lt;/p>
&lt;ul>
&lt;li>Use a more complex neural network with at least one hidden layer. The &amp;ldquo;deep&amp;rdquo; in deep learning comes from hidden layers. Each layer dramatically increases the parameter space&amp;rsquo;s dimension, which also increases computation time. But the gain in accuracy may be worth it. You can easily get about %97 accuracy with just one additional layer for the MNIST dataset.&lt;/li>
&lt;li>Batch-process your samples. Batch processing was not necessary for his example, but with higher dimensions and larger training sets, it becomes necessary. There are also indications that batch-processed (stochastic) gradient descent generalizes better.&lt;/li>
&lt;li>Use a better optimization procedure. The step size, $\lambda$, or &lt;code>lr&lt;/code>, is the most important parameter in gradient descent. I chose the rather large value of &lt;code>lr=6&lt;/code>. You can see in the plots that the loss doesn&amp;rsquo;t decrease monotonously. A simple fix for this is to multiply the learning rate with a scalar slightly less than 1, so it gets smaller at every epoch. But more importantly, you should use a better optimizer such as Adams. In fact, my interest in optimization is the reason I wrote this post. With Jingcheng Lu and Eitan Tadmor from the University of Maryland, we constructed a &lt;a href="https://anilzen.github.io/publication/2022-swarm-based-gradient-descent/">swarm-based gradient descent&lt;/a> that avoids getting trapped at local minima. The method beats existing optimizers in certain types of non-convex optimization problems. I&amp;rsquo;m experimenting to see whether it&amp;rsquo;s useful in machine learning applications.&lt;/li>
&lt;/ul>
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>NumPy is a free, open-source Python library for scientific computing and data analysis. It has a lot of functionality, but the main reason for using NumPy here is its speed. It uses vectorization instead of looping through individual elements to perform calculations on an array, allowing faster calculations and more efficient use of memory. NumPy operations are implemented in C using highly optimized libraries that take advantage of modern processor architectures, so we are not slowed down by Python&amp;rsquo;s excruciatingly inefficient loops.&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>This is common practice in theoretical physics, in particular, in general relativity, where we have maps between and projections onto various different spaces and subspaces. The tensor notation using different types of letters helps keep track of the spaces in the computations. I was tempted to introduce the Einstein summation convention here as well, but it doesn&amp;rsquo;t quite work.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>You might recognize this expression of entropy. It appears commonly as $-\sum p_i \log p_i$ in different entropy formulas such as Gibbs entropy, Shannon entropy, and von Neumann entropy. In our case, it&amp;rsquo;s a way of measuring the difference between two probability distributions. We compare the true distribution of the labeled data, $y$, and the predicted probability distribution from the model, $p$. When the same probability distribution $p$ is used on either side of the log function, for example, in Shannon entropy, it measures the amount of uncertainty or randomness in that given probability distribution, which is useful for encoding information or measuring the level of disorder in a system. The purpose and interpretation are different in those cases, but the underlying similarity is the quantification of disorder, which justifies using the term entropy.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>It seems that in machine learning, NumPy, and many other mathematical software programs, $\log$ refers to the natural logarithm with base $e$, which I adopted for this post. It is confusing because, in information theory, $\log$ commonly refers to base 2. But of course &lt;a href="https://en.wikipedia.org/wiki/Logarithm#Particular_bases" target="_blank" rel="noopener">everybody knows that&lt;/a> $\log$ is base 10, and for the other stuff, you either write $\ln$ for base $e$ or $\log_2$ for base 2. In short, use your $\log$ with caution.&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5">
&lt;p>You might think that the double transpose in &lt;code>np.dot(dL.T,x).T&lt;/code> is unnecessary. Why not write it as &lt;code>np.dot(x.T, dL)&lt;/code>? It turns out that the former dot product is faster because of the way the data is stored in memory.&amp;#160;&lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:6">
&lt;p>These are actually iterations. The term epochs alludes to a bright future where you might want to process your training data in batches. Batch processing is among the many directions you might want to expand the code.&amp;#160;&lt;a href="#fnref:6" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Special relativity as hyperbolic geometry</title><link>https://anilzen.github.io/post/hyperbolic-relativity/</link><pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate><guid>https://anilzen.github.io/post/hyperbolic-relativity/</guid><description>&lt;p>Space and time bend and stretch with every move.&lt;/p>
&lt;p>I was blown away by this as a young high schooler interested in physics and philosophy. To me, the recognition that natural laws dynamically govern space and time was a triumph of physics over philosophy, of Einstein over Kant, of empirical determination over &lt;a href="https://en.wikipedia.org/wiki/Analytic%E2%80%93synthetic_distinction#Kant%27s_version_and_the_a_priori_/_a_posteriori_distinction" target="_blank" rel="noopener">synthetic a priori&lt;/a>.&lt;/p>
&lt;p>As fascinated as I was by special relativity, the formalism seemed relatively obscure. I recognized patterns in the &lt;a href="https://en.wikipedia.org/wiki/Lorentz_transformation" target="_blank" rel="noopener">Lorentz transformations&lt;/a> but missed a deeper insight into the origins of this structure. At college, I considered various standard derivations of relativistic effects, such as the &lt;a href="https://en.wikipedia.org/wiki/Relativistic_aberration" target="_blank" rel="noopener">relativistic aberration&lt;/a> or the &lt;a href="https://en.wikipedia.org/wiki/Thomas_precession" target="_blank" rel="noopener">Thomas precession&lt;/a>, tedious and boring.&lt;/p>
&lt;p>A simple example is Einstein&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Velocity-addition_formula" target="_blank" rel="noopener">velocity-addition formula&lt;/a>. In Galilean relativity, which corresponds to our everyday experience, we can just add velocities. In special relativity, the speed of light is constant, so the addition of velocities must be such that no composition of velocities exceeds the speed of light. In the simplest case of collinear motion (velocities pointing in the same direction), velocity addition becomes speed addition and looks like this
$$ \tag{1} \label{1}
u = \frac{u_1+u_2}{1+\tfrac{u_1 u_2}{c^2}} $$
The speed of light, $c$, can never be exceeded with this composition law. While we can understand the result, it is not intuitively clear how this addition rule is related to the structure of space and time.&lt;/p>
&lt;p>It turns out that relativistic calculations have natural interpretations in hyperbolic geometry. This viewpoint not only simplifies the calculations but also provides deeper insights into the theory. Its power extends beyond special relativity into machine learning and general relativity. And it all starts with Minkowski.&lt;/p>
&lt;h3 id="hyperbole">Hyperbole&lt;/h3>
&lt;p>In 1908, &lt;a href="https://en.wikipedia.org/wiki/Hermann_Minkowski" target="_blank" rel="noopener">Minkowski&lt;/a> gave a &lt;a href="https://www.math.nyu.edu/~tschinke/papers/yuri/14minkowski/raum-und-zeit.pdf" target="_blank" rel="noopener">lecture&lt;/a> about a new mathematical model for space and time. In his opening remarks, he stated boldly that, from now on, space and time are mere shadows of a more fundamental union: spacetime.&lt;/p>
&lt;blockquote>
&lt;p>Henceforth space by itself, and time by itself, are doomed to fade away into mere shadows, and only a kind of union of the two will preserve an independent reality.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Hermann_Minkowski" target="_blank" rel="noopener">Minkowski&lt;/a> (1908)&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;/blockquote>
&lt;p>The quote is by now famous. It has made its way to various outlets since appearing on the cover page of Synge&amp;rsquo;s &lt;a href="https://www.amazon.com/Relativity-Special-J-L-Synge/dp/B000GP7PX4" target="_blank" rel="noopener">textbook on relativity&lt;/a> in 1956. But when Minkowski made the statement, it was considered hyperbole and encountered resistance from physicists and mathematicians alike.&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>
&lt;p>The spacetime view became widely celebrated a few years after the lecture because of its central role in general relativity. In the same lecture, Minkowski also promoted a non-Euclidean approach to spacetime. As a Göttingen mathematics professor, he recognized the non-Euclidean geometry of special relativity. This idea, however, didn&amp;rsquo;t gain traction as widely as the idea of spacetime. Only in the last 20 years has there been renewed interest in the hyperbolic nature of special relativity.&lt;/p>
&lt;p>To see why hyperbolic geometry is the natural geometry for special relativity, consider a two-dimensional spacetime with coordinates $(t,x)$ and Minkowski metric
$$ ds^2 = -dt^2 + dx^2. $$
A natural object for a metric is the set of points at unit distance from the origin. In two-dimensional Euclidean space, we get a circle, $x^2+y^2=1$. In two-dimensional Minkowski space, we get a hyperbola&lt;br>
$$ \tag{2}\label{2} t^2 - x^2 = 1. $$
Minkowski&amp;rsquo;s drawing below shows this hyperbola for $t&amp;gt;0$. In higher dimensions, you get the one-sheeted &lt;a href="https://en.wikipedia.org/wiki/Hyperboloid" target="_blank" rel="noopener">hyperboloid&lt;/a>, also called hyperbolic hyperboloid.&lt;/p>
&lt;p>
&lt;figure id="figure-minkowskis-drawing-depicts-hyperbola-as-the-invariant-space-under-lorentz-transformations-from-minkowskis-modern-worldhttpshalshsarchives-ouvertesfrhalshs-01234434-by-scott-walter">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Minkowski Drawing" srcset="
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_b89ce1e1b459df492f891c8a09eaf42e.webp 400w,
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_4324b0371c578b6a70ed333dddf1eab4.webp 760w,
/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://anilzen.github.io/post/hyperbolic-relativity/MinkDrawing_hu140b8f32c633551bdc76727a7862e265_291670_b89ce1e1b459df492f891c8a09eaf42e.webp"
width="473"
height="500"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Minkowski&amp;rsquo;s drawing depicts hyperbola as the invariant space under Lorentz transformations. &lt;br>From &lt;a href="https://halshs.archives-ouvertes.fr/halshs-01234434/" target="_blank" rel="noopener">Minkowski&amp;rsquo;s modern world&lt;/a> by Scott Walter.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>In his lecture, Minkowski demonstrated that Lorentz transformations are hyperbolic rotations that leave the hyperbola (\ref{2}) invariant. He died in January 1909 before he could develop this idea further, but it was embraced by a few other scientists, particularly by &lt;a href="https://en.wikipedia.org/wiki/Arnold_Sommerfeld" target="_blank" rel="noopener">Sommerfeld&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Vladimir_Vari%C4%87ak" target="_blank" rel="noopener">Varičak&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Alfred_Robb" target="_blank" rel="noopener">Robb&lt;/a>, and &lt;a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel" target="_blank" rel="noopener">Borel&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>The principle of relativity corresponds to the hypothesis that the kinematic space is a space of constant negative curvature, the space of Lobachevsky and Bolyai. The value of the radius of curvature is the speed of light.&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/%C3%89mile_Borel" target="_blank" rel="noopener">Borel&lt;/a> (1913)&lt;/p>
&lt;/blockquote>
&lt;p>The mathematical formalism for a hyperbolic rotation is just like a regular rotation, except with hyperbolic functions instead of trigonometric functions. &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">Hyperbolic functions&lt;/a> are related to their trigonometric counterparts through &lt;a href="https://en.wikipedia.org/wiki/Hyperbolic_functions#Complex_trigonometric_definitions" target="_blank" rel="noopener">complex angles&lt;/a>. An intuitive way to think about the hyperbolic rotation is the &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">squeeze mapping&lt;/a>, which should be familiar to anyone who has seen a Lorentz transformation on a spacetime diagram.&lt;/p>
&lt;p>
&lt;figure id="figure-lorentz-boosts-are-squeeze-mappingshttpsenwikipediaorgwikisqueeze_mapping-from-wikimediahttpscommonswikimediaorgwikifileminkboost2gif">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Squeeze Mapping"
src="https://anilzen.github.io/post/hyperbolic-relativity/MinkBoost.gif"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Lorentz boosts are &lt;a href="https://en.wikipedia.org/wiki/Squeeze_mapping" target="_blank" rel="noopener">squeeze mappings&lt;/a>. &lt;br>From &lt;a href="https://commons.wikimedia.org/wiki/File:MinkBoost2.gif" target="_blank" rel="noopener">Wikimedia&lt;/a>.
&lt;/figcaption>&lt;/figure>
&lt;/p>
&lt;p>Consider the Lorentz boost with relative speed $v$ between frames
$$ L(v)= \begin{pmatrix}
\gamma &amp;amp; -\gamma \beta \\
-\gamma \beta &amp;amp; \gamma
\end{pmatrix}, $$
where we defined the rescaled speed $\beta:=\tfrac{v}{c}$ and the Lorentz factor $\gamma:=1/\sqrt{1-\beta^2}$. We can write it as a hyperbolic rotation with angle $w$:
$$ L(w)= \begin{pmatrix}
\cosh w &amp;amp; -\sinh w \\
-\sinh w &amp;amp; \cosh w
\end{pmatrix}, $$
where the Lorentz factor becomes $\gamma=\cosh w$ and the rescaled speed becomes $\beta=\tanh w$. So the Lorentz transformation is simply a rotation in hyperbolic space with an angle, $w$, related to the observer&amp;rsquo;s speed as $v=c \ \mathrm{arctanh}\ w$. Its inverse is just a rotation with the inverse angle, $L(w)^{-1}=L(-w)$. We can directly add the hyperbolic angles instead of dealing with weird addition formulas because composition of rotations along the same dimension is additive $L(w_1)L(w_2)=L(w_1+w_2)$.&lt;/p>
&lt;p>Physicists usually think in terms of velocities and speeds, not hyperbolic angles. Writing the angle addition in terms of speeds, we get
$$ \beta = \tanh (w_1+w_2) = \frac{\tanh w_1+\tanh w_2}{1+\tanh w_1 \tanh w_2} = \frac{\beta_1+\beta_2}{1+\beta_1 \beta_2}. $$
Einstein&amp;rsquo;s velocity addition (\ref{1}) becomes a consequence of hyperbolic identities.&lt;/p>
&lt;p>There is much more that one can simplify or understand with this viewpoint. For example, the formula for the Doppler shift is a simple $e^w$. The non-commutativity of general Lorentz boosts in higher dimensions becomes immediately apparent: Lorentz boosts do not commute just like rotations do not commute. The Thomas precession becomes easier to visualize as the consequence of parallel transport of vectors in the curved velocity space (&lt;a href="https://en.wikipedia.org/wiki/Holonomy" target="_blank" rel="noopener">holonomy transformation&lt;/a>). Many other calculations and ideas in special relativity translate to simple exercises in hyperbolic geometry.&lt;/p>
&lt;p>If you&amp;rsquo;re intrigued, follow the rabbit. Scott Walter&amp;rsquo;s papers are excellent for the historical developments: &lt;a href="https://halshs.archives-ouvertes.fr/halshs-00319209/" target="_blank" rel="noopener">Hermann Minkowski and the scandal of spacetime&lt;/a>, &lt;a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.6918&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="noopener">The Non-Euclidean Style of Minkowskian Relativity&lt;/a>, and &lt;a href="https://halshs.archives-ouvertes.fr/halshs-01234434/" target="_blank" rel="noopener">Minkowski&amp;rsquo;s modern world&lt;/a>. There are some gems in Barrett&amp;rsquo;s &lt;a href="https://arxiv.org/abs/1102.0462" target="_blank" rel="noopener">The Hyperbolic Theory of Special Relativity&lt;/a>. Rhodes and Semon go into those tedious calculations in &lt;a href="https://arxiv.org/abs/gr-qc/0501070" target="_blank" rel="noopener">Relativistic velocity space, Wigner rotation and Thomas precession&lt;/a>. Tevian Dray&amp;rsquo;s textbook on &lt;a href="https://www.amazon.com/Geometry-Special-Relativity-Tevian-Dray/dp/1466510471" target="_blank" rel="noopener">The Geometry of Special Relativity&lt;/a> from 2012 takes the hyperbolic viewpoint to teach special relativity.&lt;/p>
&lt;p>Ungar probably has the most influential and extensive work in this space. His books on &lt;a href="https://en.wikipedia.org/wiki/Gyrovector_space" target="_blank" rel="noopener">gyrovector space&lt;/a> present relativistic calculations in hyperbolic geometry from a solid group-theoretical viewpoint. Two of his relevant books are &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/6625" target="_blank" rel="noopener">Analytic Hyperbolic Geometry and Albert Einstein&amp;rsquo;s Special Theory of Relativity&lt;/a> from 2008 and &lt;a href="https://link.springer.com/book/10.1007/0-306-47134-5" target="_blank" rel="noopener">Beyond the Einstein Addition Law and its Gyroscopic Thomas Precession&lt;/a> from 2012. This work has even found applications in machine learning.&lt;/p>
&lt;h3 id="beyond-special">Beyond special&lt;/h3>
&lt;p>The hyperbolic view opens a new window to a universe beyond rewriting calculations in special relativity.&lt;/p>
&lt;p>Hyperbolic geometry efficiently represents &lt;a href="https://arxiv.org/abs/1006.5169" target="_blank" rel="noopener">hierarchical relationships and complex networks&lt;/a>. &lt;a href="https://arxiv.org/abs/1705.08039" target="_blank" rel="noopener">Nickel and Kiela&lt;/a> demonstrated in 2017 that hyperbolic geometry is better than Euclidean geometry when applying machine learning to complex data structures. In a &lt;a href="https://arxiv.org/abs/1806.03417" target="_blank" rel="noopener">follow-up paper&lt;/a>, they compared different hyperbolic models for learning hierarchical relationships and found that the Lorentz model is more efficient than the Poincaré model. Around the same time, &lt;a href="https://arxiv.org/abs/1805.09112" target="_blank" rel="noopener">Ganea, Bécigneul, and Hofmann&lt;/a> derived hyperbolic versions of deep learning tools by using Ungar&amp;rsquo;s gyrovector formalism.&lt;/p>
&lt;p>These papers opened up a new research direction in machine learning on &lt;a href="https://arxiv.org/abs/2101.04562" target="_blank" rel="noopener">hyperbolic deep neural networks&lt;/a>. The researchers use tools developed specifically for special relativity, such as Lorentz transformations and gyrovector spaces, to improve machine learning algorithms. Physicists usually associate velocities and universal speed limits with Lorentz transformations. It is fascinating that Lorentz transformations arise in problems without a notion of motion.&lt;/p>
&lt;p>Another application closer to my heart is hyperbolic geometry in general relativity. The spacetime of special relativity is special: we can identify Minkowski space with its tangent space which allows us to talk of spacetime events as four-vectors and apply global Lorentz transformations. These properties do not generalize to arbitrary Lorentzian manifolds. It&amp;rsquo;s not immediately evident that the hyperbolic viewpoint is still valuable when the spacetime is curved.&lt;/p>
&lt;p>As we have seen above, Minkowski&amp;rsquo;s hyperboloid (\ref{2}) consists of points at unit proper distance from the origin. Hyperboloids are as natural in Minkowski space as spheres are in Euclidean space. So &lt;strong>hyperboloidal&lt;/strong> coordinates may be as natural in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds.&lt;/p>
&lt;p>How can we construct such hyperboloidal coordinates? In many applications, we need to separate spacetime into space and time. Let&amp;rsquo;s separate spacetime into hyperbolic space and time in non-Euclidean style. One approach is to take a hint from spherical coordinates. Generalize the hyperbola from (\ref{2}) to an arbitrary radius of curvature
$$ t^2-x^2 = \tau^2, $$
and use $\tau$ as a new coordinate. Setting $x=\tau \ \sinh \rho$ gives us the &lt;a href="https://en.wikipedia.org/wiki/Milne_model" target="_blank" rel="noopener">Milne universe&lt;/a> with metric
$$ ds^2 = -d\tau^2 + \tau^2 d\xi^2.$$
These coordinates have various use cases today, including &lt;a href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.21.392" target="_blank" rel="noopener">quantum field theory&lt;/a>, &lt;a href="https://arxiv.org/abs/2107.02075" target="_blank" rel="noopener">holography&lt;/a>, and the theory of &lt;a href="https://www.worldscientific.com/worldscibooks/10.1142/9427" target="_blank" rel="noopener">partial differential equations&lt;/a>.&lt;/p>
&lt;p>Another approach is to shift the hyperbola (\ref{2}) along the time direction by $\tau$ recognizing the special role of time
$$ (\tau - t)^2 - x^2 = 1 $$
Using $\tau$ as the new time coordinate, the Minkowski metric becomes
$$ ds^2 = -d\tau^2 + \frac{2x}{\sqrt{1+x^2}} d\tau dx + \frac{1}{1+x^2} dx^2.$$
Level sets of $\tau$ are hyperbolic spaces. We can bring the metric into the more recognizable Poincaré form by the spatial transformation $x=2 \rho/(1-\rho^2)$.
$$ ds^2 = -d\tau^2 + \frac{4}{(1-\rho^2)^2} \left(2\rho d\tau d\rho + d\rho^2\right).$$
This construction generalizes to &lt;a href="../../publication/zenginoglu-2008-hyperboloidal/">curved spacetimes&lt;/a>. It&amp;rsquo;s favorable for numerically solving wave equations because the coefficients don&amp;rsquo;t depend on time. Some of the problems appearing in standard coordinates, such as the outer boundary and radiation extraction problems, &lt;a href="../../publication/zenginoglu-2011-hyperboloidal/">don&amp;rsquo;t arise with this approach&lt;/a> because we can solve the wave equation on the infinite spatial domain $\rho\in[0,1]$.&lt;/p>
&lt;p>Hyperboloidal coordinates may seem unnatural at first, but that&amp;rsquo;s because we are used to thinking about space and time in Newtonian terms. Once you spend some time with them, you see that hyperboloidal coordinates are as valuable in Lorentzian manifolds as spherical coordinates are in Riemannian manifolds. I devoted most of my &lt;a href="../../publication/">work&lt;/a> exploring properties of these surfaces and I find their relation to special relativity fascinating.&lt;/p>
&lt;h3 id="hyperclusion">Hyperclusion&lt;/h3>
&lt;p>Physics doesn&amp;rsquo;t care about how we make calculations in special relativity. The Wikipedia page for the &lt;a href="https://en.wikipedia.org/wiki/History_of_Lorentz_transformations" target="_blank" rel="noopener">history of Lorentz transformations&lt;/a> lists many equivalent formalisms. We could keep the usual relativistic formulas using velocities and Lorentz factors, or employ &lt;a href="https://en.wikipedia.org/wiki/Bondi_k-calculus" target="_blank" rel="noopener">Bondi&amp;rsquo;s $k$-calculus&lt;/a>, or &lt;a href="https://en.wikipedia.org/wiki/Gyrovector_space" target="_blank" rel="noopener">Ungar&amp;rsquo;s gyrovector space&lt;/a>. Why choose one over the other?&lt;/p>
&lt;p>It may be just personal preference. How do you understand? I understand by analogy, so it&amp;rsquo;s helpful to me when formalisms lend themselves to connections to other fields.&lt;/p>
&lt;p>There is another, much more relevant reason to prefer the hyperbolic viewpoint. As scientists, we want powerful descriptions. The same thinking style should apply to a broad class of phenomena. In other words, we want to avoid overfitting our formalism to the phenomena at hand. Hyperbolic calculations translate into machine learning, spacetime curvature, and maybe other areas to be discovered. The non-Euclidean approach with hyperbolic angles seems more powerful than the usual approach with velocities.&lt;/p>
&lt;!-- [^3]: Scott Walter wrote a wonderful historical account of these developments in ["The Non-Euclidean Style of Minkowskian Relativity"](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.535.6918&amp;rep=rep1&amp;type=pdf).
[^4]: Walter, Scott. ["Minkowski’s modern world."](https://halshs.archives-ouvertes.fr/halshs-01234434/) Minkowski Spacetime: A Hundred Years Later. Springer, Dordrecht, 2010. 43-61. -->
&lt;script src="https://giscus.app/client.js"
data-repo="anilzen/anilzen.github.io"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNzExMzY1Njk="
data-category="General"
data-category-id="DIC_kwDOFh8YOc4CTAsV"
data-mapping="pathname"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="1"
data-input-position="top"
data-theme="dark"
data-lang="en"
data-loading="lazy"
crossorigin="anonymous"
async>
&lt;/script>&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>The lecture was given on September 21, 1908 in Köln during a meeting of researchers of nature (the lovely German compound word for such a meeting is Naturforscherversammlung). The &lt;a href="https://www.math.nyu.edu/~tschinke/papers/yuri/14minkowski/raum-und-zeit.pdf" target="_blank" rel="noopener">German version&lt;/a> sounds stronger in its proclamation: &lt;em>&amp;ldquo;Von Stund&amp;rsquo; an sollen Raum für sich und Zeit für sich völlig zu Schatten herabsinken und nur noch eine Art Union der beiden soll Selbständigkeit bewahren.&amp;rdquo;&lt;/em>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>Walter, Scott. &amp;ldquo;&lt;a href="https://halshs.archives-ouvertes.fr/halshs-00319209/" target="_blank" rel="noopener">Hermann Minkowski and the scandal of spacetime.&lt;/a>&amp;rdquo; ESI News 3.1 (2008): 6-8.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>